{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2024/AdvML2024_ex03notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# AdvML ex03notebookA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/AdvML/AdvML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?AdvML)\n",
        "\n",
        "※ この notebook は，授業時間中の解説や板書と併用することを想定して作っていますので，説明が不十分なところが多々あります．\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gQ0oDxG3iygx"
      },
      "source": [
        "----\n",
        "## 準備\n",
        "----\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()"
      ],
      "metadata": {
        "id": "RjLTO96x2EvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### データを生成する関数\n",
        "#\n",
        "def generateData(N, trueFunc=False, seed=None, sigma=0.0):\n",
        "\n",
        "    if trueFunc:\n",
        "        x = np.linspace(-0.1, 1.1, num=N)\n",
        "        y = np.sin(2*np.pi*x) + 1\n",
        "    else:\n",
        "        # 乱数生成器\n",
        "        rng = np.random.default_rng(seed)\n",
        "        x = rng.uniform(0.0, 1.0, N)\n",
        "        y = np.sin(2*np.pi*x) + 1 + sigma*rng.standard_normal(N)\n",
        "\n",
        "    return x, y"
      ],
      "metadata": {
        "id": "MtBcrQJmV8_i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "##### 1, x, x^2, x^3, ..., x^D をならべたデータ行列（N x (D+1)）をつくる\n",
        "#\n",
        "def makeDataMatrix(x, D):\n",
        "\n",
        "    N = x.shape[0]\n",
        "    X = np.zeros((N, D+1))\n",
        "    X[:, 0] = 1\n",
        "    for d in range(1, D+1):\n",
        "        X[:, d] = x**d\n",
        "\n",
        "    return X"
      ],
      "metadata": {
        "id": "UG1n6Z7V7evF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### 線形回帰\n",
        "#\n",
        "def LinearRegression(X, y):\n",
        "    # 正規方程式の左辺の行列を A とする\n",
        "    A = X.T @ X\n",
        "    # 正規方程式の右辺のベクトルを b とする\n",
        "    b = X.T @ y\n",
        "    # np.linalg.solve を使って正規方程式を解き，解を w とする\n",
        "    w = np.linalg.solve(A, b)\n",
        "    return w"
      ],
      "metadata": {
        "id": "fc_sd3cs7iBX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### リッジ回帰\n",
        "#\n",
        "def RidgeRegression(X, y, lam, excludeBias=True):\n",
        "    # 正規方程式の左辺の行列を A とする\n",
        "    eyet = np.ones(X.shape[1])\n",
        "    if excludeBias:\n",
        "        eyet[0] = 0.0\n",
        "    A = X.T @ X + lam * np.diag(eyet)\n",
        "    # 正規方程式の右辺のベクトルを XTY とする\n",
        "    b = X.T @ y\n",
        "    #print(X.shape, A.shape, b.shape)\n",
        "    # np.linalg.solve を使って正規方程式を解き，解を w とする\n",
        "    w = np.linalg.solve(A, b)\n",
        "    return w"
      ],
      "metadata": {
        "id": "zmAeXyzkBrc_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## バイアス vs バリアンス\n",
        "---"
      ],
      "metadata": {
        "id": "ZgcKm-AX7srC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "汎化性能の性質について，理論的な側面から解説する．ただし，ここでは厳密な議論を避けて直感的な説明にとどめる．詳しくは↓等を参考にされたい．\n",
        "\n",
        "【参考】 PRML本"
      ],
      "metadata": {
        "id": "3fzTcH1dy5Lg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "データ $\\mathbf{x}$ から $y \\in R$ を予測する回帰の問題を考える． $\\mathbf{x}$ と $y$ の関係は関数 $h(\\mathbf{x})$ で定まるものとする．\n",
        "様々な $\\mathbf{x}$ と $y$ の観測値を集めたデータ集合を ${\\cal D}$ と表記し，${\\cal D}$ を用いてあるモデルを学習させて得られる関数を $f(\\mathbf{x}; {\\cal D})$ と表記する．\n",
        "\n",
        "ここで，${\\cal D}$ はひとつに定まるものではなく，無数に存在するとして統計的な取り扱いをすることに注意．例えば，前回の多項式回帰の実験では，学習データのうち入力のサンプルは $[0, 1]$ の一様分布からランダムサンプリングした値だった．出力の正解は，真の値 $h(x)$ に正規乱数を加えた値だった．このような場合，実験のたびに学習に使うデータ集合が変化する．\n",
        "\n",
        "このような問題設定においては，${\\cal D}$ を用いて得られたモデルによる予測値 $f(\\mathbf{x}; {\\cal D})$ と真の値 $h(\\mathbf{x})$ との間の二乗誤差 $ \\{f(\\mathbf{x}; {\\cal D}) - h(\\mathbf{x})\\}^2$ が，このモデルの汎化性能の指標となる．\n",
        "この値は ${\\cal D}$ の取り方によって変化するので，その期待値 $\\textrm{E} \\left[ \\{f(\\mathbf{x}; {\\cal D}) - h(\\mathbf{x})\\}^2 \\right]$ のふるまいを調べる．"
      ],
      "metadata": {
        "id": "1EDn8rRN-Qe_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "具体的な計算過程は省略するが，$\\textrm{E} \\left[ \\{f(\\mathbf{x}; {\\cal D}) - h(\\mathbf{x})\\}^2 \\right]$ は次のように変形できる．\n",
        "\n",
        "$$\n",
        "\\textrm{E} \\left[ \\{f(\\mathbf{x}; {\\cal D}) - h(\\mathbf{x})\\}^2 \\right]\n",
        "=　\\underbrace{\\left\\{ \\textrm{E} \\left[ f(\\mathbf{x}; {\\cal D}) \\right] - h(\\mathbf{x}) \\right\\}^2}_{(\\text{bias})^2}\n",
        "+ \\underbrace{\\textrm{E} \\left[ \\left\\{ f(\\mathbf{x}; {\\cal D}) - \\textrm{E}\\left[ f(\\mathbf{x}; {\\cal D})\\right] \\right\\}^2 \\right]}_{\\text{variance}}\n",
        "$$\n",
        "\n",
        "\n",
        "右辺第1項は **バイアス** (bias) と呼ばれる量の二乗で，取り得るすべての ${\\cal D}$ に関する予測値 $f(\\mathbf{x};{\\cal D})$ の期待値 $\\textrm{E} \\left[ f(\\mathbf{x}; {\\cal D}) \\right]$ が真の関数 $h(\\mathbf{x})$ とどのくらいずれているかを表す量である．\n",
        "\n",
        "一方，右辺第2項は **バリアンス** (variance) と呼ばれる量で，個々の ${\\cal D}$ で得られる $f(\\mathbf{x}; \\cal{D})$ が，それらの期待値 $\\textrm{E}\\left[ f(\\mathbf{x}; {\\cal D})\\right]$ のまわりでどのくらい変動するのか，その変動の大きさを表している（注）．\n",
        "データ集合 ${\\cal D}$ の取り方によって $f(\\mathbf{x}; {\\cal D})$ が大きく変わるならば，この量は大きくなる．\n",
        "\n",
        "<div stype=\"font-size:x-small\">\n",
        "注: $X$ に関する「分散」（英語で variance）は $\\textrm{E}\\left[ (X - \\textrm{E}(X))^2 \\right]$ と表せるのだった．\n",
        "</div>\n",
        "\n"
      ],
      "metadata": {
        "id": "5ON0yXKdUQPv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "バイアスの2乗もバリアンスも非負だから，予測の二乗誤差の期待値が一定とすると，これらは互いにトレードオフの関係（一方が大きくなると他方が小さくなるような関係）にある．\n",
        "下図は，そのことを模式的に表したものである．\n",
        "\n",
        "上段は，複雑度の小さいモデル（例: 多項式回帰で次数が小さいときや正則化パラメータの値が大きいとき）の図で，左に，様々な${\\cal D}$ で得られる予測値 $f(x; {\\cal D})$ を，右に，それらの期待値 $\\textrm{E} \\left[ f(x; {\\cal D})\\right]$ と真の関数 $h(x)$ を描いている．\n",
        "左図に示すように， $f(x; {\\cal D})$ の変動は小さい（バリアンスが小さい）が，右図から分かるように，それらの期待値と真の値とのずれは大きい（バイアスの2乗が大きい）．\n",
        "\n",
        "一方，複雑度の大きいモデル（下段）の場合，個々のモデルがデータによく当てはまり過ぎるため，$f(x; {\\cal D})$ の変動が大きく（バリアンスが大きく）なっているが，それらの期待値と真の値とのずれは小さく（バイアスの2乗が小さく）なっている．\n",
        "\n",
        "モデルの複雑度が小さすぎる場合，試行ごとの予測値のばらつきは小さいものの，どれもデータにうまく当てはまらない．逆に，複雑度が大きすぎる場合，個々のモデルがデータによく当てはまりすぎて，試行ごとの予測値のばらつきが大きくなる．汎化性能の観点からは，バイアスとバリアンスの双方がバランスよく小さくなるようなモデルが望ましい．\n",
        "\n",
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/AdvML/biasvsvariance.png\">"
      ],
      "metadata": {
        "id": "64Y4Xd1qHViD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## モデルの検証と選択\n",
        "---"
      ],
      "metadata": {
        "id": "2YgTtAerny5L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 考え方\n",
        "\n",
        "性能評価したい，汎化能力がなるべく高くなるようにハイパーパラメータを決めたい\n",
        "\n"
      ],
      "metadata": {
        "id": "LV4OFFB1n7Fe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### ホールドアウト"
      ],
      "metadata": {
        "id": "iQQu0-tZogsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**ホールドアウト**(holdout)は，単純に学習データの一部を取り分けて学習に使わず検証用に使う方法である．"
      ],
      "metadata": {
        "id": "3EY0JKq8HlA_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習データ\n",
        "N = 25\n",
        "sig = 0.1\n",
        "x, y = generateData(N, sigma=sig)\n",
        "print(x.shape, y.shape)\n",
        "\n",
        "# うちNL個をあらためて学習用とし，残りを検証用とする\n",
        "NL = 20\n",
        "xL, yL = x[:NL], y[:NL]\n",
        "xV, yV = x[NL:], y[NL:]\n",
        "NV = len(xV)\n",
        "print(xL.shape, yL.shape)\n",
        "print(xV.shape, yV.shape)"
      ],
      "metadata": {
        "id": "Jz0oxZJ0xaHL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ホールドアウト法で正則化パラメータを決定するリッジ回帰\n",
        "D = 9\n",
        "alphaList = [0.0, 0.00001, 0.0001, 0.001, 0.01] # 正則化パラメータ\n",
        "\n",
        "msqeL = np.zeros(len(alphaList))\n",
        "msqeV = np.zeros(len(alphaList))\n",
        "\n",
        "for i, alpha in enumerate(alphaList):\n",
        "\n",
        "    # 学習用データを使って学習\n",
        "    XL = makeDataMatrix(xL, D)\n",
        "    w = RidgeRegression(XL, yL, alpha)\n",
        "\n",
        "    # 学習用データに対する msqe\n",
        "    yL_est = XL @ w\n",
        "    msqeL[i] = np.mean((yL - yL_est)**2)\n",
        "\n",
        "    # 検証用データに対する msqe\n",
        "    XV = makeDataMatrix(xV, D)\n",
        "    yV_est = XV @ w\n",
        "    msqeV[i] = np.mean((yV - yV_est)**2)\n",
        "\n",
        "    print(f'alpha = {alpha:.5f}   msqeL = {msqeL[i]:.5f}   msqeV = {msqeV[i]:.5f}')\n",
        "\n",
        "# 検証用データに対する msqe が最小のモデルを選ぶ\n",
        "idx = np.argmin(msqeV)\n",
        "print()\n",
        "print(f'The model with alpha = {alphaList[idx]} is chosen.')"
      ],
      "metadata": {
        "id": "vA1Uz0bxrFjf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### クロスバリデーション"
      ],
      "metadata": {
        "id": "OQjdPvoqoM4Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ホールドアウト法は簡便だが，元々の学習データ数が少ない場合，検証に使えるデータがごく少数となり，信頼できる検証は難しい．\n",
        "もうひと手間かける検証法に，**クロスバリデーション**（**交差検証**, **cross validation**）がある．ここでは，クロスバリデーションの方法のひとつである，**n-分割交差検証法** (**n-fold cross validation**)を紹介する．\n",
        "\n"
      ],
      "metadata": {
        "id": "U6Z5N2eLINBf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "12個の学習データが与えられたときに 3 分割交差検証を適用する例を説明する．\n",
        "この場合，学習データを次のように A, B, C の3通りに分ける．\n",
        "\n",
        "$$\n",
        "\\fbox{A}\\fbox{A}\\fbox{A}\\fbox{A}\\fbox{B}\\fbox{B}\\fbox{B}\\fbox{B}\\fbox{C}\\fbox{C}\\fbox{C}\\fbox{C}\n",
        "$$\n",
        "\n",
        "このとき，A, B, C のうちいずれか1つを検証データにすれば，次の3通りの分け方ができる．\n",
        "\n",
        "$$\n",
        "\\mbox{学習データ}\\fbox{A}\\fbox{A}\\fbox{A}\\fbox{A}\\fbox{B}\\fbox{B}\\fbox{B}\\fbox{B}\\quad\\mbox{検証データ}\\fbox{C}\\fbox{C}\\fbox{C}\\fbox{C}\\\\\n",
        "\\mbox{学習データ}\\fbox{A}\\fbox{A}\\fbox{A}\\fbox{A}\\fbox{C}\\fbox{C}\\fbox{C}\\fbox{C}\\quad\\mbox{検証データ}\\fbox{B}\\fbox{B}\\fbox{B}\\fbox{B}\\\\\n",
        "\\mbox{学習データ}\\fbox{B}\\fbox{B}\\fbox{B}\\fbox{B}\\fbox{C}\\fbox{C}\\fbox{C}\\fbox{C}\\quad\\mbox{検証データ}\\fbox{A}\\fbox{A}\\fbox{A}\\fbox{A}\n",
        "$$\n",
        "\n",
        "3通りのそれぞれで学習をおこない，検証の結果を平均する．"
      ],
      "metadata": {
        "id": "Un9JTrVtJ-xm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## scikit-learn"
      ],
      "metadata": {
        "id": "rFjfwJ-lR2dH"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "scikit-learn は，Python による機械学習ライブラリの代表格である．\n",
        "\n",
        "\n",
        "https://scikit-learn.org/"
      ],
      "metadata": {
        "id": "emTrnaEuR8JC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 線形回帰の例"
      ],
      "metadata": {
        "id": "UKGplWBkVZ7_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "データの準備をしましょう．以前にも使った，小テストの点数と睡眠時間から期末テストの点数を予測する問題を使うことにします．\n",
        "\n",
        "このデータは，\n",
        "\n",
        "「Pythonで理解する統計解析の基礎」 谷合廣紀，辻 真吾，技術評論社，2018.\n",
        "\n",
        "に掲載されているものです．以下の GitHub サイトで公開されています．\n",
        "https://github.com/ghmagazine/python_stat_sample"
      ],
      "metadata": {
        "id": "gtwuqjByt8XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "df = pd.read_csv('https://github.com/ghmagazine/python_stat_sample/raw/master/data/ch12_scores_reg.csv')\n",
        "df.drop(columns='通学方法', inplace=True)\n",
        "\n",
        "# データ配列の作成（定数 1 の列を付け足す必要はない）\n",
        "XL = df.loc[:, ['小テスト', '睡眠時間']].to_numpy()\n",
        "yL = df['期末テスト'].to_numpy()\n",
        "print(XL.shape, yL.shape)"
      ],
      "metadata": {
        "id": "djVvht0JzJo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[sklearn.linear_model.LinearRegression](\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LinearRegression.html) を使って線形回帰の実験をやってみよう．\n"
      ],
      "metadata": {
        "id": "JVURzvGFTEl4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "# LinearRegression クラスのオブジェクトを生成\n",
        "#  引数でいろいろ設定できるが，ここではデフォルトで\n",
        "model = LinearRegression()\n",
        "\n",
        "# 学習\n",
        "regressor = model.fit(XL, yL)\n",
        "\n",
        "# 得られたパラメータを表示\n",
        "print(f'w_0 = {regressor.intercept_},  [w_1, w_2] = {regressor.coef_}')"
      ],
      "metadata": {
        "id": "ZPBPW6XHSzP4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 適当なテストデータを作って予測させてみる\n",
        "XT = np.array([[8.0, 10.0], [12.0, 1.0]])\n",
        "yT_pred = regressor.predict(XT)\n",
        "\n",
        "for n in range(len(XT)):\n",
        "    print(f'{XT[n, :]} に対する予測値は {yT_pred[n]:.2f}')"
      ],
      "metadata": {
        "id": "jtdleOsWXl7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### リッジ回帰の例"
      ],
      "metadata": {
        "id": "eBBtUNJkVVyp"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "[sklearn.linear_model.Ridge](\n",
        "https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html) でリッジ回帰ができる．"
      ],
      "metadata": {
        "id": "uMnM2FHgZ1oM"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 上の例で"
      ],
      "metadata": {
        "id": "uXHS3t6iilb9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の線形回帰のコードの\n",
        "```\n",
        "from sklearn.linear_model import LinearRegression\n",
        "```\n",
        "の行を\n",
        "```\n",
        "from sklearn.linear_model import Ridge\n",
        "```\n",
        "に変え（または追加し）て，\n",
        "```\n",
        "model = LinearRegression()\n",
        "```\n",
        "の行を\n",
        "```\n",
        "model = Ridge(alpha=0.1)\n",
        "```\n",
        "に変えれば，簡単にリッジ回帰（$\\alpha = 0.1$）の実験をすることができる．やってみよう．"
      ],
      "metadata": {
        "id": "iIvl-44PZOT6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 多項式回帰の例で"
      ],
      "metadata": {
        "id": "UgzMx9-Pirs8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次は，多項式回帰の例．"
      ],
      "metadata": {
        "id": "2ZEFcigpY1i0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# D次多項式による多項式回帰のための学習データを用意\n",
        "N = 25\n",
        "D = 9\n",
        "x, y = generateData(N, sigma=0.1)\n",
        "X = np.zeros((N, D)) # 定数 1 の列を付け足す必要はない\n",
        "for d in range( D):\n",
        "    X[:, d] = x**d\n",
        "print(X.shape, y.shape)"
      ],
      "metadata": {
        "id": "OL1jhCgxR5ss"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[sklearn.model_selection.train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html) を使うと，ホールドアウト検証が簡単．"
      ],
      "metadata": {
        "id": "KZzheskqf_47"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# X, y のうち 20 個を学習用に，残りを検証用に，ランダムに分ける\n",
        "XL, XV, yL, yV = train_test_split(X, y, train_size=20)\n",
        "print(XL.shape, yL.shape)\n",
        "print(XV.shape, yV.shape)"
      ],
      "metadata": {
        "id": "T1yC4IWBTjID"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "alphaList = [1e-5, 1e-4, 1e-3, 1e-2, 0.1]\n",
        "\n",
        "for alpha in alphaList:\n",
        "\n",
        "    # 学習\n",
        "    model = Ridge(alpha=alpha)\n",
        "    regressor = model.fit(XL, yL)\n",
        "\n",
        "    # 学習データに対する平均二乗誤差\n",
        "    yL_pred = regressor.predict(XL)\n",
        "    msqeL = np.mean((yL - yL_pred)**2)\n",
        "\n",
        "    # 検証データに対する平均二乗誤差\n",
        "    yV_pred = regressor.predict(XV)\n",
        "    msqeV = np.mean((yV - yV_pred)**2)\n",
        "\n",
        "    print(f'alpha = {alpha:.5f}, msqeL = {msqeL:.5f}, msqeV = {msqeV:.5f}')"
      ],
      "metadata": {
        "id": "0ayJlI39fwoR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Fd3bssOegt6O"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
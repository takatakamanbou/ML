{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2023/ex02notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex02notebookA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2023)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IfD2FEfXh34O"
      },
      "source": [
        "----\n",
        "## 回帰のための教師あり学習(1) 平面の当てはめ\n",
        "----"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glgWbdlh34P"
      },
      "source": [
        "----\n",
        "### 準備\n",
        "\n",
        "\n",
        "以下，コードセルを上から順に実行してながら読んでいってね．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBTXo5eHh34P"
      },
      "outputs": [],
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルを実行すると，データを Pandas の DataFrame として読み込みます．\n",
        "ここでは Pandas/DataFrame についての理解は求めませんので，このコードセルが何をやっているかはわからなくても構いません．\n",
        "\n",
        "ここで読み込んでいるデータは，\n",
        "\n",
        "「Pythonで理解する統計解析の基礎」 谷合廣紀，辻 真吾，技術評論社，2018.\n",
        "\n",
        "に掲載されているものです．以下の GitHub サイトで公開されています．\n",
        "https://github.com/ghmagazine/python_stat_sample"
      ],
      "metadata": {
        "id": "T9aMWDAh-7LX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの読み込み\n",
        "df = pd.read_csv('https://github.com/ghmagazine/python_stat_sample/raw/master/data/ch12_scores_reg.csv')\n",
        "df.drop(columns='通学方法', inplace=True)\n",
        "df"
      ],
      "metadata": {
        "id": "djVvht0JzJo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このデータは，とある授業の受講者20名について，「小テスト」と「期末テスト」の点数，および各受講者の「期末テスト」前日の「睡眠時間」という3つのデータを集めたものとなっています（元のデータにはこれ以外に「通学方法」というのもありますが，ここでは省いています）．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "euU8_EhW0kb6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WM4RlkyhP5yo"
      },
      "source": [
        "----\n",
        "### 平面当てはめの問題設定\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記のデータを見たときに，「小テスト」と「睡眠時間」が「期末テスト」に影響していると考えるのは自然なことでしょう．「小テスト」と「睡眠時間」の値から「期末テスト」の値を予測する問題を考えてみます．\n",
        "「ゴリゴリ君」のときは，気温 $x$ からアイス売上数 $y$ を予測するために $y = ax+b$ という式を立てましたが，データの場合は $x$ に相当する数値が2つあります．\n",
        "\n",
        "一般化して考えると，このような問題は，$x_1, x_2, \\ldots, x_D$ という $D$ 個の値（いまは $D = 2$）から一つの値 $y$ を予測する問題ということができます．このとき，$x_1, \\ldots , x_D$ から $y$ を予測する式として単純なのは，\n",
        "\n",
        "$$\n",
        "y = w_0 + w_1x_1 + w_2x_2 + \\cdots + w_Dx_D \\qquad (1)\n",
        "$$\n",
        "\n",
        "という形のものです．入力が1つの場合は直線の式でパラメータが2つだったのに対して，この式ではパラメータが $w_0, w_1, \\ldots , w_D$ の $(D+1)$ 個あります．\n",
        "\n",
        "式(1)は，$D=1$ のときは2次元平面上の直線，$D=2$ のときは3次元空間中の平面となり，一般の $D$ については，$(D+1)$次元空間中の $D$ 次元超平面となります．\n",
        "以下の図は，2次元のデータに対する平面当てはめの例です．個々の青い点は，$(x_1, x_2, そのときの y の正解)$ を表し，赤いメッシュがこれらのデータに当てはめた平面を表しています．\n",
        "\n"
      ],
      "metadata": {
        "id": "xrIKO0QWGZtX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/planefitting.png\" width=\"50%\">"
      ],
      "metadata": {
        "id": "9RT8D4RZ94jl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "入力が1つ（1次元）の直線当てはめの問題では，最小二乗法によって最適な（二乗誤差の和が最小となる）パラメータを求めました．入力がD個（$D$次元）の場合も同様に，最小二乗法によってデータに当てはまる（超）平面のパラメータを求められます．\n"
      ],
      "metadata": {
        "id": "BpHU2O2G876D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下，最小二乗法による平面当てはめの問題設定を整理して説明します．ただし，式の見通しをよくするために，$1, x_1, x_2, \\ldots, x_D$ という $(D+1)$ 個の値をならべた $(D+1)$ 次元ベクトルを \n",
        "$$\n",
        "\\mathbf{x} = (1, x_1, x_2, \\ldots, x_D)\n",
        "$$\n",
        "と表し（先頭に$1$があることに注意），$(D+1)$個のパラメータをならべた $(D+1)$ 次元ベクトルを \n",
        "$$\n",
        "\\mathbf{w} = (w_0, w_1, w_2, \\ldots, w_D)\n",
        "$$\n",
        "と表すことにします．こうしておくと，式$(1)$を\n",
        "$$\n",
        "y = f(\\mathbf{x}) = \\mathbf{w}\\cdot\\mathbf{x} \\qquad (2)\n",
        "$$\n",
        "と簡単な形で書けます（注）．\n",
        "\n",
        "※注: 記号$\\cdot$は普通のベクトル同士の内積です．$\\mathbf{w}\\cdot\\mathbf{x} = w_0\\times1 + w_1\\times x_1+\\ldots+w_D\\times x_D$.\n"
      ],
      "metadata": {
        "id": "r5pRTvEJCgU_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**［最小二乗法による平面当てはめの問題設定］**\n",
        "\n",
        "$D$ 個の値 $x_1, x_2, \\ldots, x_D$ から $y$ の値が決まるようなデータがあり，それらの間の関係として式(2)が仮定できるとする．$N$個のデータ\n",
        "$$\n",
        "(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2),\\ldots , (\\mathbf{x}_N, y_N)\n",
        "$$\n",
        "が与えられる．ただし，$\\mathbf{x}_n$は上述のように$1$と$D$個の値をならべた$(D+1)$次元ベクトルであり，$y_n$ はこのデータに対する $y$ の値の正解である（$n=1,2,\\ldots, N$）．\n",
        "\n",
        "このとき，$f(\\mathbf{x}_n)$ とその正解の値 $y_n$ との間の二乗誤差の和\n",
        "$$\n",
        "\\sum_{n=1}^{N}(y_n - f(\\mathbf{x}_n))^2 =\\sum_{n=1}^{N}(y_n - \\mathbf{w}\\cdot\\mathbf{x}_n)^2 \\qquad (3)\n",
        "$$\n",
        "を最小にするパラメータ $\\mathbf{w}$ を求めたい．"
      ],
      "metadata": {
        "id": "9Y5G3U5ZGMCV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 平面当てはめの最小二乗法の解\n"
      ],
      "metadata": {
        "id": "aKYo9tLULqC0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まず，ベクトル $\\mathbf{x}_n$ を行ベクトル（要素が行方向に並んだベクトル）とみなして，それらを列方向に並べた行列を $X$ とします．\n",
        "\n",
        "$$\n",
        "X = \\begin{pmatrix}\n",
        "\\mathbf{x}_1\\\\\n",
        "\\mathbf{x}_2\\\\\n",
        "\\vdots\\\\\n",
        "\\mathbf{x}_N\\\\\n",
        "\\end{pmatrix}\n",
        "=\n",
        "\\begin{pmatrix}\n",
        "1 & x_{1,1} & x_{1,2} & \\cdots & x_{1,D}\\\\\n",
        "1 & x_{2,1} & x_{2,2} & \\cdots & x_{2,D}\\\\\n",
        "& & \\vdots\\\\\n",
        "1 & x_{N,1} & x_{N,2} & \\cdots & x_{N,D}\\\\\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$X$ は $N\\times(D+1)$ の行列です．次に，出力の正解の値 $y_1, y_2, \\ldots, y_N$ を列方向に並べた行列を $Y$ とします．\n",
        "\n",
        "\n",
        "$$\n",
        "Y = \\begin{pmatrix}\n",
        "y_1\\\\ y_2 \\\\ \\vdots \\\\ y_N\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "$Y$ は $N\\times 1$ の行列です．\n",
        "このとき，正規方程式は次式のようになります（正規方程式がこのようになることを示すのは後の節に回します）．\n",
        "\n",
        "$$\n",
        "X^{\\top}X\n",
        "\\begin{pmatrix}\n",
        "w_0\\\\ w_1\\\\ \\vdots \\\\ w_D\n",
        "\\end{pmatrix}\n",
        "= X^{\\top}Y \\qquad (4)\n",
        "$$\n",
        "\n",
        "この式の形からわかるように，正規方程式は $(D+1)$元の連立方程式です．\n",
        "\n",
        "したがって，与えられた学習データから行列 $X, Y$ を作り，$X^{\\top}X$ と $X^{\\top}Y$ を計算して式$(4)$を解けば，二乗誤差の和を最小とするパラメータ $w_0, w_1, \\ldots, w_{D}$ が求まります．\n"
      ],
      "metadata": {
        "id": "yxv9b1W_9LuI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 「小テスト」と「睡眠時間」から「期末テスト」を予測するモデルを作る\n",
        "\n",
        "実際に平面当てはめの実験をやってみましょう．\n"
      ],
      "metadata": {
        "id": "ZAmjBXWdWEUK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N x (D+1) 行列 X をつくる\n",
        "X_raw = df.loc[:, ['小テスト', '睡眠時間']].to_numpy()\n",
        "X = np.vstack([np.ones(len(X_raw)), X_raw.T]).T\n",
        "print(X)\n",
        "print(X.shape)"
      ],
      "metadata": {
        "id": "_mnk-2JeCkxD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 正解の値をならべたベクトル Y をつくる\n",
        "Y = df['期末テスト'].to_numpy()\n",
        "print(Y)\n",
        "print(Y.shape)"
      ],
      "metadata": {
        "id": "eFlyt-MMYcCG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 正規方程式を解く\n",
        "XTX = X.T @ X  # 正規方程式の左辺の(D+1)x(D+1)の行列\n",
        "XTY = X.T @ Y    # 正規方程式の右辺の(D+1)x1の行列\n",
        "w = np.linalg.solve(XTX, XTY) # 連立方程式を解く\n",
        "print(f'{len(w)}個のパラメータの値は')\n",
        "print(w)"
      ],
      "metadata": {
        "id": "FUiaDT_CYzIx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データに対して二乗誤差の和を最小にする3つのパラメータ $w_0, w_1, w_2$ の値が得られました．\n",
        "このパラメータで予測がどの程度できるのか試してみましょう．"
      ],
      "metadata": {
        "id": "sXIh9lepZtnf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 予測値の計算\n",
        "Yt = X @ w\n",
        "\n",
        "print(' 小テスト 睡眠 期末  予測値')\n",
        "for n in range(len(X)):\n",
        "    print(f'{n:2d}  {X[n, 1]}  {X[n, 2]}  {Y[n]}  {Yt[n]:.2f}')"
      ],
      "metadata": {
        "id": "dZBhnmsGD27v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "それっぽい値を出しているのもあれば，大きく外しているのもありますね．"
      ],
      "metadata": {
        "id": "1al9tyaDgZPt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "★ やってみよう\n",
        "\n",
        "次のセルを実行すると何が得られるのか，上のセルの内容および実行結果と比較して考えなさい．\n",
        "それをふまえて，小テストが5点で睡眠時間が8時間のひとと，小テストが8点で睡眠時間が5時間のひとの期末テストの得点の予測値を求めなさい．結果を紙媒体にメモしておきなさい．"
      ],
      "metadata": {
        "id": "hL2Bexn7h4EK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "XX = np.array([\n",
        "    [1, 4.2, 7.2],\n",
        "    [1, 7.2, 7.9],\n",
        "    ])\n",
        "Yt = XX @ w\n",
        "for n in range(len(XX)):\n",
        "    print(f'{n:2d}  {XX[n, 1]}  {XX[n, 2]}  {Yt[n]:.2f}')"
      ],
      "metadata": {
        "id": "woiT91I1FlmE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### ［補足］ 回帰分析？重回帰分析？\n",
        "\n",
        "この授業で登場している直線当てはめおよび平面当てはめの方法は，「教師あり学習」の「回帰」のための方法として最も簡単なものです．これらの方法は，データ分析や統計学の分野では，**回帰分析**，**重回帰分析**という名前で知られています（注）．\n",
        "\n",
        "※注: 変数（被説明変数）が1つなのが（単）回帰，2つ以上なのが重回帰．区別せずどちらも回帰分析ということもあります．"
      ],
      "metadata": {
        "id": "TBd9LOTLlzrD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### ［発展］平面当てはめの最小二乗法の解の導出"
      ],
      "metadata": {
        "id": "usZ45OupoHvc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "この節では，平面当てはめの最小二乗法の解が式$(4)$の連立方程式の解として求まることを示します．\n",
        "この節の内容は発展的な話題であり，この授業で必ず修得すべきものとはしません．興味・時間の余裕のあるひとは式を追いかけたり式変形を自分でやってみたりしてください．"
      ],
      "metadata": {
        "id": "oGM1Hujy1SLs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(step1) 誤差関数 $E(\\mathbf{w})$ を次のように定義します．\n",
        "$$\n",
        "E(\\mathbf{w}) = \\frac{1}{2}\\sum_{n=1}^N (y_n - \\mathbf{w}\\cdot\\mathbf{x}_n)^2\n",
        "$$\n",
        "ただし，$\\mathbf{w}$ は $w_0, w_1, \\ldots, w_D$ を並べた $(D+1)$ 次元ベクトルです．"
      ],
      "metadata": {
        "id": "ZOU0iUaRqg22"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(step2) $E(\\mathbf{w})$ が最小となるパラメータを求めるために，これを $w_d$ ($d = 0, 1, 2, \\ldots, D$) のそれぞれで偏微分したものを計算します．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial E(\\mathbf{w})}{\\partial w_d} &= \\sum_{n=1}^N (y_n - \\mathbf{w}\\cdot\\mathbf{x}_n) \\frac{\\partial}{\\partial w_d}(y_n - \\mathbf{w}\\cdot\\mathbf{x}_n)   \\\\\n",
        "&= \\sum_{n=1}^N (y_n - \\mathbf{w}\\cdot\\mathbf{x}_n)\\left( 0 -\\frac{\\partial}{\\partial w_d}(w_0+w_1x_{n,1}+\\cdots + w_Dx_{n,D})\\right) \\\\\n",
        "&= \\sum_{n=1}^N (y_n - \\mathbf{w}\\cdot\\mathbf{x}_n)(-x_{n,d})\\qquad (d = 0, 1, 2, \\ldots, D)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "$\\mathbf{x}_n$ も $\\mathbf{w}$ も要素は添字の番号が $0$ のものから $D$ のものまである（$(D+1)$個ある）ことに注意してください．さらに，$\\mathbf{x}_n$ の $0$ 番目の要素 $x_{n,0}$ は常に $1$ です．\n"
      ],
      "metadata": {
        "id": "7SX8xpUw30Ti"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(step3) \n",
        "$\\frac{\\partial E(\\mathbf{w})}{\\partial w_d} = 0$ とすると，次の式が得られます．\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "\\sum x_{n,0}\\mathbf{w}\\cdot\\mathbf{x}_n \\\\\n",
        "\\sum x_{n,1}\\mathbf{w}\\cdot\\mathbf{x}_n \\\\\n",
        "\\vdots \\\\\n",
        "\\sum x_{n,D}\\mathbf{w}\\cdot\\mathbf{x}_n\n",
        "\\end{pmatrix} = \n",
        "\\begin{pmatrix}\n",
        "\\sum x_{n,0}y_n \\\\ \\sum x_{n,1}y_n\\\\ \\vdots \\\\ \\sum x_{n,D}y_n\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "これは，$(D+1)$ 個の未知数 $w_0, w_1, \\ldots, w_D$ に対して $(D+1)$ 個の式から成る連立方程式です．これを整理して，式$(4)$を導出します．\n",
        "\n",
        "この式の左辺の $d$ 番目（$d=0,1,\\ldots,D$）の行を取り出して考えると，\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\sum_{n=1}^{N} x_{n,d}\\mathbf{w}\\cdot\\mathbf{x}_n &= \\sum_{n=1}^{N} (x_{n,d}\\mathbf{x}_n)\\cdot \\mathbf{w} = \\left( \\sum_{n=1}^{N} x_{n,d}\\mathbf{x}_n\\right) \\cdot\\mathbf{w} \\\\\n",
        "&= \\left( \\sum x_{n,d}x_{n,0}, \\sum x_{n,d}x_{n,1},  \\ldots, \\sum x_{n,d}x_{n,D} \\right) \\cdot \\mathbf{w}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "と書けるので，この式の左辺は\n",
        "\n",
        "$$\n",
        "\\begin{pmatrix}\n",
        "\\sum x_{n,0}x_{n,0} & \\sum x_{n,0}x_{n,1} & \\cdots & \\sum x_{n,0}x_{n,D} \\\\\n",
        "\\sum x_{n,1}x_{n,0} & \\sum x_{n,1}x_{n,1} & \\cdots & \\sum x_{n,1}x_{n,D} \\\\\n",
        "\\vdots & \\vdots & & \\vdots\\\\\n",
        "\\sum x_{n,D}x_{n,0} & \\sum x_{n,D}x_{n,1} & \\cdots & \\sum x_{n,D}x_{n,D} \\\\\n",
        "\\end{pmatrix}\n",
        "\\begin{pmatrix}\n",
        "w_0\\\\ w_1\\\\ \\vdots \\\\ w_D\n",
        "\\end{pmatrix}\n",
        "= \\left(\\sum_{n=1}^{N} \\mathbf{x}_n^{\\top} \\mathbf{x}_n \\right)\n",
        "\\begin{pmatrix}\n",
        "w_0\\\\ w_1\\\\ \\vdots \\\\ w_D\n",
        "\\end{pmatrix}\n",
        "= X^{\\top}X\n",
        "\\begin{pmatrix}\n",
        "w_0\\\\ w_1\\\\ \\vdots \\\\ w_D\n",
        "\\end{pmatrix}\n",
        "$$\n",
        "\n",
        "となります．\n",
        "\n",
        "右辺の方も同様にして $X^{\\top}Y$ と等しいことがわかります（そっちの方が簡単です．自分で導出してみてね）．"
      ],
      "metadata": {
        "id": "_z8gQOHII_ey"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3sRAa4grJFbq"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
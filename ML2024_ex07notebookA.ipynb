{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2024/ML2024_ex07notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex07notebookA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## 決定木\n",
        "----\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットは様々な問題で高い性能を発揮しますが，入力から出力に至る過程は，何をやっているのか人間が解釈するのが難しいブラックボックスとなりがちです．\n",
        "正しい出力さえ得られればよい，汎化してくれればよいというならそれでも構わないでしょうが，「どのようにしてその出力に至ったのか」を知りたい場合には困ります．\n",
        "今回紹介する **決定木** (decision tree)は，そういう場合によく使われる学習の手法です．\n"
      ],
      "metadata": {
        "id": "qOGaZ9WUEl5I"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glgWbdlh34P"
      },
      "source": [
        "----\n",
        "### 準備\n",
        "\n",
        "\n",
        "\n",
        "以下，コードセルを上から順に実行してながら読んでいってね．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBTXo5eHh34P"
      },
      "outputs": [],
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn import tree # 機械学習ライブラリ scikit-learn の決定木パッケージ\n",
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.colors as colors\n",
        "import seaborn\n",
        "seaborn.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 例題: ラーメン？アイス？"
      ],
      "metadata": {
        "id": "m_05KKmfwFQ_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "決定木の説明のための例題を用意します．"
      ],
      "metadata": {
        "id": "P-AmCU9NCRGc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの入手\n",
        "df = pd.read_csv('https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ramenice.csv')\n",
        "df"
      ],
      "metadata": {
        "id": "-5nFrG_ywpOV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "このデータは，ほげおくんが $(気温, 湿度)$ のときに「アイスクリーム」を食べたか「ラーメン」を食べたかを記録したものです．$(気温, 湿度)$ を入力として「アイスクリーム」と「ラーメン」の2クラスに識別する問題と考えることができます．\n",
        "\n",
        "散布図を描くと次のようになります．"
      ],
      "metadata": {
        "id": "goVX8Ln9CZ-R"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの準備\n",
        "X = np.empty((len(df), 2))\n",
        "X[:, 0] = df['気温'].to_numpy()\n",
        "X[:, 1] = df['湿度'].to_numpy()\n",
        "df['label'] = 0\n",
        "df.loc[df['ラベル'] == 'アイスクリーム', 'label'] = 1\n",
        "lab = df['label'].to_numpy()\n",
        "print(X.shape, lab.shape)\n",
        "\n",
        "# 散布図の描画\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(6, 4.5))\n",
        "ax.set_xlim(0, 40)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.scatter(X[lab == 1, 0], X[lab == 1, 1], label='icecream') # blue\n",
        "ax.scatter(X[lab == 0, 0], X[lab == 0, 1], label='ramen') # orange\n",
        "ax.set_xlabel('Temparature', fontsize=16)\n",
        "ax.set_ylabel('Humidity', fontsize=16)\n",
        "ax.legend(fontsize=16)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uLg8zK7HyDKP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 決定木とは\n"
      ],
      "metadata": {
        "id": "uyWss1-izFXS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**決定木**(Decision Tree) とは，その名の通り，木構造を利用した機械学習の仕組みです．識別にも回帰に用いることができますが，ここでは識別の例で説明します．\n",
        "\n",
        "識別のための決定木は，木構造の一種です．\n",
        "下図左のように，葉以外のノード（節点，図の灰色の箱）にデータの値に関する条件が設定されており，葉ノード（オレンジ／青の箱）に識別対象のクラスが割り当てられています．\n",
        "データが与えられたら，根ノードから子ノードを順次たどっていき，到達した葉ノードに割り当てられたクラスを出力します．\n",
        "\n",
        "図の例で $(気温[度],湿度[\\%]) = (15, 60)$ というデータを識別する場合，根ノードから左の子へとたどり，そこから右の子へとたどって「アイス」と識別されます．\n"
      ],
      "metadata": {
        "id": "K23PP0LbFHR3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"100%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/decisiontree.png\">\n",
        "\n"
      ],
      "metadata": {
        "id": "G31eDjKuvwTK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "典型的な決定木は，上図左のように，ノードごとに入力データの変数のどれか一つ（例では「気温」か「湿度」のどちらか）に対する条件があり，データが条件を満たすか否かに応じた二つの子ノードを持つ二分木となっています（注）．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 決定木には様々なバリエーションがあり，複数の特徴量を組み合わせた条件を用いるものや，3つ以上の子に分岐するものなどもあります．\n",
        "</span>\n",
        "\n",
        "この図の決定木の例では，各ノードの条件によって入力の空間が上図右のように4つの領域に分けられ，それらが「アイス」「ラーメン」のどちらかに割り当てられることになります．\n"
      ],
      "metadata": {
        "id": "0_JYU9JAMFot"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "決定木では，学習データを用いて木構造を作ります．ノードごとの条件や分岐の仕方を学習データに応じて決めることになります．決定木の学習については後述することにして，以下では上記のデータを用いて実際に決定木を学習させてみます．\n",
        "\n",
        "ここでは，[scikit-learn](https://scikit-learn.org/) という，Python による機械学習のためのライブラリを利用しています．\n",
        "\n"
      ],
      "metadata": {
        "id": "R4i6lbiePRhh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 決定木の学習\n",
        "max_depth = 2\n",
        "model = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
        "model = model.fit(X, lab) # 学習\n",
        "pred = model.predict(X)  # 学習データの識別\n",
        "ncorrect = np.sum(pred == lab)\n",
        "N = X.shape[0]\n",
        "print(f'木の深さの最大値: {max_depth}  学習データの識別率: {ncorrect}/{N} = {ncorrect/N}')\n",
        "\n",
        "# 得られた決定木の描画\n",
        "fig = plt.figure(facecolor='white', figsize=(8, 6))\n",
        "ax = fig.add_subplot(111)\n",
        "rv = tree.plot_tree(model, ax=ax, feature_names=['Temp', 'Hum'], class_names=['RAMEN', 'ICE'], filled=True)"
      ],
      "metadata": {
        "id": "qk_gI_-56F7r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上の図は，学習させて得られた決定木を可視化したものです．\n",
        "それぞれの箱がノードを表します．葉でないノードについては，箱の中の一番上の行に条件が記されています．`Temp` は気温，`Hum` は湿度を表しています．\n",
        "また，葉ノードの `class` はそのノードに割り当てられたクラスを表しています．箱の色については，次の節で説明します．\n",
        "\n",
        "例えば，$(気温,湿度) = (28, 30)$ の場合，次のように木をたどって識別結果を得ることができます．\n",
        "\n",
        "1. 根ノードの条件: `Hum <= 36.55` （$\\mbox{(湿度)} \\leq 36.55\\mbox{[%]}$ ）を満たすので，左の子ノードへ\n",
        "1. そのノードの条件: `Temp <= 22.6` （$\\mbox{(気温)} \\leq 22.6\\mbox{[度]}$ ）を満たさないので，右の子ノードへ\n",
        "1. 葉ノードへ到達．このノードは `class = ICE` なので，このデータは「アイス」と予測．\n"
      ],
      "metadata": {
        "id": "6Ob1vLxCSAJd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ★★ やってみよう ★★\n",
        "\n",
        "上記のコードを実行して得られる決定木で，次のデータはどのように識別されるか考えて，紙媒体にメモしておきましょう．\n",
        "\n",
        "- $(気温,湿度) = (26, 60)$\n",
        "- $(気温,湿度) = (28, 38)$\n",
        "\n"
      ],
      "metadata": {
        "id": "_oddkBD098CY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下に，上記の学習によって得られた決定木の作る識別境界を示します．この決定木では入力の特徴量ごとに条件を定めていますので，識別の境界は必ず入力空間のどれか一つの軸に平行となります．斜めになったり曲がったりすることはありません．"
      ],
      "metadata": {
        "id": "TCmCTxJjTDMD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 識別境界の描画\n",
        "fig = plt.figure(facecolor='white', figsize=(8, 6))\n",
        "xm, ym = np.meshgrid(np.linspace(0, 40, num=100), np.linspace(0, 100, num=100))\n",
        "Xm = np.vstack((xm.ravel(), ym.ravel())).T\n",
        "pred = model.predict(Xm).reshape(xm.shape)\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlim(0, 40)\n",
        "ax.set_ylim(0, 100)\n",
        "ax.scatter(X[lab == 1, 0], X[lab == 1, 1], label='icecream') # blue\n",
        "ax.scatter(X[lab == 0, 0], X[lab == 0, 1], label='ramen') # orange\n",
        "ax.set_xlabel('Temparature', fontsize=16)\n",
        "ax.set_ylabel('Humidity', fontsize=16)\n",
        "ax.legend()\n",
        "cmap = colors.ListedColormap(['orange','blue'])\n",
        "ax.contourf(xm, ym, pred, cmap=cmap, alpha=0.2)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "X6q5j6yEACKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 決定木の学習の考え方\n"
      ],
      "metadata": {
        "id": "w2b0RseGBW0_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "決定木には様々なバリエーションがあります．ここではその代表的な学習法のひとつを説明します．\n",
        "\n",
        "一般的に，決定木の学習では，根ノードに全ての学習データが割り当てられた状態からはじめて，学習データを二つに分けて二つの子ノードに割り振ることを繰り返して，木を成長させていきます．以下にその手続きの概略を示します．\n",
        "\n",
        "1. 全ての学習データを根ノードに割り当てる．このノードを「現在のノード」とする．\n",
        "1. 「現在のノード」に子ノードを作るかどうか判断する：\n",
        "    - 子ノードを作る場合：現在のノードに割り当てられたデータを「最もうまく」二つに分ける条件を求める．二つの子ノードを生成し，その条件にしたがってデータを子ノードに割り振る．\n",
        "    - 子ノードを作らない場合： 「現在のノード」に割り当てられたデータからこのノードのクラスを決める．\n",
        "1. 子ノードを作った場合，左右の子ノードをあらためて「現在のノード」として，2. と 3. を繰り返す\n",
        "\n",
        "ここで，\n",
        "\n",
        "> 「現在のノード」に子ノードを作るかどうか判断する\n",
        "\n",
        "ための基準としては，「そのノードに割り当てられるデータの数が一定以上になるようにする」，「木の深さ（根からそのノードまでにたどる枝の数）が一定以下になるようにする」，といったものが用いられます．\n",
        "複雑すぎる決定木は過適合しやすくなりますので，決定木の学習ではこのような基準をうまく設定してやる必要があります．\n",
        "\n",
        "また，ノードに学習データを振り分けていきますので，それらの所属クラスをなるべく偏らせて，葉ノードにはどれか一つのクラスのデータのみ含まれるようにするのが理想です．したがって，\n",
        "\n",
        "> 現在のノードに割り当てられたデータを「最もうまく」二つに分ける条件を求める\n",
        "\n",
        "の「最もうまく」の基準はそのように（ノードに振り分けられるデータの所属クラスがなるべく偏るように）定められます（下図参照）．\n",
        "\n",
        "<img width=\"50%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/decisiontree2.png\">"
      ],
      "metadata": {
        "id": "81LjlQITWFR8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "先の例題をもう一度使って学習の様子を見てみましょう．\n",
        "実は，前述の学習の実験では木の深さの最大値を 2 に限定していました．これを変えてみましょう．\n",
        "\n"
      ],
      "metadata": {
        "id": "UqyzhbIsU0ar"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 決定木の学習\n",
        "#@markdown 「ラーメン？アイス？」データの識別．\n",
        "#@markdown 以下の `max_depth` は生成する決定木の深さの最大値を指定します．\n",
        "max_depth =  2#@param [2, 3, 4, 5] {type: 'raw', allow-input: true}\n",
        "\n",
        "# 決定木の学習\n",
        "model = tree.DecisionTreeClassifier(max_depth=max_depth)\n",
        "model = model.fit(X, lab) # 学習\n",
        "pred = model.predict(X)  # 学習データの識別\n",
        "ncorrect = np.sum(pred == lab)\n",
        "N = X.shape[0]\n",
        "print(f'木の深さの最大値: {max_depth}  学習データの識別率: {ncorrect}/{N} = {ncorrect/N}')\n",
        "\n",
        "fig = plt.figure(facecolor='white', figsize=(8, 12))\n",
        "\n",
        "# 得られた決定木の描画\n",
        "ax0 = fig.add_subplot(211)\n",
        "rv = tree.plot_tree(model, ax=ax0, feature_names=['Temp', 'Hum'], class_names=['RAMEN', 'ICE'], filled=True)\n",
        "\n",
        "# 識別境界の描画\n",
        "xm, ym = np.meshgrid(np.linspace(0, 40, num=100), np.linspace(0, 100, num=100))\n",
        "Xm = np.vstack((xm.ravel(), ym.ravel())).T\n",
        "pred = model.predict(Xm).reshape(xm.shape)\n",
        "ax1 = fig.add_subplot(212)\n",
        "ax1.set_xlim(0, 40)\n",
        "ax1.set_ylim(0, 100)\n",
        "ax1.scatter(X[lab == 1, 0], X[lab == 1, 1], label='icecream') # blue\n",
        "ax1.scatter(X[lab == 0, 0], X[lab == 0, 1], label='ramen') # orange\n",
        "ax1.set_xlabel('Temparature', fontsize=16)\n",
        "ax1.set_ylabel('Humidity', fontsize=16)\n",
        "ax1.legend()\n",
        "cmap = colors.ListedColormap(['orange', 'blue'])\n",
        "ax1.contourf(xm, ym, pred, cmap=cmap, alpha=0.2)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "Up8_BOnB0xyV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "決定木を可視化した図は，次のようになっています．\n",
        "\n",
        "- 各ノードの箱の中の `samples` の数は，そのノードに割り当てられた学習データの数を表す\n",
        "- `value` の2つの数は，それらのデータのうち「アイス」クラスと「ラーメン」クラスに所属するものの数を表す\n",
        "- 箱の色は，そのノードに所属するデータのクラスを表す（青がアイスでオレンジがラーメン）．色の濃さは，そのノードに属するデータの所属クラスの偏りに対応しており，一クラスのみだと濃く，混じっていると薄くなっている．\n",
        "\n",
        "実験結果を観察すると，深いノードには少数の学習データしか割り当てられておらず，葉ノードに向かうにつれてクラスの偏りが大きくなっていることがわかります．\n",
        "木の深さが小さく限定されていると葉ノードに2クラスのデータが混ざっていますが，木の深さが大きくなるとほとんどの葉ノードにどちらか一つのクラスのデータしかいなくなっています．\n",
        "\n"
      ],
      "metadata": {
        "id": "8B1IrNqhhzAT"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "fwoOKnJbqXis"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2023/ex06notebookB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex06notebookB\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2023)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## ニューラルネットワークと深層学習 (3) + 汎化と過適合 (2)\n",
        "----\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glgWbdlh34P"
      },
      "source": [
        "----\n",
        "### 準備\n",
        "\n",
        "以下，コードセルを上から順に実行してながら読んでいってね．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBTXo5eHh34P"
      },
      "outputs": [],
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "# 機械学習ライブラリ scikit-learn のほげ\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 階層型ニューラルネットワークによる識別"
      ],
      "metadata": {
        "id": "eJsCdU1jqXi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "前の notebook では，階層型ニューラルネットワークの典型的な学習の方法について説明しました．\n",
        "中間層が一つで損失関数が二乗誤差の場合について，ネットワークの入出力や学習アルゴリズムを具体的に式で表し，非線形回帰の問題への適用例を示しました．\n",
        "ここでは，別の例として，識別問題への適用を考えます．\n",
        "\n"
      ],
      "metadata": {
        "id": "DmRQZqzC1VnX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "階層型ニューラルネットを識別問題に適用する場合，出力層のニューロンの活性化関数に softmax関数を用い，損失関数に交差エントロピーを用いるのが一般的です（注）．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※ 注: 「softmax関数」や「交差エントロピー」は，「ロジスティック回帰＋勾配法によるパラメータの最適化 (3)」で登場しました．\n",
        "</span>"
      ],
      "metadata": {
        "id": "eJEFe_do4mz4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 例: 中間層が一つで損失関数が交差エントロピーの場合\n",
        "\n",
        "<img width=\"40%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/neuralnet6.png\" align=\"right\">\n",
        "\n",
        "ここでは，図のように，中間層を一つもち，入力が$D$次元，中間層および出力層のニューロン数がそれぞれ $H$，$K$ のニューラルネットワークを考えます．$K$クラスの識別問題が対象で，出力層の $k$ 番目のニューロンの出力 $z_k$ は，入力データが $k$ 番目のクラスのものである確率を表します．\n",
        "\n",
        "このネットワークの出力 $z_k$ は次式で表わされます．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "y_h &= \\sigma\\left( v_{h,0} + \\sum_{d=1}^{D}v_{h,d}x_d\\right) & (h = 1, 2, \\ldots, H) \\\\\n",
        "z_k &= \\frac{\\displaystyle\\exp\\left(w_{k,0} + \\sum_{h=1}^{H}w_{k,h}y_h \\right)}{\\displaystyle\\sum_{m=1}^{K}\\exp\\left(w_{m,0} + \\sum_{h=1}^{H}w_{m,h}y_h \\right)} & (k=1,2,\\ldots,K) \n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "中間層ニューロンの活性化関数（式の中の $\\sigma(\\cdot)$）としては，ReLU関数やシグモイド関数などが用いられます（注）．また，出力層の活性化関数は，式が示す通り softmax 関数です．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注:「ニューラルネットワークと深層学習 (1)」参照．\n",
        "</span>\n",
        "\n",
        "学習データが $N$ 個与えられるとして，出力の正解を $\\widetilde{z}_{n,k}$ ($n=1,2,\\ldots,N$) とおくとき，損失関数 $L$ は，出力とその正解の間の交差エントロピー\n",
        "\n",
        "$$\n",
        "L = -\\sum_{n=1}^{N}\\sum_{k=1}^{K}\\widetilde{z}_{n,k}\\log{z_{n,k}}\n",
        "$$\n",
        "\n",
        "とします．以下，導出は省略しますが，前の notebook の二乗誤差損失関数の場合と同様にして，パラメータ $v_{h,d}, w_{k,h}$ に関する $L$ の勾配を求めることができます．\n"
      ],
      "metadata": {
        "id": "kg3gAk73PC1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 具体例: 2次元データの識別\n",
        "\n"
      ],
      "metadata": {
        "id": "QwiydHgvxvXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 問題設定\n",
        "\n",
        "2次元のデータを2クラスに分ける識別問題の例で，実際にニューラルネットを学習させる実験を行ってみましょう．"
      ],
      "metadata": {
        "id": "1HbH-THJBTIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データの生成\n",
        "NL, NT = 200, 2000\n",
        "Xraw, Yraw = make_moons(n_samples=NL+NT, noise=0.3, random_state=2929)\n",
        "XL, YL = Xraw[:NL], Yraw[:NL] # 学習データ  NL 個\n",
        "XT, YT = Xraw[NL:], Yraw[NL:] # テストデータ NT 個\n",
        "\n",
        "# 識別境界描画用データ\n",
        "xmin, xmax = -2, 3\n",
        "ymin, ymax = -1.5, 2\n",
        "xx, yy = np.mgrid[xmin:xmax:0.02, ymin:ymax:0.02]\n",
        "XX = np.dstack((xx, yy))\n",
        "\n",
        "# データの散布図\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 8))\n",
        "for i, s in enumerate(['Learning Data', 'Test Data']):\n",
        "    if i == 0:\n",
        "        X0, X1 = XL[YL == 0, :], XL[YL == 1, :]\n",
        "    else:\n",
        "        X0, X1 = XT[YT == 0, :], XT[YT == 1, :]\n",
        "    ax[i].scatter(X0[:, 0], X0[:, 1], label='Class0', marker='.')\n",
        "    ax[i].scatter(X1[:, 0], X1[:, 1], label='Class1', marker='.')\n",
        "    ax[i].set_xlim(xmin, xmax)\n",
        "    ax[i].set_ylim(ymin, ymax)\n",
        "    ax[i].set_aspect('equal')\n",
        "    ax[i].legend()\n",
        "    ax[i].set_title(s)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9-dKPzZ9ysW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上図の左が学習データ，右がテストデータの散布図です．"
      ],
      "metadata": {
        "id": "ps5qVTdBbgNq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 実験\n",
        "\n",
        "上記のデータをニューラルネットに学習させてみましょう．\n",
        "ネットワークの構造は，上で説明した中間層が一つのもので，中間層のニューロン数を $H = 1000$ とします．また，比較のためにロジスティック回帰もやってみます．\n",
        "\n",
        "ここでは，[scikit-learn](https://www.scikit-learn.org/) という機械学習ライブラリ用いています．"
      ],
      "metadata": {
        "id": "lifEgFfDBdNL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### 2次元データの識別\n",
        "#@markdown `useNN` が `True`（チェックがついている） だとニューラルネット，\n",
        "#@markdown `False`だとロジスティック回帰\n",
        "\n",
        "useNN = True #@param {type: 'boolean'}\n",
        "\n",
        "# モデルの準備\n",
        "if useNN:\n",
        "    model = MLPClassifier(hidden_layer_sizes=(1000,), activation='relu', max_iter=500, alpha=0.0)\n",
        "else:\n",
        "    model = LogisticRegression()\n",
        "\n",
        "# 学習\n",
        "model.fit(XL, YL)\n",
        "Yt = model.predict(XL)\n",
        "ncL = np.sum(Yt == YL) \n",
        "\n",
        "# テスト\n",
        "Yt = model.predict(XT)\n",
        "ncT = np.sum(Yt == YT)\n",
        "P = model.predict_proba(XX.reshape((-1, 2)))\n",
        "\n",
        "# グラフの描画\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.scatter(XL[YL == 0, 0], XL[YL == 0, 1], label='Class0')\n",
        "ax.scatter(XL[YL == 1, 0], XL[YL == 1, 1], label='Class1')\n",
        "pp = P[:, 0].reshape((XX.shape[0], XX.shape[1]))\n",
        "ax.contourf(xx, yy, 1 - pp, cmap='bwr', alpha=0.2)\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# 識別率の出力\n",
        "print(f'学習データの識別率: {ncL}/{NL} = {ncL/NL}')\n",
        "print(f'テストデータの識別率: {ncT}/{NT} = {ncT/NT}')\n"
      ],
      "metadata": {
        "id": "WTdydVHOIarY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "図の色が塗られた領域は，ロジスティック回帰またはニューラルネットが予測した2つのクラスの確率を可視化しています．青色は `Class0` の確率が高いことを，赤色は `Class1` の確率が高いことを示します．\n",
        "\n",
        "ロジスティック回帰では，モデルが予測する2クラスの境界（決定境界）が直線にしかなりませんが，ニューラルネットの場合，中間層によって非線形な変換が行えるおかげで，クラスをよりうまく識別できる複雑な境界を作ることができています．\n",
        "その結果，識別率（識別の正解率）は，学習データについてもテストデータについても，ニューラルネットの方が高くなっています．"
      ],
      "metadata": {
        "id": "TGVVuPhyV6g5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 汎化と過適合(2)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "tc8Qj79XgXwU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"40%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/modeldof.png\" align=\"right\">\n",
        "\n",
        "\n",
        "「汎化と過適合」の回に，多項式当てはめの例を挙げて，次のことを説明しました：\n",
        "- 教師あり学習においては，学習データに対して正解できるだけでなく，学習時に見たことのない未知データに対して **汎化** できることが重要\n",
        "- データ数と比較してパラメータ数が多い複雑な（自由度が過剰な）モデルは汎化性能が低下する **過適合** を起こすことがある\n",
        "\n",
        "階層型ニューラルネットの場合，中間層の数やニューロンの数によってパラメータの数が決まりますので，それらをいい加減に決めてしまうと過適合が起こり得ます．"
      ],
      "metadata": {
        "id": "-w79hsNlGXd3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "試しに，上の具体例のデータを識別するニューラルネットの構造を変えて実験してみましょう．ここでは，次のような条件を考えます．\n",
        "\n",
        "- 条件0: 中間層1層でニューロン数1000（上の実験で用いたのと同じ），パラメータ数は約4千（注）\n",
        "- 条件1: 中間層2層でニューロン数はそれぞれ1000，1000，パラメータ数は約100万\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※ 注: $v_{h,d}$ が $h = 1, 2, \\ldots, H, d = 0, 1, \\ldots, D$ で $H(D+1)$ 個，$w_{k,h}$ が $k = 1, 2, \\ldots, K, h = 0, 2, \\ldots, H$ で $K(H+1)$個の合計 $H(D+1)+K(H+1)$ 個．\n",
        "</span>"
      ],
      "metadata": {
        "id": "PqeVi01RK514"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデルの準備\n",
        "\n",
        "#@title #### 汎化と過適合\n",
        "#@markdown 0: 中間層1層でニューロン数1000\n",
        "\n",
        "#@markdown 1: 中間層2層でニューロン数1000,1000\n",
        "\n",
        "#@markdown 2: 中間層3層でニューロン数100,100,100\n",
        "\n",
        "\n",
        "modelType = 1 #@param [0, 1, 2] {type: 'raw'}\n",
        "\n",
        "archList = [\n",
        "    (1000,),\n",
        "    (1000, 1000),\n",
        "    (100, 100, 100),\n",
        "]\n",
        "arch = archList[modelType]\n",
        "\n",
        "# モデルの準備\n",
        "model = MLPClassifier(hidden_layer_sizes=arch, activation='relu', max_iter=500, alpha=0.0)\n",
        "\n",
        "# 学習\n",
        "model.fit(XL, YL)\n",
        "Yt = model.predict(XL)\n",
        "ncL = np.sum(Yt == YL) \n",
        "\n",
        "# テスト\n",
        "Yt = model.predict(XT)\n",
        "ncT = np.sum(Yt == YT)\n",
        "P = model.predict_proba(XX.reshape((-1, 2)))\n",
        "\n",
        "# グラフの描画\n",
        "fig, ax = plt.subplots(figsize=(8, 8))\n",
        "ax.scatter(XL[YL == 0, 0], XL[YL == 0, 1], label='Class0')\n",
        "ax.scatter(XL[YL == 1, 0], XL[YL == 1, 1], label='Class1')\n",
        "pp = P[:, 0].reshape((XX.shape[0], XX.shape[1]))\n",
        "ax.contourf(xx, yy, 1 - pp, cmap='bwr', alpha=0.2)\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "# 識別率の出力\n",
        "print(f'モデルの構成: {arch}')\n",
        "print(f'学習データの識別率: {ncL}/{NL} = {ncL/NL}')\n",
        "print(f'テストデータの識別率: {ncT}/{NT} = {ncT/NT}')\n"
      ],
      "metadata": {
        "id": "2P6EWhBNOjV2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "条件0と条件1の結果を比較すると，より複雑なモデルを用いる条件1の方が得られる境界が複雑で，学習データの識別率が高くなっています．しかし，テストデータの識別率は条件1の方が低くなっており，過適合が疑われる結果となっています（注）．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 階層型ニューラルネットの学習では最適解が得られる保証がありませんので，パラメータの初期値によってたまたま良い解に到達できなかったという場合もあり得ます．上のようにそれぞれの条件で1回ずつだけの実験ではその辺りの調査が不十分です．本当は，それぞれの条件でパラメータの初期値を何通りか変えて複数回学習させた結果を考察すべきところです．\n",
        "</span>\n"
      ],
      "metadata": {
        "id": "feg6uNLONmyN"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
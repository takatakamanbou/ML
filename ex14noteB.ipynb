{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2022/ex14noteB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex14noteB\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2022)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "# 正規分布の確率密度関数\n",
        "from scipy.stats import norm\n",
        "from scipy.stats import multivariate_normal"
      ],
      "metadata": {
        "id": "Uba-rqwa906l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## 統計的パターン認識入門\n",
        "----\n",
        "\n",
        "前回登場した密度推定は，教師なし学習の一種とみなせるのでした．\n",
        "この密度推定を教師あり学習の識別の問題へと応用することを考えます．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 統計的パターン認識とは\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "fzhGFh4epP9F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "識別に関する研究分野や手法のうち，特に画像や音声のような「パターン情報」を対象とするものを，**パターン認識**（pattern recognition）といいます．今回扱う**統計的パターン認識**(statistical pattern recognition)は，その中でも，データ $\\mathbf{x}$ やそのクラスラベル $y$ を確率変数とみなし，写像 $\\mathbf{x} \\rightarrow y$ を確率的なものとして扱う方法です．\n",
        "\n"
      ],
      "metadata": {
        "id": "7atlel3LXGrE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "画像の画素値を入力すると，それらを「ネコ」「イヌ」「カピバラ」の3クラスに分類する識別問題を考えてみましょう．入力を連続型確率変数として $\\mathbf{x}$ と表し，3つのクラスを離散型確率変数 $y$ で表します\n",
        "（$y \\in \\{ \\mbox{ネコ}, \\mbox{イヌ}, \\mbox{カピバラ} \\}$）．\n",
        "このとき，$\\mathbf{x}$の値が与えられたときの3つの $y$ の確率，つまり条件付き確率 $p(y|\\mathbf{x})$ の値が\n",
        "$$\n",
        "p(\\mbox{ネコ}|\\mathbf{x}) = 0.2,\\quad\n",
        "p(\\mbox{イヌ}|\\mathbf{x}) = 0.75,\\quad\n",
        "p(\\mbox{カピバラ}|\\mathbf{x}) = 0.05\n",
        "$$\n",
        "のように求められれば，「この確率が最大なのは「イヌ」なので，$\\mathbf{x}$は「イヌ」に識別する」といったことが可能になります．\n",
        "これが，統計的パターン認識の基本的な考え方です．"
      ],
      "metadata": {
        "id": "Ctw_WVnnZ_rU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで登場した条件付き確率 $p(y|\\mathbf{x})$ は，「クラス事後確率」と呼ばれます．詳しくは説明しませんが，ロジスティック回帰は，このクラス事後確率をモデル化して，$\\mathbf{x}$はそのクラス事後確率が最大のクラスに識別する，つまり\n",
        "$$\n",
        "(\\mbox{予測クラス}) = \\arg\\max_y p(y|\\mathbf{x})\n",
        "$$\n",
        "というルールを用いる統計的パターン認識の手法となっています．\n",
        "一方，統計的パターン認識の手法の中には，ロジスティック回帰のように事後確率を直接モデル化する代わりに，**ベイズの公式**\n",
        "$$\n",
        "p(\\mathbf{x},y) = p(\\mathbf{x}|y)p(y) = p(y|\\mathbf{x})p(\\mathbf{x})\n",
        "$$\n",
        "を使って\n",
        "$$\n",
        "\\arg\\max_y p(y|\\mathbf{x}) = \\arg\\max_y \\frac{p(\\mathbf{x}|y)p(y)}{p(\\mathbf{x})} = \\arg\\max_y p(\\mathbf{x}|y)p(y)\n",
        "$$\n",
        "と変形できることを利用して，$p(\\mathbf{x}|y)$ と $p(y)$ を計算することで識別をする手法もあります（注）.\n",
        "\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: この式の最後の変形では，$p(\\mathbf{x})$ が $y$ によらないことを利用してこれを無視しています．クラス事後確率を直接モデル化するやり方は「識別モデル」(discriminative model)による手法，$p(\\mathbf{x}|y)$ と $p(y)$ から求める手法は「生成モデル」(generative model)による手法と呼ばれます．\n",
        "</span>"
      ],
      "metadata": {
        "id": "GmwkmCwfzrvt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下では，$p(\\mathbf{x}|y)$ と $p(y)$ を計算することで識別をする方法について説明します．こちらの方法では，それぞれ，学習データから次のようにして定めるのが一般的です．\n",
        "\n",
        "<dl>\n",
        "<dt>$p(y)$</dt>\n",
        "<dd>どんな$\\mathbf{x}$が来るか知る前の時点でのクラス $y$ の出現確率．「クラス事前確率」と呼ぶ．学習データ中の各クラスの出現頻度をそのまま使ったり，単純に1/クラス数の値を使ったりする．</dd>\n",
        "<dt>$p(\\mathbf{x}|y)$</dt>\n",
        "<dd>\n",
        "クラス$y$に所属するデータ$\\mathbf{x}$の分布．学習データをクラス毎に分けて，それぞれで密度推定を行うことで求められる．\n",
        "</dd>\n",
        "</dl>"
      ],
      "metadata": {
        "id": "PEwvN74ig58n"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### ［実験］2次元3クラスのデータを識別してみる\n"
      ],
      "metadata": {
        "id": "DuFKWzx_pqRh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のようなデータを対象にして，$p(\\mathbf{x}|y)$ を2次元正規分布でモデル化する方法で識別する実験をやってみましょう.\n"
      ],
      "metadata": {
        "id": "iKwzBjF_pzkm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 2次元3クラスのデータ\n",
        "df = pd.read_csv('https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/2dim3class2.csv', header=0)\n",
        "X = df.loc[:, ['x1', 'x2']].to_numpy()\n",
        "N, D = X.shape\n",
        "lab = df['label'].to_numpy()\n",
        "K = 3\n",
        "\n",
        "# 散布図を描く\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(8, 8))\n",
        "ax.scatter(X[lab==0, 0], X[lab==0, 1], label='class0')\n",
        "ax.scatter(X[lab==1, 0], X[lab==1, 1], label='class1')\n",
        "ax.scatter(X[lab==2, 0], X[lab==2, 1], label='class2')\n",
        "ax.set_xlim(-5, 5)\n",
        "ax.set_ylim(-5, 5)\n",
        "ax.set_aspect('equal')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "UQt37-SPlHb3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルを実行すると，$p(\\mathbf{x}|y)$ およびクラス事後確率 $p(y|\\mathbf{x})$ それぞれの推定結果を可視化します．"
      ],
      "metadata": {
        "id": "CB01XTR4qFRH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 事前確率を推定\n",
        "py = np.empty(K)\n",
        "for k in range(K):\n",
        "    py[k] = np.sum(lab==k)/N\n",
        "print('### 事前確率')\n",
        "print('p(y) = ', py)\n",
        "\n",
        "# p(x|y) を正規分布でモデル化（平均，共分散行列の推定）\n",
        "mu = np.empty((K, D))\n",
        "cov = np.empty((K, D, D))\n",
        "for k in range(K):\n",
        "    XX = X[lab==k, :]\n",
        "    mu[k] = np.mean(XX, axis=0)\n",
        "    cov[k] = (XX - mu[k]).T @ (XX - mu[k]) / XX.shape[0]\n",
        "    print(f'### class {k} の正規分布のパラメータ')\n",
        "    print(mu[k])\n",
        "    print(cov[k])\n",
        "\n",
        "# グラフ描画用のグリッドデータの作成\n",
        "xmin, xmax = -5, 5\n",
        "ymin, ymax = -5, 5\n",
        "x_mesh, y_mesh = np.mgrid[xmin:xmax:0.02, ymin:ymax:0.02]\n",
        "X_mesh = np.dstack((x_mesh, y_mesh))\n",
        "\n",
        "# 事後確率の推定\n",
        "p = np.empty((K, X_mesh.shape[0]*X_mesh.shape[1]))\n",
        "for k in range(K):\n",
        "    p[k, :] = py[k] * multivariate_normal.pdf(X_mesh, mean=mu[k], cov=cov[k]).reshape(-1)\n",
        "p /= np.sum(p, axis=0)\n",
        "pp = p.reshape((K, X_mesh.shape[0], X_mesh.shape[1]))\n",
        "\n",
        "# グラフ\n",
        "fig = plt.figure(facecolor=\"white\", figsize=(12, 8))\n",
        "\n",
        "# Gaussian を当てはめた結果\n",
        "ax0 = fig.add_subplot(121)\n",
        "for k in range(K):\n",
        "    ax0.scatter(X[lab==k, 0], X[lab==k, 1])\n",
        "    ax0.contour(x_mesh, y_mesh, multivariate_normal.pdf(X_mesh, mean=mu[k], cov=cov[k]))\n",
        "ax0.set_xlim(xmin, xmax)\n",
        "ax0.set_ylim(ymin, ymax)\n",
        "ax0.set_aspect('equal')\n",
        "\n",
        "# 事後確率の可視化\n",
        "cmap = ['Blues', 'Oranges', 'Greens']\n",
        "ax1 = fig.add_subplot(122)\n",
        "for k in range(K):\n",
        "    ax1.scatter(X[lab==k, 0], X[lab==k, 1])\n",
        "    ax1.contourf(x_mesh, y_mesh, pp[k], levels=[0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0], cmap=cmap[k], alpha=0.3)\n",
        "ax1.set_xlim(xmin, xmax)\n",
        "ax1.set_ylim(ymin, ymax)\n",
        "ax1.set_aspect('equal')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "hASQuaBilLQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "左図は，$p(\\mathbf{x}|\\textrm{class0}), p(\\mathbf{x}|\\textrm{class1}), p(\\mathbf{x}|\\textrm{class2})$ をそれぞれ2次元正規分布でモデル化し，そのパラメータを学習データから推定した結果を可視化しています．\n",
        "一方，右図は，$p(\\mathbf{x}|y)$ と $p(y)$ から求めたクラス事後確率 $p(y|\\mathbf{x})$ を可視化しています．青，オレンジ，緑の色が濃いところは，それぞれ $p(\\textrm{class0}|\\mathbf{x}), p(\\textrm{class1}|\\mathbf{x}), p(\\textrm{class2}|\\mathbf{x})$ の値が大きいところです．\n",
        "学習データの分布を反映して，うまくクラス間の境界が作られていることがわかります．"
      ],
      "metadata": {
        "id": "IIrefP4qqZUz"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "rMncTReGmzRc"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
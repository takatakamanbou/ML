{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2024/ML2024_ex04notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex04notebookA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## ロジスティック回帰＋勾配法によるパラメータの最適化 (1) 2クラス識別のロジスティック回帰\n",
        "----\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "これまでに説明した最短距離法や最近傍法とはだいぶん仕組みの異なる識別の手法として，**ロジスティック回帰** (Logistic Regression) を取り上げます．\n",
        "名前に「回帰」とついていますが，回帰／識別という分け方でいえば識別のための手法です．"
      ],
      "metadata": {
        "id": "iywhbTsteIcm"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glgWbdlh34P"
      },
      "source": [
        "----\n",
        "### 準備\n",
        "\n",
        "\n",
        "以下，コードセルを上から順に実行してながら読んでいってね．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBTXo5eHh34P"
      },
      "outputs": [],
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 2クラス識別のためのロジスティック回帰\n",
        "\n"
      ],
      "metadata": {
        "id": "g2To2HYoev2K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "簡単のため，まずは識別すべきクラスが2つしかない場合に限定してロジスティック回帰モデルを説明します．\n",
        "以下，2つのクラスを $0$ と $1$ で表すことにして，$D$ 次元のデータ $\\mathbf{x} = (x_1, x_2, \\ldots, x_D)$ に対する正解クラスを $y\\in \\{0, 1\\}$ で表すことにします．\n",
        "\n"
      ],
      "metadata": {
        "id": "YBVyH4wmhIjK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### シグモイド関数"
      ],
      "metadata": {
        "id": "kCadBqy4wNnx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "というわけでロジスティック回帰モデルの説明に入りたいところですが，まずは準備...．次のような関数を考えます．\n",
        "\n",
        "$$\n",
        "\\sigma(s) = \\frac{1}{1+\\exp{(-s)}}\\qquad (1)\n",
        "$$\n",
        "\n",
        "これは，**シグモイド関数**（sigmoid function）と呼ばれるものです．\n",
        "\n"
      ],
      "metadata": {
        "id": "HtQDwFPMlQzi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# シグモイド関数の値を計算\n",
        "xmin, xmax = -6, 6\n",
        "X = np.linspace(xmin, xmax, num=100)\n",
        "Y = 1/(1+np.exp(-X))\n",
        "\n",
        "# グラフに描く\n",
        "fig = plt.figure(facecolor='white')\n",
        "ax = fig.add_subplot(111)\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(-0.1, 1.1)\n",
        "ax.axhline(y=0, color='black', linestyle='-')\n",
        "ax.axvline(x=0, color='black', linestyle='-')\n",
        "ax.axhline(y=1, color='gray', linestyle='--')\n",
        "ax.plot(X, Y, linewidth=2, label='$\\sigma(s)$')\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "t_NuupWwmGJA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "式からもグラフからもわかるように，シグモイド関数には\n",
        "$$\n",
        "\\lim_{s \\rightarrow -\\infty}\\sigma(s) = 0,\\qquad\n",
        "\\lim_{s \\rightarrow +\\infty}\\sigma(s) = 1,\\qquad\n",
        "\\sigma(0) = \\frac{1}{2}\n",
        "$$\n",
        "という性質があります．"
      ],
      "metadata": {
        "id": "suas8K1SoZ19"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ロジスティック回帰モデル\n",
        "\n",
        "$D$次元のデータ $\\mathbf{x}$ を入力として受け取るロジスティック回帰モデルは，シグモイド関数を使って次のような式で表されます．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(\\mathbf{x}) &= \\sigma(w_0 + w_1x_1+\\cdots + w_Dx_D) = \\sigma\\left(w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\\\\n",
        "&= \\frac{1}{1+\\exp{\\left( - \\left( w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\right)}} \\qquad (2)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "パラメータは，$w_0, w_1, \\ldots, w_D$ の $(D+1)$ 個あります．\n",
        "ちなみに，シグモイド関数の括弧の中は，平面当てはめのモデルと全く同じ式になっています．\n",
        "\n"
      ],
      "metadata": {
        "id": "m6-eacAkfPEY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "シグモイド関数でできていることから分かるように，このモデルの出力は $ 0 < f(\\mathbf{x}) < 1$ を満たします．\n",
        "そこで，出力の値が $0$ に近いか $1$ に近いかによって 2 クラスを識別することができます（典型的には，$\\frac{1}{2}$ との大小を比較します）．"
      ],
      "metadata": {
        "id": "rrduVeWLqncU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 2次元データの例\n",
        "\n",
        "上記の式だけではイメージするのが難しいです．また，$D$次元の話のままではグラフに描いたりするのも困難です．そこで，$D=2$ の場合について，人工的に作ったデータを例にして説明します．"
      ],
      "metadata": {
        "id": "Pv8KKt0Pw1Jr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2次元正規分布で2クラスのデータを生成する関数\n",
        "\n",
        "def getData(seed = None):\n",
        "\n",
        "    if seed != None:\n",
        "        np.random.seed( seed )\n",
        "\n",
        "    # two 2-D spherical Gaussians\n",
        "    X0 = 1.0*np.random.randn(200, 2) + [3.0, 3.0]\n",
        "    X1 = 1.0*np.random.randn(200, 2) + [7.0, 6.0]\n",
        "    X  = np.vstack((X0, X1))\n",
        "    lab0 = np.zeros(X0.shape[0], dtype=int)\n",
        "    lab1 = np.zeros(X1.shape[0], dtype=int) + 1\n",
        "    label = np.hstack((lab0, lab1))\n",
        "\n",
        "    return X, label"
      ],
      "metadata": {
        "id": "l5wCgl12w9aQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "入力データが $(x_1, x_2)$ であるとき，このデータの所属クラスの正解を $y$ とおくと，$y$ は $0$ または $1$ です．\n",
        "以下の左の散布図は，いままで通り $(x_1, x_2)$ 平面に学習データを色分けして描いたものです．青い点は所属クラスが$0$の学習データ，オレンジの点は所属クラスが$1$の学習データです．\n",
        "一方，右の3次元散布図は，$(x_1, x_2, y)$ の値を描いたものです．\n",
        "青い点たちが $y = 0$ の平面上にいるのに対して，オレンジの点たちは $y=1$ の平面上にいます．そのため，このグラフではオレンジの点たちが浮いています．"
      ],
      "metadata": {
        "id": "BK0O91gY0ASQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X, lab = getData()\n",
        "\n",
        "fig = plt.figure(facecolor='white', figsize=(14, 6))\n",
        "\n",
        "# 左の2次元散布図\n",
        "ax0 = fig.add_subplot(121)\n",
        "ax0.set_xlim(0, 10)\n",
        "ax0.set_ylim(0, 10)\n",
        "ax0.set_aspect('equal')\n",
        "ax0.scatter(X[lab == 0, 0], X[lab == 0, 1]) # blue\n",
        "ax0.scatter(X[lab == 1, 0], X[lab == 1, 1]) # orange\n",
        "ax0.set_xlabel('$x_1$')\n",
        "ax0.set_ylabel('$x_2$')\n",
        "\n",
        "# 右の3次元散布図\n",
        "elevation = 20\n",
        "azimuth = -70\n",
        "ax1 = fig.add_subplot(122, projection='3d')\n",
        "ax1.scatter(X[lab==0, 0], X[lab==0, 1], 0) # blue\n",
        "ax1.scatter(X[lab==1, 0], X[lab==1, 1], 1) # orange\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.set_ylim(0, 10)\n",
        "ax1.view_init(elevation, azimuth)\n",
        "ax1.set_xlabel('$x_1$')\n",
        "ax1.set_ylabel('$x_2$')\n",
        "ax1.set_zlabel('$y$')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "gJqxsLwsyEsx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に，これらの学習データを用いてロジスティック回帰の学習を行った結果を示します（学習をどのように行うか，パラメータをどのように決めるか，については後で説明します）．\n",
        "緑の網目で表された曲面は，様々な $(x_1, x_2)$ の値に対して計算したモデルの出力を表しています．\n",
        "学習データの青い点，オレンジの点のそれぞれにうまく当てはまる形をしていることがわかります．\n",
        "また，以下のセルをそのまま実行した場合，$(x_1, x_2) = (4, 4)$ という入力に対する出力の値を計算して表示させるとともに，赤い点として描画しています．\n",
        "出力の値は $0.285$ であり，このデータは $y = 0$ の方のクラス（青い方）に識別されると考えられます．"
      ],
      "metadata": {
        "id": "ooRniBcg14rq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (x1, x2) の値をいろいろ変えてセルを実行してみよう\n",
        "\n",
        "w0, w1, w2 = -8.2, 1.1, 0.72\n",
        "\n",
        "x1 = 4.0 #@param {type: 'number'}\n",
        "x2 = 4.0 #@param {type: 'number'}\n",
        "\n",
        "# 入力 (x1, x2) に対する出力を計算\n",
        "z = 1.0/(1+np.exp(-(w0 + w1*x1 + w2*x2)))\n",
        "\n",
        "# 等間隔にデータを作ってモデル出力を計算\n",
        "xx1, xx2 = np.meshgrid(np.linspace(0, 10, num=16), np.linspace(0, 10, num=16))\n",
        "XX = np.vstack((xx1.ravel(), xx2.ravel())).T\n",
        "ZZ = 1.0/(1+np.exp(-(w0 + w1*XX[:, 0] + w2*XX[:, 1]))) # モデル出力を計算\n",
        "zz = ZZ.reshape(xx1.shape)\n",
        "\n",
        "# 3次元プロットの視点の設定\n",
        "elevation = 20 # 上下方向の角度\n",
        "azimuth = -70  # 左右方向の角度\n",
        "\n",
        "# 3次元プロット\n",
        "fig = plt.figure(facecolor='white', figsize=(8, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X[lab==0, 0], X[lab==0, 1], 0)\n",
        "ax.scatter(X[lab==1, 0], X[lab==1, 1], 1)\n",
        "ax.plot_wireframe(xx1, xx2, zz, color='green')\n",
        "ax.scatter(x1, x2, z, s=100, color='red')\n",
        "ax.set_xlim(0, 10)\n",
        "ax.set_ylim(0, 10)\n",
        "ax.view_init(elevation, azimuth)\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$y$')\n",
        "plt.show()\n",
        "\n",
        "# (x1, x2) に対する出力\n",
        "print(f'入力 ({x1}, {x2}) に対する出力: {z:.3f}')"
      ],
      "metadata": {
        "id": "l3YfZqrA3Ckl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ★ やってみよう\n",
        "\n",
        "1. 上のセルの `x1, x2` の値を適当に変えて，どのような識別をしているか観察しなさい．\n",
        "1. 下のセルに以下のパラメータの組のそれぞれを入力して実行し，どの組が最もこのデータをうまく識別できるか考えましょう．\n",
        "これらのうち最もうまく識別できそうなものの記号をノート等（紙媒体）にメモしておこう．\n",
        "    - (a) `(w0, w1, w2) = ( 12,  1.5, -1)`\n",
        "    - (b) `(w0, w1, w2) = (-12,  1.5,  1)`\n",
        "    - (c) `(w0, w1, w2) = ( 12, -1.5, -1)`\n",
        "    - (d) `(w0, w1, w2) = (-12, -1.5,  1)`\n"
      ],
      "metadata": {
        "id": "6J7LiBmt8tFF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title (w0, w1, w2) の値をいろいろ変えてセルを実行してみよう\n",
        "\n",
        "# パラメータの例\n",
        "#w0, w1, w2 = -8.2, 1.1, 0.72\n",
        "\n",
        "w0 =  -8.2#@param {type: 'number'}\n",
        "w1 =  1.1#@param {type: 'number'}\n",
        "w2 =  0.72#@param {type: 'number'}\n",
        "\n",
        "# 等間隔にデータを作ってモデル出力を計算\n",
        "xx1, xx2 = np.meshgrid(np.linspace(0, 10, num=16), np.linspace(0, 10, num=16))\n",
        "XX = np.vstack((xx1.ravel(), xx2.ravel())).T\n",
        "ZZ = 1.0/(1+np.exp(-(w0 + w1*XX[:, 0] + w2*XX[:, 1]))) # モデル出力を計算\n",
        "zz = ZZ.reshape(xx1.shape)\n",
        "\n",
        "# 3次元プロットの視点の設定\n",
        "elevation = 20 # 上下方向の角度\n",
        "azimuth = -70  # 左右方向の角度\n",
        "\n",
        "# 3次元プロット\n",
        "fig = plt.figure(facecolor='white', figsize=(8, 8))\n",
        "ax = fig.add_subplot(111, projection='3d')\n",
        "ax.scatter(X[lab==0, 0], X[lab==0, 1], 0)\n",
        "ax.scatter(X[lab==1, 0], X[lab==1, 1], 1)\n",
        "ax.plot_wireframe(xx1, xx2, zz, color='green')\n",
        "ax.set_xlim(0, 10)\n",
        "ax.set_ylim(0, 10)\n",
        "ax.view_init(elevation, azimuth)\n",
        "ax.set_xlabel('$x_1$')\n",
        "ax.set_ylabel('$x_2$')\n",
        "ax.set_zlabel('$y$')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "qox181OQ_CIt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 交差エントロピー\n"
      ],
      "metadata": {
        "id": "zfBFOOjaoedu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ロジスティック回帰モデルのパラメータはどうやって決めたらよいのでしょうか？\n",
        "最小二乗法の場合，パラメータを決めるための規準として「学習データに対する二乗誤差」を考え，それを最小化するようにパラメータを決めていました．式(2)のロジスティック回帰の場合は，以下に述べる **交差エントロピー** (cross entropy, 注) の最小化を考えます．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: この「交差エントロピー」は，「情報理論」という分野で登場する概念です．\n",
        "二つの確率分布の間の近さを表す尺度です．\n",
        "ロジスティック回帰モデルは確率モデルの一種であり，その出力が確率を表すものとして，その確率分布が正解の確率分布に近づくように学習している，というような話が裏に隠れています．が，この授業ではその辺りのことは説明しません．\n",
        "</span>"
      ],
      "metadata": {
        "id": "-VpYd2bAootI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> $N$ 個のデータ\n",
        "$$\n",
        "(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2), (\\mathbf{x}_N, y_N)\n",
        "$$\n",
        "> が与えられるとする．$\\mathbf{x}_n \\in {\\cal R}^{D}$ は入力データであり，$y_n \\in \\{0, 1\\}$ はこのデータの所属クラスの正解を表す値である（$n=1,2,\\ldots,N$）．\n",
        ">\n",
        "> このとき，式(2)のロジスティック回帰モデルの交差エントロピー $H$ は次式のようになる（注）．\n",
        ">\n",
        "> $$\n",
        "\\begin{aligned}\n",
        "H &= -\\sum_{n=1}^{N} \\left( y_n\\log{f(\\mathbf{x}_n})+(1-y_n)\\log{\\left( 1-f(\\mathbf{x}_n)\\right)} \\right) \\qquad (3)\n",
        "\\end{aligned}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "EpqiVkJ7p9Pc"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "式(3)右辺の $\\sum$ の中のものを $\\ell_n$ とおくと，その値は，\n",
        "\n",
        "$$\n",
        "\\ell_n = \\left\\{ \\begin{array}{ll}\n",
        "\\log{f(\\mathbf{x}_n}) & \\mbox{$y_n = 1$ のとき}\\\\\n",
        "\\log{\\left( 1-f(\\mathbf{x}_n)\\right)} & \\mbox{$y_n = 0$ のとき}\\\\\n",
        "\\end{array} \\right.\n",
        "$$\n",
        "\n",
        "となります．$0 < f(\\mathbf{x}) < 1$ であることに注意して考えると，$\\log{f(\\mathbf{x}})$ の値は，$f(\\mathbf{x})$ が大きくなって $1$ に近づけば近づくほど大きくなることがわかります．一方，$\\log{\\left( 1-f(\\mathbf{x})\\right)}$ の値は，$f(\\mathbf{x})$ が小さくなって $0$ に近づけば近づくほど大きくなります．つまり，$\\ell_n$ の値は，$f(\\mathbf{x}_n)$ が $y_n$ に近くなる（正解に近づく）ほど大きくなります．\n",
        "\n",
        "したがって，交差エントロピー $H$ の値は，個々の学習データに対するモデルの出力 $f(\\mathbf{x}_n)$ が正解 $y_n$ に近づくほど小さくなります（右辺の先頭に負号が付いてることに注意）．\n",
        "ロジスティック回帰では，この交差エントロピーが小さくなるようにパラメータを定めます（どうやって？の話はまた後で）．"
      ],
      "metadata": {
        "id": "rDBchwADv2P0"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### **ロジスティック回帰の問題設定（2クラスの場合）**\n",
        "\n",
        "ここまでの説明をふまえて，2クラス識別のためのロジスティック回帰の問題設定を示しておきます．"
      ],
      "metadata": {
        "id": "40Cu-jzc0jnJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "**［ロジスティック回帰の問題設定（2クラスの場合）］**\n",
        "\n",
        "$D$次元のデータを二つのクラスに識別するモデルを学習させる．学習データは $N$ 個あり，次のように与えられる．\n",
        "\n",
        "$$\n",
        "(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2),\\ldots , (\\mathbf{x}_N, y_N)\n",
        "$$\n",
        "\n",
        "ただし，$\\mathbf{x}_n \\in {\\cal R}^{D}$ はモデルへの入力であり，$y_n \\in \\{0, 1\\}$ はこのデータの所属クラスの正解を表す値である（$n=1,2,\\ldots,N$）．\n",
        "\n",
        "学習モデルは次式で定める．\n",
        "$$\n",
        "f(\\mathbf{x}) = \\frac{1}{1+\\exp{\\left( - \\left( w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\right)}}\n",
        "$$\n",
        "このモデルのパラメータは $w_0, w_1, \\ldots, w_D$ の $(D+1)$ 個ある．\n",
        "\n",
        "このとき，モデルの出力と正解の値との間の「遠さ」を，次式の交差エントロピーで定義する．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "H &= -\\sum_{n=1}^{N} \\left( y_n\\log{f(\\mathbf{x}_n})+(1-y_n)\\log{\\left( 1-f(\\mathbf{x}_n)\\right)} \\right) \\qquad (3)\n",
        "\\end{aligned}\n",
        "$$\n",
        "この $H$ の値がなるべく小さくなるようにパラメータ $w_0, w_1, \\ldots, w_D$ を求めたい．"
      ],
      "metadata": {
        "id": "sw6V34iSqS7t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "最小二乗法のときは，二乗誤差をパラメータで微分して $=0$ とおくことで，最適なパラメータが満たす方程式を導出できました．\n",
        "しかし，上記の場合は，交差エントロピーがより複雑な式をしているため，そのような手は使えません．\n",
        "ではどうするのか，という話はまた後で．"
      ],
      "metadata": {
        "id": "Wx8-AYKwvR0P"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eLuEb17x6Fd1"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
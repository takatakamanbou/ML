{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2024/ML2024_ex06notebookC.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex06notebookC\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## 階層型ニューラルネットで手書き数字識別\n",
        "----\n",
        "\n",
        "手書き数字データの識別をニューラルネットワークにやらせる実験をしましょう．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glgWbdlh34P"
      },
      "source": [
        "----\n",
        "### 準備\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### GPUを利用するようにランタイムのタイプを変更する\n"
      ],
      "metadata": {
        "id": "3xaSo__5tMel"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "この notebook では，通常の Colab Notebook の動かし方では時間がかかる所があります（特に最後の実験）．次のようにしてランタイムのタイプを変更し，より高速な計算ができるようにしましょう．\n",
        "\n",
        "1. メニューの「ランタイム」 > 「ランタイムのタイプを変更」 を選択．\n",
        "1. ポップアップウィンドウが開くので，「ハードウェアアクセラレータ」を「CPU」から「T4 GPU」に変更し，「保存」する\n",
        "1. いつもどおりコードセルを実行する．すでに実行していた場合，「以前のランタイムを削除する」というポップアップウィンドウが現れるので，「OK」を押して，一番最初のコードセルから実行し直ます．\n",
        "\n",
        "Colab Notebook は Linux を OS とする PC （クラウド上の仮想マシン）で実行されます．\n",
        "通常は，その実行は CPU 上で行われますが，上記のように設定を変更することで，一部の計算を GPU にまかせることができるようになります（注）．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 無料でできますが，計算時間等が制限されています．\n",
        "</span>\n",
        "\n",
        "GPU (Graphical Processing Unit) というのは，PCのグラフィックスボード／ビデオカードやゲーム機等に搭載される，画像処理に特化した演算装置です．CPUのような汎用性がない代わりに，特定の処理をCPUよりずっと高速に実行できます．\n",
        "この notebook では PyTorch という深層学習フレームワークを使用しますが， PyTorch ではニューラルネットの出力や学習のための計算を GPU 上で行って高速化することが簡単にできるようになっています．"
      ],
      "metadata": {
        "id": "DHWsvGROH9y5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### いろいろ import"
      ],
      "metadata": {
        "id": "b52pSlrFynUp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBTXo5eHh34P"
      },
      "outputs": [],
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch 関係のほげ\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "from torchvision.datasets import MNIST\n",
        "import torchsummary\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "metadata": {
        "id": "hqKwr1oJD4mn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GPU が使えるようになっていれば，↑のセルを実行すると `cuda:0` と出力されるはずです．\n"
      ],
      "metadata": {
        "id": "lmSbyUjm1MSR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 手書き数字データの入手\n",
        "! wget -nc https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/minimnist.npz\n",
        "minimnist = np.load('minimnist.npz')\n",
        "#datL = rv['datL'].astype(float)\n",
        "#labL = rv['labL']\n",
        "#datT = rv['datT'].astype(float)\n",
        "#labT = rv['labT']\n",
        "#print(datL.shape, labL.shape, datT.shape, labT.shape)\n",
        "\n",
        "K = 10 # クラス数\n",
        "D = minimnist['datL'].shape[1] # データの次元数 28 x 28 = 784"
      ],
      "metadata": {
        "id": "17p1fbo5EhsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 実験1\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "szdqM433mzUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以前にも使ったことのある手書き数字データ（MNISTデータセットの一部）の識別をニューラルネットワークにやらせてみます．学習モデルは，中間層が一つおよび二つのニューラルネットワークと，ロジスティック回帰とします．\n",
        "\n",
        "\n",
        "<img width=\"75%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/neuralnet5.png\">\n",
        "\n",
        "入力の次元数は784で，クラスは0から9までの10通りです．ニューラルネットワークの中間層ニューロン数は全て1000（3層では二つの中間層に1000個ずつ），それらの活性化関数は ReLU としました．出力層の活性化関数は softmax として，損失関数には交差エントロピーを用いました．\n",
        "\n",
        "このような設定ですので，ロジスティック回帰モデルよりも2層ニューラルネットワークの方が，そして，2層よりも3層の方が，パラメータ数の多い複雑な学習モデルとなっています．"
      ],
      "metadata": {
        "id": "sCIJPaPSvSNE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 関数等の定義\n",
        "\n",
        "実験で使う関数等の定義をしておきます．"
      ],
      "metadata": {
        "id": "4Id3X5nHLhwz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データを扱うためのクラス\n",
        "#\n",
        "class MMDataset(Dataset):\n",
        "\n",
        "    def __init__(self, data, LT):\n",
        "        if LT == 'L':\n",
        "            self.X = data['datL'].astype(float) / 255\n",
        "            self.Y = data['labL']\n",
        "        else:\n",
        "            self.X = data['datT'].astype(float) / 255\n",
        "            self.Y = data['labT']\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.Y)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        x = torch.tensor(self.X[idx], dtype=torch.float32)\n",
        "        y = torch.tensor(self.Y[idx], dtype=torch.int64)\n",
        "        return x, y"
      ],
      "metadata": {
        "id": "mMBOc6THELRZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# ネットワークの構造と入出力を定義するクラス\n",
        "#\n",
        "class MLP(nn.Module):\n",
        "\n",
        "    def __init__(self, numNeurons):\n",
        "        super(MLP, self).__init__()\n",
        "        numLayers = len(numNeurons)\n",
        "        assert numLayers >= 2\n",
        "        L = []\n",
        "        # 中間層\n",
        "        for i in range(numLayers-2):\n",
        "            L.append(nn.Linear(numNeurons[i], numNeurons[i+1]))\n",
        "            L.append(nn.ReLU())\n",
        "        # 出力層\n",
        "        L.append(nn.Linear(numNeurons[-2], numNeurons[-1]))\n",
        "        self.layers = nn.ModuleList(L)\n",
        "        self.numNeurons = numNeurons\n",
        "\n",
        "    def forward(self, X):\n",
        "        X = X.reshape((-1, self.numNeurons[0]))\n",
        "        for layer in self.layers:\n",
        "            X = layer(X)\n",
        "        return X\n"
      ],
      "metadata": {
        "id": "_cT2xl5gF6xw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 学習の関数\n",
        "#\n",
        "def train(model, lossFunc, optimizer, dl):\n",
        "    loss_sum = 0.0\n",
        "    ncorrect = 0\n",
        "    n = 0\n",
        "    for i, (X, lab) in enumerate(dl):\n",
        "        X, lab = X.to(device), lab.to(device)\n",
        "        Y = model(X)           # 一つのバッチ X を入力して出力 Y を計算\n",
        "        loss = lossFunc(Y, lab) # 正解ラベル lab に対する loss を計算\n",
        "        optimizer.zero_grad()   # 勾配をリセット\n",
        "        loss.backward()         # 誤差逆伝播でパラメータ更新量を計算\n",
        "        optimizer.step()         # パラメータを更新\n",
        "        n += len(X)\n",
        "        loss_sum += loss.item()  # 損失関数の値\n",
        "        ncorrect += (Y.argmax(dim=1) == lab).sum().item()  # 正解数\n",
        "\n",
        "    return loss_sum/n, ncorrect/n"
      ],
      "metadata": {
        "id": "pyKVTIH5sExA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 損失関数や識別率の値を求める関数\n",
        "#\n",
        "@torch.no_grad()\n",
        "def evaluate(model, lossFunc, dl):\n",
        "    loss_sum = 0.0\n",
        "    ncorrect = 0\n",
        "    n = 0\n",
        "    for i, (X, lab) in enumerate(dl):\n",
        "        X, lab = X.to(device), lab.to(device)\n",
        "        Y = model(X)           # 一つのバッチ X を入力して出力 Y を計算\n",
        "        loss = lossFunc(Y, lab)  # 正解ラベル lab に対する loss を計算\n",
        "        n += len(X)\n",
        "        loss_sum += loss.item() # 損失関数の値\n",
        "        ncorrect += (Y.argmax(dim=1) == lab).sum().item()  # 正解数\n",
        "\n",
        "    return loss_sum/n, ncorrect/n"
      ],
      "metadata": {
        "id": "5EMUiU3jMR2I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 実験"
      ],
      "metadata": {
        "id": "HbcV40O7LqxE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルをそのまま実行しましょう．実行が終わるとグラフが表示されますので，その下のセルの説明を読みましょう．"
      ],
      "metadata": {
        "id": "6lKQr7h9MAqr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データ読み込みの仕組み\n",
        "dsL = MMDataset(minimnist, 'L')\n",
        "dsT = MMDataset(minimnist, 'T')\n",
        "dlL = DataLoader(dsL, batch_size=100, shuffle=False)\n",
        "dlT = DataLoader(dsT, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークモデルの定義\n",
        "#neurons = [784, 10]             # ロジスティック回帰\n",
        "#neurons = [784, 1000, 10]       # 2層（中間層1層）の階層型ニューラルネット\n",
        "neurons = [784, 1000, 1000, 10] # 3層（中間層2層）の階層型ニューラルネット\n",
        "net = MLP(neurons).to(device)\n",
        "\n",
        "# 損失関数（交差エントロピー）\n",
        "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "# パラメータ最適化器\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "# 学習の繰り返し回数\n",
        "nepoch = 50\n",
        "\n",
        "# 学習\n",
        "L = []\n",
        "print('########## 実験1 ##########');\n",
        "print(f'学習データ数: {len(dsL)}  テストデータ数: {len(dsT)}')\n",
        "print()\n",
        "print('# epoch  lossL  lossT  rateL  rateT')\n",
        "for t in range(1, nepoch+1):\n",
        "    lossL, rateL = train(net, loss_func, optimizer, dlL)\n",
        "    lossT, rateT = evaluate(net, loss_func, dlT)\n",
        "    L.append([t, lossL, lossT, rateL, rateT])\n",
        "    if (t < 10) or (t % 10 == 0):\n",
        "        print(f'{t}   {lossL:.5f}   {lossT:.5f}   {rateL:.4f}   {rateT:.4f}')\n",
        "\n",
        "# 学習曲線の表示\n",
        "data = np.array(L)\n",
        "fig, ax = plt.subplots(1, 2, facecolor='white', figsize=(12, 4))\n",
        "ax[0].plot(data[:, 0], data[:, 1], '.-', label='loss for training data')\n",
        "ax[0].plot(data[:, 0], data[:, 2], '.-', label='loss for test data')\n",
        "ax[0].axhline(0.0, color='gray')\n",
        "ax[0].set_ylim(-0.05, 1.0)\n",
        "ax[0].legend()\n",
        "ax[0].set_title(f'loss (network = {neurons})')\n",
        "ax[1].plot(data[:, 0], data[:, 3], '.-', label='accuracy for training data')\n",
        "ax[1].plot(data[:, 0], data[:, 4], '.-', label='accuracy for test data')\n",
        "ax[1].axhline(1.0, color='gray')\n",
        "ax[1].set_ylim(0.8, 1.025)\n",
        "ax[1].legend()\n",
        "ax[1].set_title(f'accuracy (network = {neurons})')\n",
        "plt.show()\n",
        "\n",
        "# 学習後の損失と識別率\n",
        "loss2, rrate = evaluate(net, loss_func, dlL)\n",
        "print(f'# 学習データに対する損失: {loss2:.5f}  識別率: {rrate:.4f}')\n",
        "loss2, rrate = evaluate(net, loss_func, dlT)\n",
        "print(f'# テストデータに対する損失: {loss2:.5f}  識別率: {rrate:.4f}')\n",
        "\n",
        "# torchsummary\n",
        "torchsummary.summary(net, (1, 784))"
      ],
      "metadata": {
        "id": "gGBjwmg62ox3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上のセルをそのまま実行すると，ロジスティック回帰モデルに手書き数字の識別を学習させることができます．\n",
        "\n",
        "```\n",
        "# epoch  lossL  lossT  rateL  rateT\n",
        "1   0.90272   0.44098   0.7610   0.8640\n",
        "2   0.35590   0.33107   0.8924   0.9040\n",
        "```\n",
        "のような出力の各行の数値は，左から学習回数（単位は epoch（注）），学習データに対する損失，テストデータに対する損失，学習データの識別率（正解率），テストデータの識別率です．学習の進み方や汎化の様子を理解しやすくするため，学習中にテストデータを使って損失の値や識別率を算出しています（もちろん，テストデータはパラメータ更新には使っていません）．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 一般に，「全ての学習データを一回ずつ使ってパラメータを更新する作業」相当の学習作業を 1epoch（エポック）とします．1epoch中にパラメータの更新そのものは複数回行われることになるのが普通です．\n",
        "この例では，学習データ数が5000でバッチサイズ 100 の mini-batch 学習を行っているので，1epoch中にパラメータの更新が50回行われています．\n",
        "</span>\n"
      ],
      "metadata": {
        "id": "cDmK3KKx6kpd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2つのグラフは，左が学習回数（横軸）に対する損失の値（縦軸）の変化，右が学習回数に対する識別率の値の変化を表します．\"X for training data\"（青） が学習データに対する値，\"X for test data\"（オレンジ） がテストデータに対する値です．\n",
        "\n",
        "グラフの下の出力のうち，\n",
        "```\n",
        "# 学習データに対する損失: 0.00044  識別率: 1.0000\n",
        "# テストデータに対する損失: 0.25200  識別率: 0.9410\n",
        "```\n",
        "の部分は，学習後のネットワークモデルに学習データおよびテストデータを入力して求めた損失と識別率の値を表します．\n",
        "\n",
        "その下の出力のうち， `Trainable params:` の数は，学習に使用したネットワークモデルに含まれるパラメータの数を表します．\n",
        "\n",
        "セルの内容を変更せずに再度実行すると，パラメータの初期値が変わるので，異なる結果が得られます．何度か実行し直してみましょう．"
      ],
      "metadata": {
        "id": "XBIZmBRUFxhg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次に，ニューラルネットワークを使った実験をしましょう．上のコードセルの中に\n",
        "```\n",
        "# ネットワークモデルの定義\n",
        "neurons = [784, 10]             # ロジスティック回帰\n",
        "#neurons = [784, 1000, 10]       # 2層（中間層1層）の階層型ニューラルネット\n",
        "#neurons = [784, 1000, 1000, 10] # 3層（中間層2層）の階層型ニューラルネット\n",
        "```\n",
        "という箇所があるはずです．いまは「ロジスティック回帰」の行の先頭に `#` がなく，その下の2行の先頭に `#` がついてコメントになっています．この `#` を付け替えることで，ネットワークモデルを切り替えることができます．2層や3層の階層型ニューラルネットでも実験してみましょう．\n"
      ],
      "metadata": {
        "id": "dlLYD2AF9B1R"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 考察"
      ],
      "metadata": {
        "id": "XTVRDj916fgA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実験の結果を観察し，次のことを考えてみましょう．\n",
        "\n",
        "- ロジスティック回帰，2層ニューラルネット，3層ニューラルネットのいずれも，学習を繰り返すと，学習データに対する損失の値は（ほぼ）単調に減少しているはずです．では，テストデータに対する損失の値はどうだろう？\n",
        "- 3つのモデルを比較すると，学習を終えたネットワークの学習データに対する識別率はどれがよいだろう？テストデータに対する識別率ではどうだろう？\n",
        "- 汎化，過適合，モデルのパラメータ数，といった言葉を使って上記の結果を説明してみよう"
      ],
      "metadata": {
        "id": "DU0Gi4YY0YlV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 実験2"
      ],
      "metadata": {
        "id": "JN7p1QVbMcYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "前の実験では，MNISTという手書き数字のデータセットから一部（学習データ5000個，テストデータ1000個）だけ抜き出したもので学習・テストを行っていました．今度は，MNISTのデータを全部使ってみましょう．学習データが60000個，テストデータが10000個あります．\n",
        "\n",
        "次のセルを実行してみましょう．実行が終わるまでに3,4分かかります（CPUのみだと十数分）．まずは3層ニューラルネットを動かして，時間があれば他のも試せばよいです．"
      ],
      "metadata": {
        "id": "ABTTJvXH--IM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データ読み込みの仕組み\n",
        "dsL = MNIST(root='mnist', train=True, download=True, transform=ToTensor())\n",
        "dsT = MNIST(root='mnist', train=False, download=True, transform=ToTensor())\n",
        "dlL = DataLoader(dsL, batch_size=100, shuffle=False)\n",
        "dlT = DataLoader(dsT, batch_size=100, shuffle=False)\n",
        "\n",
        "# ネットワークモデルの定義\n",
        "neurons = [784, 10]             # ロジスティック回帰\n",
        "#neurons = [784, 1000, 10]       # 2層（中間層1層）の階層型ニューラルネット\n",
        "#neurons = [784, 1000, 1000, 10] # 3層（中間層2層）の階層型ニューラルネット\n",
        "net = MLP(neurons).to(device)\n",
        "\n",
        "# 損失関数（交差エントロピー）\n",
        "loss_func = nn.CrossEntropyLoss(reduction='sum')\n",
        "\n",
        "# パラメータ最適化器\n",
        "optimizer = torch.optim.Adam(net.parameters(), lr=1e-3)\n",
        "\n",
        "# 学習の繰り返し回数\n",
        "nepoch = 20\n",
        "\n",
        "# 学習\n",
        "L = []\n",
        "print('########## 実験2 ##########');\n",
        "print(f'学習データ数: {len(dsL)}  テストデータ数: {len(dsT)}')\n",
        "print()\n",
        "print('# epoch  lossL  lossT  rateL  rateT')\n",
        "for t in range(1, nepoch+1):\n",
        "    lossL, rateL = train(net, loss_func, optimizer, dlL)\n",
        "    lossT, rateT = evaluate(net, loss_func, dlT)\n",
        "    L.append([t, lossL, lossT, rateL, rateT])\n",
        "    print(f'{t}   {lossL:.5f}   {lossT:.5f}   {rateL:.4f}   {rateT:.4f}')\n",
        "\n",
        "# 学習曲線の表示\n",
        "data = np.array(L)\n",
        "fig, ax = plt.subplots(1, 2, facecolor='white', figsize=(12, 4))\n",
        "ax[0].plot(data[:, 0], data[:, 1], '.-', label='loss for training data')\n",
        "ax[0].plot(data[:, 0], data[:, 2], '.-', label='loss for test data')\n",
        "ax[0].axhline(0.0, color='gray')\n",
        "ax[0].set_ylim(-0.05, 0.5)\n",
        "ax[0].legend()\n",
        "ax[0].set_title(f'loss (network = {neurons})')\n",
        "ax[1].plot(data[:, 0], data[:, 3], '.-', label='accuracy for training data')\n",
        "ax[1].plot(data[:, 0], data[:, 4], '.-', label='accuracy for test data')\n",
        "ax[1].axhline(1.0, color='gray')\n",
        "ax[1].set_ylim(0.9, 1.01)\n",
        "ax[1].legend()\n",
        "ax[1].set_title(f'accuracy (network = {neurons})')\n",
        "plt.show()\n",
        "\n",
        "# 学習後の損失と識別率\n",
        "loss2, rrate = evaluate(net, loss_func, dlL)\n",
        "print(f'# 学習データに対する損失: {loss2:.5f}  識別率: {rrate:.4f}')\n",
        "loss2, rrate = evaluate(net, loss_func, dlT)\n",
        "print(f'# テストデータに対する損失: {loss2:.5f}  識別率: {rrate:.4f}')\n",
        "\n",
        "# torchsummary\n",
        "torchsummary.summary(net, (1, 784))"
      ],
      "metadata": {
        "id": "p8G3v2NUOL6n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "実行結果の見方は実験1と同じです．ただし，グラフの縦軸の範囲が実験1とは異なっていることに注意．"
      ],
      "metadata": {
        "id": "mxSwKeeKA9cj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実験結果を実験1と比較すると，ニューラルネットの場合，学習データに対する識別率は実験1より実験2の方が少し低くなるものの，テストデータに対する識別率は実験2の方が高くなっているはずです．実験1と2で用いているニューラルネットの構造とパラメータ数は同じですので，この結果は，実験2の方が学習データ数が多いためパラメータ数の多いモデルがあまり過適合を起こさず，良好な汎化性能を示したのだと解釈できます．\n",
        "\n",
        "このように，ニューラルネットのような複雑な機械学習モデルに性能を発揮させるには，学習データをなるべく多く用意することが重要となります（いろいろ考慮すべきことがあるので少々荒っぽい議論ですが...）．"
      ],
      "metadata": {
        "id": "joFbFE7MCM9P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### よだんだよん\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "JyPD1_MsC1FY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 畳み込みニューラルネット\n",
        "\n",
        "この授業で説明した階層型ニューラルネットでは，ある中間層とその前後の層との間の全てのニューロン同士がつながっています．このような構造の層は「全結合型」と呼ばれます．ここまで全ての層が全結合型の階層型ニューラルネットしか登場していませんが，実際には，対象とするデータや解きたい問題の性質に合わせて様々な構造を持つニューラルネットが用いられます．\n",
        "\n",
        "全結合型とは異なる構造のニューラルネットの代表例に，「畳み込み (convolution)」という演算を行う層を持つ，「畳み込みニューラルネットワーク(Convolutional Neural Network, CNN)」と呼ばれるものがあります．画像や音声のようなデータを処理するのに向いており，全結合型よりもずっと少ないパラメータ数でより高い汎化性能が得られます．\n",
        "\n",
        "ここでは実際に動かしてみることはしませんが，PyTorch では上記の `MLP` クラスの中身をちょろっと書き換えるだけで CNN を作ることができます．興味があれば調べて試してみるとよいでしょう．\n",
        "\n"
      ],
      "metadata": {
        "id": "Xlfz7w-Us3R9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 大規模モデルの不思議\n",
        "\n",
        "上の実験で用いた3層の階層型ニューラルネットは，約180万個のパラメータを持っています．2014年の画像識別コンテスト（120万枚の学習画像を用いて1000クラスを識別するというもの）でトップクラスの性能を示した VGG16 という畳み込みニューラルネットは，約1億3千4百万個のパラメータを持っています（ takataka の授業のデモでよく動かしてみてもらっているやつです）．\n",
        "最近話題の ChatGPT の裏にある，自然言語のデータを大量に学習した「大規模言語モデル(Large Language Model, LLM)」である GPT-3 は，1750億個のパラメータを持つとされています（GPT-3には複数種類あり，ChatGPT の裏にいるものは実際にはもっと大きいかもしれません）．\n",
        "\n",
        "これだけパラメータ数が多いと過適合が深刻な問題となりそうなものですが，実際には VGG16 や GPT-3 等の近年の大規模モデルは高い汎化性能を示します．過適合を抑制する様々な技術が用いられていることや，学習データが非常に多いこともその理由と考えられますが，本質的なことはまだわかっていません．謎の解明に向けて理論的な面の研究が進められているところです．"
      ],
      "metadata": {
        "id": "f8c48EMbs5N7"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8WVcsurWC3BV"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
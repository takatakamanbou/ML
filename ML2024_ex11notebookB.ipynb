{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2024/ML2024_ex11notebookB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex11notebookB\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## $K$-平均法\n",
        "----\n",
        "\n",
        "\n",
        "クラスタリングの手法は，**階層型クラスタリング** と **非階層型クラスタリング** に大別できます．\n",
        "\n",
        "**階層型クラスタリング** (hierarchical clustering): 「クラスタAとクラスタBをあわせたものがクラスタPで，クラスタPとクラスタQをあわせたものがクラスタR」というように，階層的になったクラスタを作るクラスタリング手法．\n",
        "\n",
        "**非階層型クラスタリング** (non-hierarchical clustering): クラスタ同士に上記のような階層構造をつくらないクラスタリング手法．\n",
        "\n",
        "それぞれ，実際の手法には様々なものがあり，データや問題の性質に応じて使い分けられます．\n",
        "\n",
        "この授業では，階層型クラスタリングについては説明しません．2年次科目「[多変量解析及び演習](https://www-tlab.math.ryukoku.ac.jp/wiki/?MVA)」で扱っていますので，興味がある人はそちらの資料等を参照してください（[MVA2023では第12回](https://www-tlab.math.ryukoku.ac.jp/wiki/?MVA/2023#ex12)）．\n",
        "以下では，非階層型クラスタリングの代表例である **$K$-平均法** ($K$-means method) について説明します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 準備\n",
        "\n",
        "\n",
        "\n",
        "以下，コードセルを上から順に実行してながら読んでいってね．"
      ],
      "metadata": {
        "id": "FC26yZvbMUv-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "# 実験用データの入手\n",
        "df = pd.read_csv('https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/data4kmeans.csv')\n",
        "dat1 = df[['x1', 'x2']].to_numpy()\n",
        "dat2 = df[['y1', 'y2']].to_numpy()"
      ],
      "metadata": {
        "id": "-NS8fISbMPpj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### $K$-平均法とは\n",
        "\n",
        "**$K$-平均法**（K-means method）は，データ同士の間のユークリッド距離が計算できるデータを対象としたクラスタリング手法です．データをいくつのクラスタに分けるかを予め決めておいて，各データをどれか一つのクラスタに割り振ります．名前の$K$は予め定めるクラスタの数を表します．"
      ],
      "metadata": {
        "id": "TNM_IntoKLoL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "例えば，下図左のような2次元のデータに対して，クラスタ数を $K=3$ として$K$-平均法を適用すると，下図右のような結果が得られます．\n",
        "\n",
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/kmeans.png\">\n",
        "\n",
        "右の図の各データ点は，3つのクラスタのうちのどれに割り振られたかに応じて3色に塗り分けられています．\n",
        "また，図中に描かれた3つの★印は，それぞれのクラスタに所属するデータの重心を表しています．\n",
        "$K$-平均法では，クラスタごとのデータの重心のことを **セントロイド**(centroid)と呼びます．"
      ],
      "metadata": {
        "id": "xhto5VvyI0oG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### $K$-平均法のアルゴリズム"
      ],
      "metadata": {
        "id": "B5if5mWsKuzL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$N$個の$D$次元ベクトルから成るデータ集合\n",
        "$$\n",
        "\\{ \\mathbf{x}_n \\in {\\cal R}^{D} | n = 1, 2, \\ldots, N\\}\n",
        "$$\n",
        "を学習データとして，これを$K$個のクラスタ $C_1, C_2, \\ldots, C_K$ に分ける $K$-平均法のアルゴリズムは，次のようになります．\n"
      ],
      "metadata": {
        "id": "6Dv5TTcd__zw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "(0) クラスタごとのセントロイド $\\mathbf{c}_{k} \\in {\\cal R}^{D}$ ($k=1, 2, \\ldots, K$)の初期値を適当に決める（具体的な方法は後述）．\n",
        "\n",
        "(1) 学習データ $\\mathbf{x}_n$ ($n = 1, 2, \\ldots, N$) のそれぞれを，次の手順で $C_1, C_2, \\ldots, C_K$ のいずれかに割り振る．\n",
        "- $\\mathbf{x}_n$ に対して，$K$個のセントロイドのうち最も距離の小さいものの番号 $y_n$ を次式のように求める（注1）．\n",
        "$$\n",
        "\\newcommand{\\argmin}{\\mathop{\\rm argmin}\\limits}\n",
        "y_n = \\argmin_{k=1,2,\\ldots,K}\\Vert \\mathbf{x}_{n} - \\mathbf{c}_{k} \\Vert^2\n",
        "$$\n",
        "- $\\mathbf{x}_n$ をクラスタ $C_{y_n}$ に割り振る\n",
        "\n",
        "(2) 各クラスタに割り振られた学習データたちの重心を求め，その値でそれぞれのセントロイドを更新する．式で書くと次の通り（注2）．\n",
        "$$\n",
        "\\mathbf{c}_{k} = \\frac{1}{|C_k|}\\sum_{n:y_n = k} \\mathbf{x}_n\n",
        "$$\n",
        "\n",
        "(3) 結果が一定の条件（後述）を満たしていれば終了，さもなくば (1) へ戻る．"
      ],
      "metadata": {
        "id": "B0zA7LwmIwvY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "※注1: $\\rm{argmin}$ という記号の意味について： たとえば，$f(x) = (x-2)^2 + 3$ のとき，\n",
        "$$\n",
        "\\newcommand{\\argmin}{\\mathop{\\rm argmin}\\limits}\n",
        "\\begin{aligned}\n",
        "\\min_x f(x) &= 3\\\\\n",
        "\\argmin_x f(x) &= 2\\\\\n",
        "\\end{aligned}\n",
        "$$\n",
        "となります．つまり，$\\min$ の方は $x$ が動いたときの $f(x)$ の最小値を表すのに対して，$\\rm{argmin}$ の方は $f(x)$ が最小となるときの $x$ の値を表します．\n",
        "したがって，上の式の右辺は，$\\Vert \\mathbf{x}_{n} - \\mathbf{c}_{k} \\Vert^2$ ($k=1,2,\\ldots,K$) が最小となる $k$ を表します．\n",
        "\n",
        "\n",
        "※注2: この式の和は，$y_n$ が $k$ と等しいような $n$ たちについてとります．すなわち，クラスタ $C_k$ に所属するデータの和です．また，$|C_k|$ は集合 $C_k$ の元の数，つまり，クラスタ $C_k$ に割り振られたデータの個数です．\n"
      ],
      "metadata": {
        "id": "CDX-cBHyIuHF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "セントロイドの初期値は，例えば，「学習データから $K$ 個を選んでそれらをセントロイドとする」，「学習データをランダムに$K$個のクラスタに割り振って，(2)の方法でセントロイドを求める」といった方法で決めることができます．\n",
        "\n",
        "学習の繰り返しの終了条件としては，「クラスタ割り振り結果が変わらなくなった」，「あらかじめ定めた繰り返し回数に達した」等が用いられます．"
      ],
      "metadata": {
        "id": "63sSVs34KDl3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### $K$-平均法の性質\n"
      ],
      "metadata": {
        "id": "3QFH98rYKvtJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$K$-平均法には，次のような性質があります．\n",
        "\n",
        "性質1: $K$-平均法の学習では，次式で表される量が最小化されます．\n",
        "$$\n",
        "E = \\sum_{k=1}^{K}\\sum_{n:y_n = k} \\Vert \\mathbf{x}_n - \\mathbf{c}_{k}\\Vert^2\n",
        "$$\n",
        "この値は，「学習データのそれぞれが割り振られたクラスタのセントロイドとの距離の二乗」の和となっています．アルゴリズムの繰り返しごとに，この $E$ の値は単調減少します．\n",
        "\n",
        "性質2: $K$-平均法の結果は，初期値のとり方によって変わります．初期値によっては，$E$ の（最小でない）極小解に到達してそれ以上解が変化しなくなります．そのため，実用の際は，何通りかの初期値で学習を繰り返し，$E$の値が最も小さかった結果を採用する，といった方法がとられます．"
      ],
      "metadata": {
        "id": "p8ONk5anK6gm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 実験: 2次元のデータに$K$-平均法を適用してみよう(1)"
      ],
      "metadata": {
        "id": "MBWxzcTRL706"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実際のデータに$K$-平均法を適用した際の学習の過程を観察するために，次のような2次元のデータをクラスタリングしてみましょう．"
      ],
      "metadata": {
        "id": "X3jYIVd2OD_P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データその1\n",
        "X = dat1\n",
        "\n",
        "# クラスタリング結果描画用のデータ\n",
        "xmin, xmax = -5, 5\n",
        "ymin, ymax = -5, 5\n",
        "p = np.dstack(np.mgrid[xmin:xmax:0.05, ymin:ymax:0.05])\n",
        "P = p.reshape((-1, p.shape[2]))\n",
        "\n",
        "# 散布図\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(4, 4))\n",
        "ax.scatter(X[:, 0], X[:, 1])\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "uDZEK7lzMCbJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルは，$K$-平均法のアルゴリズムを Python で実装したものです．関数を定義しているだけですので，このセルを実行しても何も出力されません．"
      ],
      "metadata": {
        "id": "_RqO9dB9OJJP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## セントロイドの初期化\n",
        "#\n",
        "def initCentroid(X, centroid, seed=None):\n",
        "    assert X.shape[1] == centroid.shape[1]\n",
        "    K = centroid.shape[0]\n",
        "    # 学習データからランダムに K 個を選択して初期セントロイドとする\n",
        "    N = X.shape[0]\n",
        "    idx = np.arange(N, dtype=int)\n",
        "    if seed is not None:\n",
        "        np.random.seed(seed)\n",
        "    np.random.shuffle(idx)\n",
        "    centroid[:] = X[idx[:K], :]\n",
        "\n",
        "## データをクラスタに割り振る\n",
        "#\n",
        "def assignCluster(X, centroid, label):\n",
        "    assert X.shape[1] == centroid.shape[1] and X.shape[0] == label.shape[0]\n",
        "    K = centroid.shape[0]\n",
        "    N = X.shape[0]\n",
        "    sqe = 0.0\n",
        "    for n in range(N):\n",
        "        # 各セントロイドとの距離の二乗を計算\n",
        "        d = np.sum((X[n, :] - centroid)**2, axis=1)\n",
        "        # 距離最小のクラスタへ割り振る\n",
        "        i = np.argmin(d)\n",
        "        label[n] = i\n",
        "        sqe += d[i]\n",
        "\n",
        "    return sqe/N  # 割り振られたセントロイドとの距離の二乗の平均\n",
        "\n",
        "## セントロイドを計算し直す\n",
        "#\n",
        "def updateCentroid(X, centroid, label):\n",
        "    assert X.shape[1] == centroid.shape[1] and X.shape[0] == label.shape[0]\n",
        "    K = centroid.shape[0]\n",
        "    for ik in range(K):\n",
        "        # ik 番目のクラスタに割り当てられたデータの平均をそのクラスタの新しいセントロイドとする\n",
        "        centroid[ik, :] = np.mean(X[label==ik, :], axis=0)"
      ],
      "metadata": {
        "id": "b-rNgijPNVPP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次の手順で，以下の2つのセルを実行しましょう\n",
        "1. 先頭に `## 初期化` と書かれたセルを実行\n",
        "2. つづけて，`## 1ステップの学習を実行` と書かれたセルを実行し，表示されるグラフを観察する\n",
        "3. グラフの下のセルに説明の続きがあるのでそれを読む"
      ],
      "metadata": {
        "id": "R16EP9_lRH5Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 初期化\n",
        "\n",
        "K = 3  # クラスタ数\n",
        "\n",
        "N, D = X.shape\n",
        "centroid = np.empty((K, D)) # セントロイド\n",
        "label = np.empty(N, dtype=int) # 各学習データの所属するセントロイドの番号\n",
        "labelP = np.empty(P.shape[0], dtype=int)\n",
        "\n",
        "initCentroid(X, centroid)  # セントロイドを初期化\n",
        "\n",
        "i = 0  # K-平均法の繰り返し回数"
      ],
      "metadata": {
        "id": "Jpz6iYuoOBWX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 1ステップの学習を実行\n",
        "\n",
        "# データを各クラスタに割り振る\n",
        "msqe = assignCluster(X, centroid, label)\n",
        "assignCluster(P, centroid, labelP)\n",
        "\n",
        "# 現在のクラスタ割り振り結果を描画\n",
        "colors = seaborn.color_palette(n_colors=K)\n",
        "fig = plt.figure(facecolor=\"white\", figsize=(10, 5))\n",
        "ax0 = fig.add_subplot(121)\n",
        "ax0.set_xlim(xmin, xmax)\n",
        "ax0.set_ylim(ymin, ymax)\n",
        "ax0.set_aspect('equal')\n",
        "for ik in range(K):\n",
        "    Xk = X[label==ik, :]\n",
        "    Pk = P[labelP==ik, :]\n",
        "    ax0.scatter(Xk[:, 0], Xk[:, 1], color=colors[ik])\n",
        "    ax0.plot(centroid[ik, 0], centroid[ik, 1], color='white', marker='*', markerfacecolor=colors[ik], markersize=25)\n",
        "    ax0.scatter(Pk[:, 0], Pk[:, 1], marker='.', alpha=0.1, color=colors[ik])\n",
        "    ax0.text(xmin+0.5, ymin+0.5, f'step{i}: {msqe:.3f}', size=20)\n",
        "\n",
        "# セントロイドを更新\n",
        "updateCentroid(X, centroid, label)\n",
        "i += 1\n",
        "\n",
        "# 新しいセントロイドを表示\n",
        "ax1 = fig.add_subplot(122)\n",
        "ax1.set_xlim(xmin, xmax)\n",
        "ax1.set_ylim(ymin, ymax)\n",
        "ax1.set_aspect('equal')\n",
        "for ik in range(K):\n",
        "    Xk = X[label==ik, :]\n",
        "    ax1.scatter(Xk[:, 0], Xk[:, 1], color=colors[ik])\n",
        "    ax1.plot(centroid[ik, 0], centroid[ik, 1], color='white', marker='*', markerfacecolor=colors[ik], markersize=25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "a6Kn9YmbO-qW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記のセルを実行して表示される二つの図のうち，左のものは，現在のセントロイドを用いて学習データをクラスタに振り分けた結果を可視化したものです（アルゴリズムの(1)の結果に相当）．★印の点がセントロイドです．\n",
        "学習データの各点は，割り振られたクラスタに応じて，セントロイドと同じ色に塗られています．\n",
        "平面上の各点も同様に塗り分けられています．\n",
        "また，左下の `stepX: Y` の部分は，学習の繰り返し回数が X で，その時点での「所属クラスタとの距離の2乗の平均」の値が Y であることを示しています．\n",
        "\n",
        "一方，右は，左の結果を用いて，各クラスタのセントロイドを計算し直した結果を示しています（アルゴリズムの(2)の結果に相当）．★印で示されたセントロイドが，それぞれの色に塗られた学習データの重心に移動しています．\n",
        "\n",
        "図の見方が分かったら，`## 1ステップの学習を実行` のセルのみを何度か実行して，次のことを観察しましょう．\n",
        "- 学習の繰り返しによってクラスタの割り振りやセントロイドの位置はどのように変化する？\n",
        "- 「所属クラスタとの距離の2乗の平均」の値はどのように変化する？\n",
        "\n",
        "初期値にもよりますが，およそ十数回の繰り返しで変化がなくなるはずです．"
      ],
      "metadata": {
        "id": "QqhhpCPCSBWf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ★★ やってみよう ★★\n",
        "\n",
        "1. $K$-平均法の結果は，初期値（学習開始時のセントロイドの値）によって変わります．`## 初期化` を実行するたびに初期値が変わりますので，それを実行してから `## 1ステップの学習を実行` を繰り返し実行して，異なる初期値では結果がどのように違ってくるか観察してみましょう．\n",
        "1. `## 初期化` のセルの `K = 3  # クラスタ数` のところをいじれば，$K=3$ 以外の条件でも実験できます．いろいろ試してみましょう．"
      ],
      "metadata": {
        "id": "O6dKcVYTV4ug"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 実験: 2次元のデータに$K$-平均法を適用してみよう(2)"
      ],
      "metadata": {
        "id": "nmLluvLiXm1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "別の2次元データでもやってみましょう．"
      ],
      "metadata": {
        "id": "r_t5QuaTXsX3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データその2\n",
        "Y = dat2\n",
        "\n",
        "# クラスタリング結果描画用のデータ\n",
        "xmin, xmax = -5, 5\n",
        "ymin, ymax = -5, 5\n",
        "p = np.dstack(np.mgrid[xmin:xmax:0.05, ymin:ymax:0.05])\n",
        "P = p.reshape((-1, p.shape[2]))\n",
        "\n",
        "# 散布図\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(4, 4))\n",
        "ax.scatter(Y[:, 0], Y[:, 1])\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "k0kFn25rQrjp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 初期化\n",
        "\n",
        "K = 3  # クラスタ数\n",
        "\n",
        "N, D = Y.shape\n",
        "centroid = np.empty((K, D)) # セントロイド\n",
        "label = np.empty(N, dtype=int) # 各学習データの所属するセントロイドの番号\n",
        "labelP = np.empty(P.shape[0], dtype=int)\n",
        "\n",
        "# セントロイドを初期化\n",
        "initCentroid(Y, centroid, seed=1)\n",
        "\n",
        "i = 0  # K-平均法の繰り返し回数"
      ],
      "metadata": {
        "id": "4rZLtKoxX95L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "## 1ステップの学習を実行\n",
        "\n",
        "# データを各クラスタに割り振る\n",
        "msqe = assignCluster(Y, centroid, label)\n",
        "assignCluster(P, centroid, labelP)\n",
        "\n",
        "# 現在のクラスタ割り振り結果を描画\n",
        "colors = seaborn.color_palette(n_colors=K)\n",
        "fig = plt.figure(facecolor=\"white\", figsize=(10, 5))\n",
        "ax0 = fig.add_subplot(121)\n",
        "ax0.set_xlim(xmin, xmax)\n",
        "ax0.set_ylim(ymin, ymax)\n",
        "ax0.set_aspect('equal')\n",
        "for ik in range(K):\n",
        "    Xk = Y[label==ik, :]\n",
        "    Pk = P[labelP==ik, :]\n",
        "    ax0.scatter(Xk[:, 0], Xk[:, 1], color=colors[ik])\n",
        "    ax0.plot(centroid[ik, 0], centroid[ik, 1], color='white', marker='*', markerfacecolor=colors[ik], markersize=25)\n",
        "    ax0.scatter(Pk[:, 0], Pk[:, 1], marker='.', alpha=0.1, color=colors[ik])\n",
        "    ax0.text(xmin+0.5, ymin+0.5, f'step{i}: {msqe:.3f}', size=20)\n",
        "\n",
        "# セントロイドを更新\n",
        "updateCentroid(Y, centroid, label)\n",
        "i += 1\n",
        "\n",
        "# 新しいセントロイドを表示\n",
        "ax1 = fig.add_subplot(122)\n",
        "ax1.set_xlim(xmin, xmax)\n",
        "ax1.set_ylim(ymin, ymax)\n",
        "ax1.set_aspect('equal')\n",
        "for ik in range(K):\n",
        "    Xk = Y[label==ik, :]\n",
        "    ax1.scatter(Xk[:, 0], Xk[:, 1], color=colors[ik])\n",
        "    ax1.plot(centroid[ik, 0], centroid[ik, 1], color='white', marker='*', markerfacecolor=colors[ik], markersize=25)\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "iFIqU5ImYFhy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この実験では，初期値の違いによる結果の違いを確認しやすくするために，初期値を決めるための乱数の種を固定して，何度実行しても同じ結果が得られるようにしてあります．\n",
        "次のことをやってみましょう．\n",
        "\n",
        "#### ★★ やってみよう ★★\n",
        "\n",
        "1. まずはそのまま実行して結果を観察しましょう．\n",
        "1. 次に，`## 初期化` のセルの中で\n",
        "```\n",
        "# セントロイドを初期化\n",
        "initCentroid(Y, centroid, seed=1)\n",
        "```\n",
        "と書かれた行の，`seed=1` のところを，`seed=0` や `seed=2` に変えてやり直してみましょう．違う初期値で学習させることができます．クラスタリング結果や「所属クラスタとの距離の2乗の平均」の値はどうなるでしょうか．"
      ],
      "metadata": {
        "id": "O0aWtkv5ahXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### その他の非階層的クラスタリングアルゴリズム"
      ],
      "metadata": {
        "id": "sQL4v5-57KzS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "非階層的なクラスタリングのアルゴリズムは，$K$-平均法の他にもたくさんありますが，この授業では説明を省略します．興味のあるひとは，機械学習やデータ分析・データマイニングの教科書などを読んでみてください．\n",
        "\n",
        "以下のリンク先には，Pythonの機械学習ライブラリ scikit-learn で実装されている各種クラスタリングアルゴリズムの紹介があります．\n",
        "https://scikit-learn.org/stable/modules/clustering.html"
      ],
      "metadata": {
        "id": "54EBjF897ygD"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "5q8S5KQ57wzg"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
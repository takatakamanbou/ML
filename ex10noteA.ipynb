{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2022/ex10noteA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex10noteA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2022)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glgWbdlh34P"
      },
      "source": [
        "----\n",
        "## 準備\n",
        "----\n",
        "\n",
        "Google Colab の Notebook では， Python というプログラミング言語のコードを動かして計算したりグラフを描いたりできます．\n",
        "Python は，機械学習・人工知能やデータサイエンスの分野ではメジャーなプログラミング言語ですが，それを学ぶことはこの授業の守備範囲ではありません．以下の所々に現れるプログラムっぽい記述の内容は，理解できなくて構いません．\n",
        "\n",
        "以下，コードセルを上から順に実行してながら読んでいってね．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBTXo5eHh34P"
      },
      "outputs": [],
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## 過適合の抑制とモデル選択(1)\n",
        "----\n",
        "\n",
        "「汎化と過適合」の回に，**過適合**(over-fitting)の話をしました．今回は，過適合を抑制して汎化能力の高い学習モデルを得る方法について説明します．\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 例題\n",
        "\n",
        "説明のための例題として，多項式を当てはめる回帰の問題を考えます．"
      ],
      "metadata": {
        "id": "m_05KKmfwFQ_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## データを生成\n",
        "#\n",
        "def gendat(N, true_data=False, seed=0, sigma=0.0):\n",
        "\n",
        "    if true_data:\n",
        "        x = np.linspace(-0.1, 1.1, num=N)\n",
        "        y = np.sin(2*np.pi*x)\n",
        "    else:\n",
        "        np.random.seed(seed)\n",
        "        x = np.linspace(0.0, 1.0, num=N)\n",
        "        y = np.sin(2*np.pi*x) + sigma*np.random.randn(N)\n",
        "\n",
        "    return np.vstack((x, y)).T\n",
        "\n",
        "## 1, x, x^2, x^3, ..., X^D をならべたデータ行列（N x (D+1)）をつくる\n",
        "#\n",
        "def makeDataMatrix(x, D):\n",
        "\n",
        "    N = x.shape[0]\n",
        "    X = np.zeros((N, D+1))\n",
        "    X[:, 0] = 1\n",
        "    for i in range(1, D+1):\n",
        "        X[:, i] = x**i\n",
        "\n",
        "    return X\n",
        "\n",
        "##  正則化ありの最小二乗法\n",
        "#\n",
        "def solve2(X, y, alpha=0.0):\n",
        "\n",
        "    A = np.dot(X.T, X)\n",
        "    b = np.dot(X.T, y)\n",
        "    if alpha > 0.0:\n",
        "        Dp1 = X.shape[1]\n",
        "        Id = np.eye(Dp1)\n",
        "        Id[0, 0] = 0.0  # バイアス項のみ正則化の対象外とする\n",
        "        A += alpha * Id\n",
        "\n",
        "    # x is the solution of the equation Ax = b\n",
        "    x = np.linalg.solve(A, b)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "mTq-4eNkCeV9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルを実行すると，データをグラフに描きます．"
      ],
      "metadata": {
        "id": "PC3h3ONIDGZX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ndata = 20   # 学習データ・テストデータの数\n",
        "sigma = 0.1  # それらに乗せるノイズの大きさ\n",
        "\n",
        "# ノイズの乗った学習データ\n",
        "datL = gendat(ndata, seed=0, sigma=sigma)\n",
        "\n",
        "# 同じく検証データ\n",
        "datV = gendat(ndata, seed=1, sigma=sigma)\n",
        "\n",
        "# 真の値\n",
        "datTrue = gendat(1000, true_data=True)\n",
        "\n",
        "# グラフを描く\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(8, 6))\n",
        "ax.set_xlim(-0.1, 1.1)\n",
        "ax.set_ylim(-1.2, 1.2)\n",
        "ax.plot(datTrue[:, 0], datTrue[:, 1], color=\"gray\", label=\"true function\")\n",
        "ax.scatter(datL[:, 0], datL[:, 1], color=\"red\", label=\"learning data\")\n",
        "ax.scatter(datV[:, 0], datV[:, 1], color=\"blue\", label=\"validation data\")\n",
        "ax.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "H4QrzRGTCjCt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この問題は，赤い点で描かれた学習データに多項式を当てはめるというものです．これらの学習データは，$[0, 1]$ の範囲の $x$ の値に対して，$y = \\sin{2\\pi x} + \\textrm{ノイズ}$ として作られています．観測されたデータに小さなノイズが乗っているという状況です．ノイズの値は，平均 $0$ 標準偏差 $0.1$ の正規分布をする乱数としています．\n",
        "\n",
        "青い点は，学習データとは別に作ったデータです．作り方は全く同じですが，ノイズの値が異なるために異なる値をとっています．\n",
        "赤い点の学習データを用いて学習（最小二乗法による多項式あてはめ）を行い，青い点のデータに対する誤差を測ることで，得られたモデルの汎化性能を検証します．\n",
        "\n",
        "この青い点のデータのように，学習モデルの汎化性能を検証・評価するために，学習データとは別に用意するデータのことを，**検証データ**(validation data)と呼びます．"
      ],
      "metadata": {
        "id": "UeOlW8tkDNq5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルを実行すると，最小二乗法によって学習データに$D$次多項式を当てはめた結果を表示します．また，学習データと検証データに対する誤差（平均二乗誤差）の値も表示します．\n",
        "\n"
      ],
      "metadata": {
        "id": "WO77nZyPIFuH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title 多項式の次数 D をいろいろ変えてこのセルを何度か実行し直してみよう {run: \"auto\"}\n",
        "D = 1 #@param{type:\"slider\", min:1, max:20, step:1}\n",
        "\n",
        "# 学習データにD次多項式を当てはめて平均二乗誤差を計算\n",
        "XL = makeDataMatrix(datL[:, 0], D)\n",
        "yL = datL[:, 1]\n",
        "w = solve2(XL, yL)\n",
        "yL_est = XL @ w\n",
        "msqeL = np.mean((yL - yL_est)**2)\n",
        "\n",
        "# 検証データに対する平均二乗誤差を計算\n",
        "XV = makeDataMatrix(datV[:, 0], D)\n",
        "yV = datV[:, 1]\n",
        "yV_est = XV @ w\n",
        "msqeV = np.mean((yV - yV_est)**2)\n",
        "\n",
        "# 真の値に対する予測値を計算\n",
        "datTrue = gendat(200, true_data=True)\n",
        "X = makeDataMatrix(datTrue[:, 0], D)\n",
        "y = datTrue[:, 1]\n",
        "y_est = X @ w\n",
        "\n",
        "# グラフを描く\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(8, 6))\n",
        "ax.set_xlim(-0.1, 1.1)\n",
        "ax.set_ylim(-1.2, 1.2)\n",
        "ax.plot(datTrue[:, 0], datTrue[:, 1], color=\"gray\", label=\"true function\")\n",
        "ax.scatter(datL[:, 0], datL[:, 1], color=\"red\", label=\"learning data\")\n",
        "ax.scatter(datV[:, 0], datV[:, 1], color=\"blue\", label=\"validation data\")\n",
        "ax.plot(datTrue[:, 0], y_est, color=\"green\", label=\"estimated curve\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f'### D = {D}')\n",
        "print('# 得られたパラメータ（多項式の係数） = ', w)\n",
        "print(f'# 学習データに対する平均二乗誤差 = {msqeL:.3e}')\n",
        "print(f'# 検証データに対する平均二乗誤差 = {msqeV:.3e}')"
      ],
      "metadata": {
        "id": "mTdKWtaiwwsF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習データまたは検証データを $\\{(x_n, y_n)\\}, n = 1, 2, \\ldots, N$ として，学習モデルの入出力を $f(x)$ で表すとき，平均二乗誤差は次式の通りです．\n",
        "$$\n",
        "\\frac{1}{N}\\sum_{n=1}^{N}(y_n - f(x_n))^2\n",
        "$$\n",
        "最小二乗法の定式化で出てきた二乗誤差の和とは定数倍違っているだけ（$1/2$だったのが$1/N$になっただけ）ですので，意味合いは同じです．\n",
        "\n",
        "上記のセルを実行すると，$D$を大きくするとモデルの予測値が赤い点に近づいていき，学習データに対する平均二乗誤差も小さくなっていることが分かります．しかし，青い点からは離れてしまうところもあって，検証データに対する平均二乗誤差はむしろ大きくなっていることがわかります．"
      ],
      "metadata": {
        "id": "LI2irfajKfGB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の観察結果をもう少しきちんと確認するために，$D$を $1$ から $20$ まで変化させたときの学習データと検証データそれぞれに対する平均二乗誤差を求めてグラフに描いてみましょう．"
      ],
      "metadata": {
        "id": "JZMqAYM_jYrJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## D を 1 から 20 まで変えて平均二乗誤差を求めてみる\n",
        "\n",
        "Dlist = np.arange(1, 21, dtype=int)\n",
        "msqeLlist = np.empty(len(Dlist))\n",
        "msqeVlist = np.empty(len(Dlist))\n",
        "\n",
        "for id, D in enumerate(Dlist):\n",
        "\n",
        "    # 学習データにD次多項式を当てはめて平均二乗誤差を計算\n",
        "    XL = makeDataMatrix(datL[:, 0], D)\n",
        "    yL = datL[:, 1]\n",
        "    w = solve2(XL, yL)\n",
        "    yL_est = XL @ w\n",
        "    msqeLlist[id] = np.mean((yL - yL_est)**2)\n",
        "\n",
        "    # テストデータに対する平均二乗誤差を計算\n",
        "    XV = makeDataMatrix(datV[:, 0], D)\n",
        "    yV = datV[:, 1]\n",
        "    yV_est = XV @ w\n",
        "    msqeVlist[id] = np.mean((yV - yV_est)**2)\n",
        "\n",
        "# グラフを描く\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(8, 6))\n",
        "ax.set_xlim(0, 21)\n",
        "ax.set_ylim(0, 0.04)\n",
        "ax.set_xticks(np.arange(1, 21))\n",
        "ax.set_xlabel('D')\n",
        "ax.set_ylabel('mean squared error')\n",
        "ax.scatter(Dlist, msqeLlist, color=\"red\", label=\"msqeL\")\n",
        "ax.scatter(Dlist, msqeVlist, color=\"blue\", label=\"msqeV\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "ii = np.argmin(msqeVlist)\n",
        "print(f'msqeV の最小値は {msqeVlist[ii]:.3f} (D = {Dlist[ii]})')"
      ],
      "metadata": {
        "id": "tL8szkkZi8Gl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この結果から，大まかな傾向として，次のようなことが読み取れます．\n",
        "- $D$が小さすぎると，学習データに対しても検証データに対しても誤差が大きい．\n",
        "上の図では，$D=1,2$のときの誤差の値は縦軸の上限より大きくて表示されていません．\n",
        "- $D$が大きくなるにつれて学習データに対する誤差は減少する傾向にあるが，検証データに対する誤差はいったん減ってから再び増加する傾向にある．\n",
        "\n",
        "この例題については，汎化性能の観点からは，$D$が小さすぎても大きすぎてもよくない，ということのようです．"
      ],
      "metadata": {
        "id": "yTq0ZW9sjtSW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### パラメータの正則化\n"
      ],
      "metadata": {
        "id": "eHotXheUL0H4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以前にも説明した通り，パラメータの数が多い学習モデルは，自由度が大きいためうまく学習データに当てはまるのだけれど，学習データに当てはまりすぎて過適合を起こしやすくなります（下図参照）．\n",
        "上記の実験の結果も，そのことを示しています．\n",
        "\n",
        "<img width=\"50%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/modeldof.png\">\n",
        "\n",
        "一般に，汎化性能を高めるためには，学習モデルの自由度を適切に定めることが望ましいです．\n",
        "しかし，どれくらいが適切かは問題の複雑さや学習モデルの性質によるものであり，あらかじめ知ることは困難です．\n",
        "そのため，モデルの構造を固定したままでパラメータの実質的な自由度を調節するという手法が考えられています．\n",
        "以下で説明する「正則化」は，そのような手法の代表例です．\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2MxcY_10M_FB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 正則化とは"
      ],
      "metadata": {
        "id": "9Ofb3sW8x2vD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の実験の結果をよく観察すると，パラメータ数すなわち多項式の次数$D$が大きい場合，学習によって得られるパラメータの値（の絶対値）がずいぶん大きくなっていることが分かります．そのために，モデルが表す曲線の形が過剰に複雑になって，過適合を起こしています．\n",
        "もしも，次数が大きくてもパラメータの値があまり大きくならないように抑えることができれば，曲線の形が複雑になりすぎず，過適合を抑制できるかもしれません．\n",
        "\n",
        "そこで，パラメータの値が大きくなりすぎるのを防ぐために，パラメータの二乗和を学習の目的関数に付け足したものを新たな目的関数として，その値を最小化する，という学習の方法が考えられています．これを，パラメータの **正則化** (regularization)といいます．\n",
        "\n"
      ],
      "metadata": {
        "id": "pbb9WH3Imnwv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下，さきほどの多項式当てはめの問題の例で正則化の方法を説明します（注）．\n",
        "この場合，学習モデルは\n",
        "$$\n",
        "f(x) = w_0 + w_1x + w_2x^2 + \\cdots + w_Dx^D\n",
        "$$\n",
        "という式で表され，$w_0, w_1, \\ldots, w_D$ がそのパラメータです．\n",
        "学習データを $\\{(x_n, y_n)\\}, n = 1, 2, \\ldots, N$ とすると，通常の最小二乗法による多項式当てはめでは，以下の式で表される $E$ を目的関数として，この値を最小にするパラメータを求めていました．\n",
        "$$\n",
        "E = \\frac{1}{2}\\sum_{n=1}^{N}(y_n - f(x_n))^2\n",
        "$$\n",
        "\n",
        "※注: 正則化は，多項式当てはめに限らず，機械学習の様々な問題に適用できる手法です．"
      ],
      "metadata": {
        "id": "r7HtX2mxuT2Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これに対して，正則化を行う場合，次のような目的関数を最小化します．\n",
        "$$\n",
        "F = E + \\frac{\\alpha}{2}\\sum_{d=1}^{D}w_d^2\n",
        "$$\n",
        "この目的関数 $F$ は，$E$に，モデルのパラメータの二乗和の項（**正則化項**）を付け足したものとなっています（注）．\n",
        "通常の最小二乗法では$E$を最小化しますが，かわりに$F$を最小化することで，学習データに対する二乗誤差を小さくするだけでなく，パラメータの二乗和も同時に小さくすることができます．\n",
        "ここで，$\\alpha$は正の定数であり，学習データに対する二乗誤差の項と正則化項のバランスを決めるハイパーパラメータです．\n",
        "\n",
        "※この正則化項では，バイアス項 $w_0$ を二乗和から除外しています．ここで考えている学習モデルの場合，バイアス項には正則化の影響を及ぼさない方がより適切だからです．"
      ],
      "metadata": {
        "id": "RXwHbsZrxi2M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 例題に正則化ありの最小二乗法を適用してみる"
      ],
      "metadata": {
        "id": "J8bUDCkzAx7L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルでは，多項式当てはめの例題で $D=20$とパラメータ数を多く設定した場合について，正則化付きの最小二乗法を適用した結果を示します．比較のため，正則化なしの結果も示しています．"
      ],
      "metadata": {
        "id": "mYMEi58KzNko"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = 20\n",
        "alpha = 0.0002  # 正則化項の重み\n",
        "\n",
        "# 学習データにD次多項式を当てはめて平均二乗誤差を計算\n",
        "XL = makeDataMatrix(datL[:, 0], D)\n",
        "yL = datL[:, 1]\n",
        "w0 = solve2(XL, yL)              # 正則化なし\n",
        "wa = solve2(XL, yL, alpha=alpha) # 正則化あり\n",
        "yL_est0 = XL @ w0\n",
        "yL_esta = XL @ wa\n",
        "msqeL0 = np.mean((yL - yL_est0)**2)\n",
        "msqeLa = np.mean((yL - yL_esta)**2)\n",
        "\n",
        "# 検証データに対する平均二乗誤差を計算\n",
        "XV = makeDataMatrix(datV[:, 0], D)\n",
        "yV = datV[:, 1]\n",
        "yV_est0 = XV @ w0\n",
        "yV_esta = XV @ wa\n",
        "msqeV0 = np.mean((yV - yV_est0)**2)\n",
        "msqeVa = np.mean((yV - yV_esta)**2)\n",
        "\n",
        "# 真の値に対する予測値を計算\n",
        "datTrue = gendat(200, true_data=True)\n",
        "X = makeDataMatrix(datTrue[:, 0], D)\n",
        "y = datTrue[:, 1]\n",
        "y_est0 = X @ w0\n",
        "y_esta = X @ wa\n",
        "\n",
        "# グラフを描く\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(8, 6))\n",
        "ax.set_xlim(-0.1, 1.1)\n",
        "ax.set_ylim(-1.2, 1.2)\n",
        "ax.plot(datTrue[:, 0], datTrue[:, 1], color=\"gray\", label=\"true function\")\n",
        "ax.scatter(datL[:, 0], datL[:, 1], color=\"red\", label=\"learning data\")\n",
        "ax.scatter(datV[:, 0], datV[:, 1], color=\"blue\", label=\"validation data\")\n",
        "ax.plot(datTrue[:, 0], y_est0, color=\"green\", label=r\"$\\alpha=0$\", linestyle=\"dashed\")\n",
        "ax.plot(datTrue[:, 0], y_esta, color=\"green\", label=f\"$\\\\alpha={alpha}$\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f'### D = {D}, alpha = 0')\n",
        "print('# 得られたパラメータ（多項式の係数） = ', w0)\n",
        "print(f'# 学習データに対する平均二乗誤差 = {msqeL0:.3e}')\n",
        "print(f'# 検証データに対する平均二乗誤差 = {msqeV0:.3e}')\n",
        "print()\n",
        "print(f'### D = {D}, alpha = {alpha}')\n",
        "print('# 得られたパラメータ（多項式の係数） = ', wa)\n",
        "print(f'# 学習データに対する平均二乗誤差 = {msqeLa:.3e}')\n",
        "print(f'# 検証データに対する平均二乗誤差 = {msqeVa:.3e}')"
      ],
      "metadata": {
        "id": "0tpYqEFd4JgT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "得られたパラメータの値を見ると，正則化のおかげでパラメータの値の絶対値が小さくなっていることがわかります．\n",
        "そのことはグラフの方にも反映されており，正則化なしの場合（緑の破線）に比べて，正則化ありの方（緑の実線）が曲線の急激な変化が抑えられてることが見てとれます．\n",
        "検証データに対する平均二乗誤差の値もより小さくなっており，正則化によって汎化性能を改善できていることが確認できます．\n"
      ],
      "metadata": {
        "id": "OuZd5T107s41"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記では，$\\alpha = 0.0002$ としたときの結果を見ましたが，$\\alpha$を様々に変化させるとどうなるか，平均二乗誤差のグラフを描いて考えてみましょう．"
      ],
      "metadata": {
        "id": "9k8vTHT6Dw5o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "D = 20\n",
        "\n",
        "alphalist = np.arange(0, 0.001, 0.00001)\n",
        "msqeLlist = np.empty(len(alphalist))\n",
        "msqeVlist = np.empty(len(alphalist))\n",
        "\n",
        "for i, alpha in enumerate(alphalist):\n",
        "\n",
        "    # 学習データにD次多項式を当てはめて平均二乗誤差を計算\n",
        "    XL = makeDataMatrix(datL[:, 0], D)\n",
        "    yL = datL[:, 1]\n",
        "    w = solve2(XL, yL, alpha)\n",
        "    yL_est = XL @ w\n",
        "    msqeLlist[i] = np.mean((yL - yL_est)**2)\n",
        "\n",
        "    # テストデータに対する平均二乗誤差を計算\n",
        "    XV = makeDataMatrix(datV[:, 0], D)\n",
        "    yV = datV[:, 1]\n",
        "    yV_est = XV @ w\n",
        "    msqeVlist[i] = np.mean((yV - yV_est)**2)\n",
        "\n",
        "    #print(f'D = {D}, msqeL = {msqeLlist[id]:.3e}, msqeT = {msqeVlist[id]:.3e}')\n",
        "\n",
        "\n",
        "# グラフを描く\n",
        "fig, ax = plt.subplots(facecolor=\"white\", figsize=(8, 6))\n",
        "ax.set_ylim(0, 0.03)\n",
        "ax.set_xlabel('alpha')\n",
        "ax.set_ylabel('mean squared error')\n",
        "ax.scatter(alphalist, msqeLlist, color=\"red\", s=5, label=\"msqeL\")\n",
        "ax.scatter(alphalist, msqeVlist, color=\"blue\", s=5, label=\"msqeV\")\n",
        "ax.legend()\n",
        "plt.show()\n",
        "\n",
        "ii = np.argmin(msqeVlist)\n",
        "print(f'正則化なし（alpha = 0 のとき）の msqeV = {msqeVlist[0]:.3f}')\n",
        "print(f'msqeV の最小値は {msqeVlist[ii]:.3f} (alpha = {alphalist[ii]})')"
      ],
      "metadata": {
        "id": "u9aLxz_O3yMQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$F$の式を見ると，$\\alpha = 0$ の場合，通常の最小二乗法の目的関数 $E$ に一致することがわかります．\n",
        "上記のグラフの左端の点がそのときの平均二乗誤差の値です．\n",
        "\n",
        "そこから $\\alpha$を大きくしていくと，学習データに対する誤差の項よりも正則化項の効き目が強くなります．\n",
        "すると，学習データに対する誤差が増加する一方で，検証データに対する誤差が減少していきます．\n",
        "しかし，$\\alpha$を大きくしすぎると，今度は正則化項の効き目が強すぎて，どちらの誤差も大きくなってしまっています．\n",
        "\n",
        "正則化項の重み $\\alpha$ は手動で調節するハイパーパラメータですが，このように，適切な値を選択してやる必要があります．その方法については，「モデル選択」のセクションで説明します．\n",
        "\n"
      ],
      "metadata": {
        "id": "xecWQSRxElUh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 【発展】$L_2$正則化と$L_1$正則化\n",
        "\n",
        "上記の正則化の説明では，パラメータの二乗和を最小化していました．パラメータをならべたベクトルを\n",
        "$$\n",
        "\\mathbf{w} = (w_1, w_2, \\ldots, w_D)\n",
        "$$\n",
        "のように表すと，パラメータの二乗和は，ベクトル$\\mathbf{w}$のユークリッドノルム（$L_2$ノルム）の二乗です．\n",
        "$$\n",
        "||\\mathbf{w}||_2^2 = \\sum_{d=1}^{D}w_d^2\n",
        "$$\n",
        "そのため，ここで説明している正則化の方法は，「$L_2$正則化」と呼ばれることもあります．\n",
        "\n",
        "正則化の方法としてはこの「$L_2$正則化」が一般的ですが，場合によっては，$L_1$ノルム\n",
        "$$\n",
        "||\\mathbf{w}||_1^2 = \\sum_{d=1}^{D}|w_d|\n",
        "$$\n",
        "を用いる「$L_1$正則化」が用いられることもあります．ここではこれ以上説明しませんが，「$L_2$正則化」とはまた違った効果を発揮します．\n",
        "\n",
        "$L_2$正則化を用いる回帰は「リッジ回帰」（ridge regression），$L_1$正則化を用いる回帰は「ラッソ回帰」（LASSO(Least Absolute Shrinkage and Selection Operator) regression）と呼ばれることもあります．\n",
        "\n"
      ],
      "metadata": {
        "id": "Crn49y3nKaCq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 【発展】正則化付きの最小二乗法の正規方程式と，「正則化」の意味\n",
        "\n",
        "「回帰のための教師あり学習(2) 平面の当てはめ」の notebook で，平面当てはめの最小二乗法における正規方程式を導出しました．\n",
        "ここでは，正則化項を付加するとその形がどのように変わるかを示します．\n"
      ],
      "metadata": {
        "id": "AI2jLlchIKHb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "正則化項なし場合，平面当てはめの最小二乗法の正規方程式は，以下の式(4)の形となるのでした．\n",
        "\n",
        "> 個々の学習データ（の先頭に1を付け足したもの）$\\mathbf{x}_n$ を $(D+1)\\times 1$ 行列（列ベクトル）とみなし，これをならべた行列を $X$ と表記します．つまり $X$は\n",
        "$$\n",
        "X = (\\mathbf{x}_1\\ \\mathbf{x}_2\\ \\cdots\\ \\mathbf{x}_N)\n",
        "$$\n",
        "という $(D+1)\\times N$ の行列です．また，正解の値 $y_n$ をならべた $1 \\times N$ 行列を $Y$ と表記します．\n",
        "$$\n",
        "Y = (y_1\\ y_2\\ \\ldots\\ y_N)\n",
        "$$\n",
        "このとき，正規方程式は次式のようになります（正規方程式がこのようになることを示すのは後の節に回します）．\n",
        "$$\n",
        "XX^{\\top}\\mathbf{w} = XY^{\\top} \\qquad (4)\n",
        "$$\n",
        "ただし，$\\mathbf{w}$ はパラメータをならべた $(D+1)\\times 1$ 行列です．\n",
        "この式の形からわかるように，正規方程式は $(D+1)$元の連立方程式です．\n"
      ],
      "metadata": {
        "id": "PmWUIVaaJKmE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これに対して，正則化項付きの場合，正規方程式は次の形になります（大学初年次レベルの線形代数と微積分の知識があれば証明できますが，ここでは省略します）．\n",
        "$$\n",
        "(XX^{\\top} + \\alpha I)\\mathbf{w} = XY^{\\top} \\qquad (*)\n",
        "$$\n",
        "\n",
        "式(4)の正規方程式の左辺の行列 $X$ の大きさは $(D+1)\\times N$ ，$XX^{\\top}$ の大きさは $(D+1)\\times (D+1)$ です．\n",
        "データ数$N$がパラメータ数$(D+1)$に比べて少なかったり，多かったとしても同じ値の繰り返しだったりすると，行列$XX^{\\top}$のランクが $D+1$ より小さくなり得ます．その場合，$XX^{\\top}$ は非正則となり逆行列が存在しないので，正規方程式の解がまともに求まらなくなります．\n",
        "\n",
        "ところが，$\\textrm{rank}(XX^{\\top}) < D+1$ となっているような場合でも，$XX^{\\top} + \\alpha I$ の固有値は $\\alpha$ 以上となるため，$XX^{\\top} + \\alpha I$ はフルランク，正則になります（注）．したがって，式(*)の正規方程式がちゃんと唯一の解を持ちます．\n",
        "\n",
        "このような話がベースにあるため，ここで説明した方法は「正則化」と呼ばれています．\n",
        "\n",
        "※注: このことは，任意の正方行列$A$の固有値と固有ベクトルを$\\lambda$と$\\mathbf{u}$としたとき，$B = A + \\alpha I$ に対して $B\\mathbf{u} = A\\mathbf{u} + \\alpha\\mathbf{u} = (\\lambda+\\alpha)\\mathbf{u}$ が成り立つこと，すなわち，$B$の固有値が $\\lambda+\\alpha$となることから示せます．"
      ],
      "metadata": {
        "id": "rOfa2hgsN1ko"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "7Mbh3JKltPvj"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
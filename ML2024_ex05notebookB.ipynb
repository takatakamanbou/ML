{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2024/ML2024_ex05notebookB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex05notebookB\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## ニューラルネットワークと深層学習 (1)\n",
        "----\n",
        "\n",
        "**ニューラルネットワーク**(Neural Network)は，もともとは，ヒトなどの生物の「ニューロン」（神経細胞，Neuron）が互いに繋がり合って作る「回路網」(Network)を指す言葉です（注）．\n",
        "もともとは，神経細胞や脳の仕組みをまねた数理モデルという色合いがありましたが，近年ではその色は薄れ，実用的な機械学習モデルとして発展してきています．\n",
        "このようなニューラルネットワークの学習は **深層学習**(Deep Learning) と呼ばれ，いわゆる人工知能(AI)の中核的な技術となっています．何が「深層」，「Deep」なのかは後述します．回帰にも識別にも使うことができます．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 日本語では「神経回路網」と言います．生体内の本物の神経回路網と区別するために，「人工」(Artifitial)を付けて「人工神経回路網」(Artifitial Neural Netrowk)と呼ぶこともあります．\n",
        "</span>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glgWbdlh34P"
      },
      "source": [
        "----\n",
        "### 準備\n",
        "\n",
        "\n",
        "\n",
        "以下，コードセルを上から順に実行してながら読んでいってね．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBTXo5eHh34P"
      },
      "outputs": [],
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "# scikit-learn のいろいろ\n",
        "from sklearn.datasets import make_moons\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.neural_network import MLPClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 線形モデルの限界"
      ],
      "metadata": {
        "id": "JSqZjrEtZnM9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "回帰のための機械学習手法である線形回帰や，識別のための機械学習手法であるロジスティック回帰は，モデルの構造が簡単である分，性能に限界があります．"
      ],
      "metadata": {
        "id": "kvcghtmhnRWl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 線形回帰"
      ],
      "metadata": {
        "id": "AfmPTFT8Zr4J"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$D$次元入力の線形回帰モデルは，次の式で表されます．\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = w_0 + w_1x_1 + w_2x_2 + \\cdots + w_Dx_D\n",
        "$$\n",
        "\n",
        "この式は，$\\mathbf{x}$ と出力の正解が作る $(D+1)$ 次元空間の中で $D$次元の平面（超平面）を表します．\n",
        "そのため，平面にうまく当てはまらないようなデータではよい予測ができません．\n",
        "以下に例を図示します．\n",
        "\n",
        "<img src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/planefitting2.png\" width=\"75%\">\n",
        "\n",
        "\n",
        "\n",
        "上の図の青い点 $(x_1, x_2, y)$ は学習データを表し，赤いメッシュは線形回帰モデルを学習させて得られた平面を表します．\n",
        "一方，青いメッシュは学習データの点が乗っている曲面，すなわち，$(x_1, x_2)$ と $y$ の間の真の関係を表します．\n",
        "\n",
        "左の場合，学習データがほぼ平面に乗っているため，線形回帰モデルは真の関係をうまくとらえられています．\n",
        "しかし，右の場合，データが非線形な性質を持つため，線形回帰モデルは学習データの真の関係をとらえきれておらず，予測がうまくいきません．"
      ],
      "metadata": {
        "id": "LoS7fE7yd9Oq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ロジスティック回帰"
      ],
      "metadata": {
        "id": "2jn7_FYGZuC9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$D$ 次元入力を 2 クラスに分類するロジスティック回帰モデルは，次の通りです．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "f(\\mathbf{x}) &= \\sigma(w_0 + w_1x_1+\\cdots + w_Dx_D) = \\sigma\\left(w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\\\\n",
        "&= \\frac{1}{1+\\exp{\\left( - \\left( w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\right)}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "出力 $f(\\mathbf{x})$ は $0 < f(\\mathbf{x}) < 1$ であり， このモデルは，$f(\\mathbf{x}) = \\frac{1}{2}$ を境界として入力 $\\mathbf{x}$ を2つのクラスに分けています．\n",
        "\n",
        "$$\n",
        "f(\\mathbf{x}) = \\frac{1}{2} \\Leftrightarrow w_0 + \\sum_{d=1}^{D} w_dx_d = 0\n",
        "$$\n",
        "\n",
        "ですから，2つのクラスの境界は，平面（$D$次元空間中の $(D-1)$次元超平面，$D=2$のときは直線）となります．クラス数が3以上の場合の説明は省略しますが，そのときも2つのクラスの間の境界はやはり平面となります．\n",
        "\n"
      ],
      "metadata": {
        "id": "Qa1_DE_ScEBN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "このように，ロジスティック回帰モデルは，クラスとクラスの間を平面でしか分けられないため，クラスの境界がより複雑な場合には予測があまりうまくいきません．\n",
        "そのような問題の例を以下に示します．\n",
        "\n"
      ],
      "metadata": {
        "id": "ZwZYo64d4MIJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 線形モデルではうまく識別できない例\n",
        "moonX, moonY = make_moons(n_samples=200, noise=0.25, random_state=4649)\n",
        "\n",
        "fig, ax = plt.subplots()\n",
        "ax.scatter(moonX[moonY==0, 0], moonX[moonY==0, 1])\n",
        "ax.scatter(moonX[moonY==1, 0], moonX[moonY==1, 1])\n",
        "ax.set_xlim(-1.5, 2.5)\n",
        "ax.set_ylim(-2, 2)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "d-hxzcBR36fD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の2次元2クラス識別の問題をロジスティック回帰で解いてみると...\n",
        "\n",
        "ここでは，Python のための機械学習ライブラリのひとつである [scikit-learn](https://scikit-learn.org/) の中の [sklearn.linear_model.LogisticRegression](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html) を用いています．"
      ],
      "metadata": {
        "id": "tz2WLmoxbcTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ロジスティック回帰\n",
        "model = LogisticRegression()\n",
        "model.fit(moonX, moonY)\n",
        "yy = model.predict(moonX)\n",
        "cc = np.sum(yy == moonY)\n",
        "print(f'正答率: {cc}/{len(yy)} = {cc/len(yy):.3f}')\n",
        "\n",
        "# グラフ描画用のグリッドデータの作成\n",
        "K = 2\n",
        "xmin, xmax = -1.5, 2.5\n",
        "ymin, ymax = -2, 2\n",
        "npoints = 100\n",
        "dx, dy = (xmax - xmin)/npoints, (ymax - ymin)/npoints\n",
        "x_mesh, y_mesh = np.mgrid[xmin:xmax:dx, ymin:ymax:dy]\n",
        "X_mesh = np.dstack((x_mesh, y_mesh))\n",
        "\n",
        "# X_mesh の各点における事後確率の推定\n",
        "p = model.predict_proba(X_mesh.reshape((-1, 2)))\n",
        "pp = p.reshape((X_mesh.shape[0], X_mesh.shape[1], K))\n",
        "\n",
        "# グラフの描画\n",
        "fig, ax = plt.subplots()\n",
        "cmap = ['Blues', 'Oranges', 'Greens']\n",
        "for k in range(K):\n",
        "    ax.scatter(moonX[moonY==k, 0], moonX[moonY==k, 1])\n",
        "    ax.contourf(x_mesh, y_mesh, pp[:, :, k], levels=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0], cmap=cmap[k], alpha=0.3)\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "116gh4CIbtwX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "この図では，モデル出力 $f(\\mathbf{x})$ が $0$ に近い領域と $1$ に近い領域をオレンジと青で塗り分けてあります（白いところが両者の境界）．\n",
        "ロジスティック回帰モデルでは2クラスを平面で（この場合データが2次元なので直線で）しか分けられないため，適切な識別ができていないことが分かります．"
      ],
      "metadata": {
        "id": "_0v7JQ8Yb8rW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 階層型ニューラルネットワーク"
      ],
      "metadata": {
        "id": "4ecHE0Iz85p_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "機械学習の研究分野では，線形回帰やロジスティック回帰のモデルの限界を超えるために，様々な改良が考えられてきました．\n",
        "ここで紹介する **階層型ニューラルネットワーク** は，そのような改良手法のひとつです．\n",
        "階層型ニューラルネットワークでは，入力から出力へと至る計算過程を複数の **層** (layer)を直列につなげた形で構成し，途中の層で入力を非線形変換することで，非線形な回帰や識別を実現します．"
      ],
      "metadata": {
        "id": "1QT-EgfU_gOB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"60%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/neuralnet1.png\" align=\"right\">\n",
        "\n",
        "図の左側は，2次元のデータを3クラスに分類するロジスティック回帰モデルを模式的に表したものです．\n",
        "図の「出力層」（薄く色づけされた縦長の長方形）の中の3つの○は，このモデルの出力 $\\hat{y}_1, \\hat{y}_2, \\hat{y}_3 $ を表しています．\n",
        "これらの値は，入力 $x_1, x_2$ （「入力」の所にある小さい○） から1段階の計算で求まります．\n",
        "\n",
        "一方，図の右側は，入力と出力の間に層をひとつ挟んだ階層型ニューラルネットワークの例です．\n",
        "入力と出力層の間に挟まれる層を，**中間層** (middle layer)または **隠れ層** (hidden layer) といいます．中間層と出力層にある○は **ニューロン** (neuron，神経細胞のこと)と呼ばれるもので，階層型ニューラルネットにおける計算の単位です．\n",
        "\n",
        "この例では，2つの入力が中間層の4つのニューロンに渡されて4つの値が計算され，その値が出力層の3つのニューロンに渡されて3つの出力が計算されます．\n",
        "階層型ニューラルネットワークは，このように層ごとに段階的に計算を行う構造をもち，ある層の値から次の層の値を求める計算に非線形性を持たせることで，複雑な入出力関係を表すことができます．\n",
        "\n"
      ],
      "metadata": {
        "id": "EEJ3WWgm-sO3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"40%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/neuralnet2.png\" align=\"right\">\n",
        "\n",
        "右図は，階層型ニューラルネットワークの例をもっと具体的に示したものです．このニューラルネットワークの入出力をきちんと式で表してみましょう．\n",
        "\n",
        "このニューラルネットワークが$D$次元の入力を受け取ると，それらはまず中間層にある $H$ 個のニューロンへと伝わります．\n",
        "中間層の $h$ 番目のニューロンは，次の式にしたがって自身の値 $y_h$ を計算します．\n",
        "\n",
        "$$\n",
        "y_h = \\sigma\\left( v_{h,0} + \\sum_{d=1}^{D}v_{h,d}x_d\\right) \\qquad (H = 1, 2, \\ldots, H)\n",
        "$$\n",
        "\n",
        "このニューロンは， $v_{h,0}$ から $v_{h,D}$ までの $D+1$ 個のパラメータ（注）を持っています（図の青い線はその一つ）．\n",
        "$\\sigma$ は **活性化関数** (activation function) と呼ばれる非線形変換です（詳しくは後述します）．\n",
        "\n",
        "\n",
        "次に，これら中間層ニューロンの値が出力層にある $M$ 個のニューロンへと伝わります．\n",
        "出力層の $m$ 番目のニューロンは，次の式にしたがって自身の値 $z_m$ を計算します．\n",
        "\n",
        "$$\n",
        "z_m = \\sigma\\left( w_{m,0} + \\sum_{h=1}^{H}w_{m,h}y_h\\right)\\qquad (M = 1, 2, \\ldots, M)\n",
        "$$\n",
        "\n",
        "このニューロンは， $w_{m,0}$ から $w_{m,H}$ までの $H+1$ 個のパラメータを持っています（図の緑の線はその一つ）．\n",
        "これら出力層ニューロンの値がニューラルネットワークの出力となります．\n",
        "\n",
        "このニューラルネットワークの場合，学習によって調節されるパラメータは $v_{h,d}$ ($h = 1, 2, \\ldots, H, d = 0, 1, 2, \\ldots, D$) が $H\\times(D+1)$ 個と $w_{m,h}$ ($m=1,2,\\ldots,M, h = 1, 2, \\ldots, H$) が $M\\times(H+1)$ の計 $H(D+1)+M(H+1)$ 個あります．\n",
        "これらは通常教師あり学習によって調節されます．その方法については後述します．\n",
        "\n",
        "ここでは中間層が一つだけの例を示しましたが，2層以上の中間層を用いることも可能です．\n",
        "\n",
        "※注: ニューラルネットワークのパラメータのことを **重み** (weight) や **結合重み**  (connection weight) ということもあります．\n",
        "\n"
      ],
      "metadata": {
        "id": "PYvMHuDQM2Hs"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ニューラルネットワークの活性化関数は，ニューロンの入出力に非線形性をもたせる役割を果たします．\n",
        "活性化関数としては，ロジスティック回帰モデルでも使われている**シグモイド関数**(sigmoid function)の他に，**双曲線正接関数**(hyperbolic tangent function) や **Rectified Linear 関数**（**ReLU関数**ともいいます）などがよく用いられます（注）．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mbox{Logistic Sigmoid}\\quad  & \\sigma(s) = \\frac{1}{1+{\\rm e}^{-s}}\\\\\n",
        "\\mbox{Hyperbolic Tangent}\\quad & \\sigma(s) = {\\rm tanh}(s) = \\frac{{\\rm e}^{s} - {\\rm e}^{-s}}{{\\rm e}^{s} + {\\rm e}^{-s}}\\\\\n",
        "\\mbox{Rectified Linear}\\quad & \\sigma(s) = \\left\\{\n",
        "    \\begin{array}{ll}\n",
        "    s & (s \\ge 0)\\\\\n",
        "    0 & {\\rm otherwise}\\\\\n",
        "\\end{array} \\right.\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "以下にそれぞれのグラフを示します．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 「シグモイド」は「S字状の」という意味の語なので，双曲線正接関数なども含めて類似した形をしている関数たちの総称です．式(X)のものは，正確には「ロジスティックシグモイド関数」(logistic sigmoid function)といいます．\n",
        "また，「Rectified Linear」な活性化関数を採用したニューロンを Rectified Linear Unit と呼ぶことから，この活性化関数を「ReLU」と呼ぶことがあります．\n",
        "</span>"
      ],
      "metadata": {
        "id": "JxiLOypzG_io"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 活性化関数の値を計算\n",
        "xmin, xmax = -4, 4\n",
        "X = np.linspace(xmin, xmax, num=100)\n",
        "Y1 = 1/(1+np.exp(-X)) # ロジスティックシグモイド\n",
        "Y2 = np.tanh(X)       # 双曲線正接\n",
        "Y3 = np.maximum(X, 0) # ReLU\n",
        "\n",
        "# グラフに描く\n",
        "fig = plt.figure(facecolor='white', figsize=(10,2.5))\n",
        "ax1 = fig.add_subplot(131)\n",
        "ax1.set_xlim(xmin, xmax)\n",
        "ax1.set_ylim(-1.2, 1.2)\n",
        "ax1.axhline(y=0, color='black', linestyle='-')\n",
        "ax1.axvline(x=0, color='black', linestyle='-')\n",
        "ax1.axhline(y=1, color='gray', linestyle='--')\n",
        "ax1.plot(X, Y1, c='red', linewidth=2, label='Sigmoid')\n",
        "ax1.legend()\n",
        "ax2 = fig.add_subplot(132)\n",
        "ax2.set_xlim(xmin, xmax)\n",
        "ax2.set_ylim(-1.2, 1.2)\n",
        "ax2.axhline(y=0, color='black', linestyle='-')\n",
        "ax2.axvline(x=0, color='black', linestyle='-')\n",
        "ax2.axhline(y=1, color='gray', linestyle='--')\n",
        "ax2.axhline(y=-1, color='gray', linestyle='--')\n",
        "ax2.plot(X, Y2, c='red', linewidth=2, label='tanh(s)')\n",
        "ax2.legend()\n",
        "ax3 = fig.add_subplot(133)\n",
        "ax3.set_xlim(xmin, xmax)\n",
        "ax3.set_ylim(xmin, xmax)\n",
        "ax3.axhline(y=0, color='black', linestyle='-')\n",
        "ax3.axvline(x=0, color='black', linestyle='-')\n",
        "ax3.plot(X, Y3, c='red', linewidth=2, label='ReLU')\n",
        "ax3.legend()\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "xvJGp8Wlk6vo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上述の例では中間層が一つだけでしたが，中間層を2つ以上持つ階層型ニューラルネットワークも作ることができます．\n",
        "中間層を増やせば，より複雑な非線形変換が可能となります．しかし，多くの中間層から成るニューラルネットワークの学習は難しく，計算コストもかかるため，以前（20世紀末ころ）は数層程度が限界でした．それが，研究の進展と，計算機の能力の飛躍的な向上により，近年（2010年ころ以降）では数十層以上あるニューラルネットワークの学習が可能となり，実用レベルの性能が得られるようになりました（注）．\n",
        "そのような層の多いニューラルネットワークを **深層ニューラルネットワーク** (Deep Neural Network) といい，深層ニューラルネットワークを用いる機械学習を **深層学習** (Deep Learning) といいます．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: インターネットの普及により，データを容易にたくさん集められるようになって，大規模なデータで学習させることが可能になった，というのも大きな原動力になりました．\n",
        "</span>\n",
        "\n"
      ],
      "metadata": {
        "id": "FVbtcXaUYO7-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### ［補足］ ニューロン？ニューラルネットワーク？\n"
      ],
      "metadata": {
        "id": "XHBpgD4DBOim"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"40%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/neuron3.png\" align=\"right\">\n",
        "\n",
        "\n",
        "ヒトの脳には数百億のニューロンが存在しています．\n",
        "ニューロンは互いに信号を伝達する機能をもっており，多数のニュー\n",
        "ロンが相互につながりあって情報処理を行なっています．\n",
        "生命維持から感情・思考にいたるまでの脳機能は，主にこれらニューロ\n",
        "ンの集団が担っているものと考えられています．\n",
        "\n",
        "右図の上段は，実際のニューロンを模式的に描いたもの，中段は，これらニューロン間の信号伝達の様子を表したものです．大きな丸がニューロンを表し，線分がニューロン間のつながりを表します．\n",
        "\n",
        "ニューロンのふるまいを思い切って単純化すると，次のようにまとめられます．\n",
        "\n",
        "1. 他のニューロンの出力を受け取る\n",
        "1. それらの和を計算する．ただし，ニューロン間のつながりの強さに応じて，他のニューロンからやってきた値を重み付けした和を計算する．\n",
        "1. その重み付け和の値に応じて自分の値を決めて出力する．\n",
        "\n",
        "このようなニューロンのふるまいは，次式のようにモデル化できます．\n",
        "\n",
        "$$\n",
        "y = \\sigma\\left(w_0 + \\sum_{d=1}^{D}w_dx_d\\right) \\qquad (1)\n",
        "$$\n",
        "\n",
        "ただし，$x_d$ は他のニューロンの出力，$y$ は自分の出力です．\n",
        "\n",
        "$w_d$ は $d$ 番目のニューロンと自分との間のつながりの強さを決める量で，「結合重み」(connection weight)や「重み」(weight)などと呼ばれます．$w_0$は「しきい値」(threshold)または「バイアス項」(bias term)などと呼ばれます．\n",
        "\n",
        "$\\sigma$は，重み付け和の値を変換する役割をする関数です．活性化関数(activation function)と呼ばれます．\n",
        "\n",
        "ニューロンの出力は，他のニューロンからやってくる値が同じでも，重みの値が変わると変化します．つまり，重みがパラメータとなっています．実際の脳内でも，シナプス（神経細胞同士の信号伝達を橋渡しする役割をする）での信号の伝わり方が変化することで記憶や学習が起こっていると考えられています．\n"
      ],
      "metadata": {
        "id": "1FtxTuyMBYFI"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"60%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/neuralnet.png\" align=\"right\">\n",
        "\n",
        "ニューラルネットワークとは，上述のようなニューロンが相互に繋がりあったものです．\n",
        "その繋がり方としては，図の左のように相互に無秩序に繋がるようなものも考えられるのですが，近年のAIで幅広く採用されているのは，上述した階層型ニューラルネットワークです（図右）．\n",
        "\n",
        "\n",
        "階層型ニューラルネットワークでは，情報が入力されると，層から層へと順次伝わっていき，最終的な出力がなされます．\n",
        "このように情報の伝わり方が一方向であることから，「フィードフォワード（feed-forward, 順伝播）型」と呼ぶこともあります（注）．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 「多層パーセプトロン」(Multi-Layer Perceptron)という呼び方もあります．\n",
        "</span>"
      ],
      "metadata": {
        "id": "hiEYKQuZxtI8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 例: ニューラルネットワークによる2次元データの2クラス識別"
      ],
      "metadata": {
        "id": "-M8qb7xCZ4ta"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "「線形モデルの限界」の節に出てきた，ロジスティック回帰ではうまく識別できない例をニューラルネットワークに識別させてみましょう．\n",
        "ネットワークの構造の詳細や学習の方法については，後の回で説明します．\n",
        "\n",
        "次のコードセルの選択肢は，それぞれ次のモデルを表します：\n",
        "- `logistic`: ロジスティック回帰モデル．\n",
        "- `2-layer (2000,)`: 入力-中間層-出力層という構造のニューラルネット（2層ニューラルネット）．中間層のニューロン数は2000．\n",
        "- `3-layer (1000, 1000)`: 入力-中間層-中間層-出力層という構造のニューラルネット（3層ニューラルネット）．2つの中間層のニューロン数はいずれも1000．\n",
        "\n",
        "ニューラルネットワークはロジスティック回帰よりも複雑なモデルなので，学習に少し時間がかかります．\n",
        "\n",
        "ここでは，Python のための機械学習ライブラリのひとつである [scikit-learn](https://scikit-learn.org/) の中の [sklearn.neural_network.MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) を用いています．\n",
        "\n"
      ],
      "metadata": {
        "id": "ocGnWkYdaGxw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#@title デモ: 学習させるモデルを選んでね\n",
        "option = '3-layer (1000, 1000)' #@param ['logistic', '2-layer (2000,)',  '3-layer (1000, 1000)'] {allow-input: false}\n",
        "\n",
        "if option == 'logistic':\n",
        "    model = LogisticRegression()\n",
        "else:\n",
        "    if option == '2-layer (2000,)':\n",
        "        numNeurons = (2000, )\n",
        "    elif option == '3-layer (1000, 1000)':\n",
        "        numNeurons = (1000, 1000, )\n",
        "    model = MLPClassifier(hidden_layer_sizes=numNeurons, activation='relu', verbose=False)\n",
        "\n",
        "moonX, moonY = make_moons(n_samples=200, noise=0.25, random_state=4649)\n",
        "model.fit(moonX, moonY)\n",
        "yy = model.predict(moonX)\n",
        "cc = np.sum(yy == moonY)\n",
        "print()\n",
        "print(f'{option}   正答率: {cc}/{len(yy)} = {cc/len(yy):.3f}')"
      ],
      "metadata": {
        "id": "DSZpv1WlCT_s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルを実行すると，上で学習させたモデルが入力の2次元平面をどのように2クラスに分けるかを可視化します．"
      ],
      "metadata": {
        "id": "R_0cQmSxdOtT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# グラフ描画用のグリッドデータの作成\n",
        "K = 2\n",
        "xmin, xmax = -1.5, 2.5\n",
        "ymin, ymax = -2, 2\n",
        "npoints = 100\n",
        "dx, dy = (xmax - xmin)/npoints, (ymax - ymin)/npoints\n",
        "x_mesh, y_mesh = np.mgrid[xmin:xmax:dx, ymin:ymax:dy]\n",
        "X_mesh = np.dstack((x_mesh, y_mesh))\n",
        "\n",
        "# X_mesh の各点における事後確率の推定\n",
        "p = model.predict_proba(X_mesh.reshape((-1, 2)))\n",
        "pp = p.reshape((X_mesh.shape[0], X_mesh.shape[1], K))\n",
        "\n",
        "# グラフの描画\n",
        "fig, ax = plt.subplots()\n",
        "cmap = ['Blues', 'Oranges', 'Greens']\n",
        "for k in range(K):\n",
        "    ax.scatter(moonX[moonY==k, 0], moonX[moonY==k, 1])\n",
        "    ax.contourf(x_mesh, y_mesh, pp[:, :, k], levels=[0.5, 0.6, 0.7, 0.8, 0.9, 1.0], cmap=cmap[k], alpha=0.3)\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(ymin, ymax)\n",
        "ax.set_aspect('equal')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "TIzwdnqmFd9W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2層や3層のニューラルネットワークでは，2クラスの境界複雑な曲線を描くことでこのデータをうまく識別できていることが分かります．"
      ],
      "metadata": {
        "id": "eTKZOztddaA0"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
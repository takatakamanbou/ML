{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/main/ex05noteB.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex05noteB\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2022)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glgWbdlh34P"
      },
      "source": [
        "----\n",
        "## 準備\n",
        "----\n",
        "\n",
        "Google Colab の Notebook では， Python というプログラミング言語のコードを動かして計算したりグラフを描いたりできます．\n",
        "Python は，機械学習・人工知能やデータサイエンスの分野ではメジャーなプログラミング言語ですが，それを学ぶことはこの授業の守備範囲ではありません．以下の所々に現れるプログラムっぽい記述の内容は，理解できなくて構いません．\n",
        "\n",
        "以下，コードセルを上から順に実行してながら読んでいってね．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBTXo5eHh34P"
      },
      "outputs": [],
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc  # アニメーションのため\n",
        "import pandas as pd\n",
        "import seaborn\n",
        "seaborn.set()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## ロジスティック回帰＋勾配法によるパラメータの最適化 (3)\n",
        "----\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 最急降下法によるロジスティック回帰モデルの学習（2クラス識別の場合）\n",
        "\n",
        "2クラス識別のロジスティック回帰モデルで，パラメータを最急降下法で最適化する手順を考えましょう．\n",
        "\n",
        "2クラス識別のロジスティック回帰の問題設定は以下の通りでした．"
      ],
      "metadata": {
        "id": "szdqM433mzUP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**［ロジスティック回帰の問題設定（2クラスの場合）］**\n",
        "\n",
        "$D$次元のデータを二つのクラスに識別するモデルを学習させる．学習データは $N$ 個あり，次のように与えられる．\n",
        "\n",
        "$$\n",
        "(\\mathbf{x}_1, y_1), (\\mathbf{x}_2, y_2),\\ldots , (\\mathbf{x}_N, y_N)\n",
        "$$\n",
        "\n",
        "ただし，$\\mathbf{x}_n \\in {\\cal R}^{D}$ はモデルへの入力であり，$y_n \\in \\{0, 1\\}$ はこのデータの所属クラスの正解を表す値である（$n=1,2,\\ldots,N$）．\n",
        "\n",
        "学習モデルは次式で定める．\n",
        "$$\n",
        "f(\\mathbf{x}) = \\frac{1}{\\displaystyle 1+\\exp{\\left( - \\left( w_0 + \\sum_{d=1}^{D}w_dx_d \\right) \\right)}} \\qquad (1)\n",
        "$$\n",
        "このモデルのパラメータは $w_0, w_1, \\ldots, w_D$ の $(D+1)$ 個ある．\n",
        "\n",
        "このとき，モデルの出力と正解の値との間の「遠さ」を，次式の交差エントロピーで定義する．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "H &= -\\sum_{n=1}^{N} \\left( y_n\\log{f(\\mathbf{x}_n})+(1-y_n)\\log{\\left( 1-f(\\mathbf{x}_n)\\right)} \\right) \\qquad (2)\n",
        "\\end{aligned}\n",
        "$$\n",
        "この $H$ の値がなるべく小さくなるようにパラメータ $w_0, w_1, \\ldots, w_D$ を求めたい．"
      ],
      "metadata": {
        "id": "H23SZSZ_nK-z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "パラメータをならべたベクトルを $\\mathbf{w} = (w_0, w_1, \\ldots, w_D)$ と表すことにして，$H$ を最小にする $\\mathbf{w}$ を求める最急降下法の手続きを導出します．"
      ],
      "metadata": {
        "id": "wbpaIS08cdoh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 勾配の計算\n",
        "\n",
        "最急降下法では $H$ の $\\mathbf{w}$ に関する勾配\n",
        "$$\n",
        "\\nabla{H} = \\left( \\frac{\\partial H}{\\partial w_0}, \\frac{\\partial H}{\\partial w_1}, \\ldots, \\frac{\\partial H}{\\partial w_D}\\right)\n",
        "$$\n",
        "が必要ですので，$\\frac{\\partial H}{\\partial w_d}$ ($d=0,1,\\ldots,D$) を求めましょう．"
      ],
      "metadata": {
        "id": "zDEYO9tdnSXw"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここで，$\\hat{y}_n = f(\\mathbf{x}_n)$ および\n",
        "$$\n",
        "\\ell_n = y_n\\log \\hat{y}_n + (1-y_n)\\log(1-\\hat{y}_n) \\qquad (3)\n",
        "$$\n",
        "とおくことにします．このとき，\n",
        "$$\n",
        "\\frac{\\partial H}{\\partial w_d} = -\\sum_{n=1}^{N}\\frac{\\partial \\ell_n}{\\partial w_d} \\qquad (4)\n",
        "$$\n",
        "です．"
      ],
      "metadata": {
        "id": "iEV-N3WdeO1B"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "また，準備として，シグモイド関数の微分を計算しておきます．\n",
        "シグモイド関数は\n",
        "$$\n",
        "\\sigma(s) = \\frac{1}{1+\\exp{(-s)}} \\qquad (5)\n",
        "$$\n",
        "というものでした．その微分は\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{d\\sigma(s)}{ds} &= \\frac{\\exp{(-s)}}{(1+\\exp{(-s)})^2} = \\frac{1}{1+\\exp{(-s)}}\\cdot\\frac{\\exp{(-s)}}{1+\\exp{(-s)}}\\\\\n",
        "&= \\sigma(s) \\cdot (1-\\sigma(s)) \\qquad(6)\n",
        "\\end{aligned}\n",
        "$$\n",
        "となり，自分自身の値を使って表すことができます．"
      ],
      "metadata": {
        "id": "svBizLdkn0Pt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "それでは，$\\frac{\\partial \\ell_n}{\\partial w_d}$ を求めましょう．式(3)より，\n",
        "$$\n",
        "\\frac{\\partial \\ell_n}{\\partial w_d} = y_n \\frac{1}{\\hat{y}_n}\\frac{\\partial \\hat{y}_n}{\\partial w_d} + (1-y_n)\\frac{1}{1-\\hat{y}_n}\\left(-\\frac{\\partial \\hat{y}_n}{\\partial w_d}\\right) \n",
        "$$\n",
        "です．ここで，\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial \\hat{y}}{\\partial w_d} &= \\frac{\\partial}{\\partial w_d} \\sigma\\left( w_0 + \\sum_{d=1}^{D}w_dx_{n,d} \\right) = \\hat{y}_n(1-\\hat{y}_n)\\frac{\\partial}{\\partial w_d}\\left( w_0 + \\sum_{d=1}^{D}w_dx_{n,d} \\right) \\\\\n",
        "&= \\hat{y}_n(1-\\hat{y}_n)x_{n,d}\n",
        "\\end{aligned}\n",
        "$$\n",
        "となることから，\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell_n}{\\partial w_d} = (y_n - \\hat{y}_n)x_{n,d} =  \\left(y_n-f(\\mathbf{x}_n)\\right)x_{n,d} \\qquad (n = 1, 2, \\ldots, N, d = 0, 1, \\ldots, D)\n",
        "$$\n",
        "\n",
        "が得られます．ただし，$x_{n,0} \\equiv 1$ としました．"
      ],
      "metadata": {
        "id": "gzHa09Mzf_LY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以上より，勾配ベクトルの要素は\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial H}{\\partial w_d} &= -\\sum_{n=1}^{N}\\frac{\\partial \\ell_n}{\\partial w_d} = -\\sum_{n}^{N}(y_n - f(\\mathbf{x}_n))x_{n,d}\\qquad (d = 0, 1, \\ldots, D) \\qquad (7)\n",
        "\\end{aligned}\n",
        "$$\n",
        "となります．ベクトルの形にまとめて書くと，\n",
        "$$\n",
        "\\nabla{H} =  -\\sum_{n}^{N}(y_n - f(\\mathbf{x}_n)) \\mathbf{x}_n \\qquad (8)$$\n",
        "です．"
      ],
      "metadata": {
        "id": "vAN-rXfTrF3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### パラメータ更新式の導出\n",
        "\n",
        "勾配の式が求まりました．これを用いて最急降下法によるパラメータ更新式を求めると，次のようになります．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\mathbf{w}^{\\rm new} &= \\mathbf{w} - \\eta \\nabla H \\\\\n",
        "&=\\mathbf{w} + \\eta\\sum_{n}^{N}(y_n - f(\\mathbf{x}_n)) \\mathbf{x}_n \\qquad (9)\n",
        "\\end{aligned}\n",
        "$$\n",
        "$\\eta$ は正の定数（学習係数）です．\n",
        "\n",
        "交差エントロピーやシグモイドに $\\log$ や $\\exp$ が入っていたわりには，パラメータを更新するための式はシンプルな形になります．\n",
        "$f(\\mathbf{x}_n)$ はデータ $\\mathbf{x}_n$ をモデルに入力して得られる出力（$0 < f(\\mathbf{x}_n) < 1$），$y_n$ はその正解の値（$y_n \\in\\{0,1\\}$）でした．\n",
        "この式を見ると，両者の差 $y_n - f(\\mathbf{x}_n)$ と入力の値との積に応じてパラメータを更新することになっています．"
      ],
      "metadata": {
        "id": "obXGlYF4naBk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 例: 2次元2クラスのデータのロジスティック回帰\n",
        "\n",
        "最急降下法のパラメータ更新式が求まりましたので，ロジスティック回帰モデルの学習手順をプログラムとして書くことができます．\n",
        "以前使ったのと同じ2次元2クラスのデータで実際にロジスティック回帰の学習を行ってみましょう．"
      ],
      "metadata": {
        "id": "GTOt0lzoni0z"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### データの準備"
      ],
      "metadata": {
        "id": "JKWnQM7QpyHb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "## 2次元正規分布で2クラスのデータを生成する関数\n",
        "\n",
        "def getData(seed = None):\n",
        "\n",
        "    if seed != None:\n",
        "        np.random.seed( seed )\n",
        "\n",
        "    # two 2-D spherical Gaussians\n",
        "    X0 = 1.0*np.random.randn(200, 2) + [3.0, 3.0]\n",
        "    X1 = 1.0*np.random.randn(200, 2) + [7.0, 6.0]\n",
        "    X  = np.vstack((X0, X1))\n",
        "    lab0 = np.zeros(X0.shape[0], dtype=int)\n",
        "    lab1 = np.zeros(X1.shape[0], dtype=int) + 1\n",
        "    label = np.hstack((lab0, lab1))\n",
        " \n",
        "    return X, label"
      ],
      "metadata": {
        "id": "TWdplvkZoLwQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# データの準備\n",
        "X, lab = getData(seed=0)\n",
        "N, D = X.shape\n",
        "Yt = lab\n",
        "X = np.vstack((np.ones(N), X.T)).T\n",
        "print(f'データ数 N = {N}, 次元数 D = {D}')\n",
        "\n",
        "fig = plt.figure(facecolor='white', figsize=(14, 6))\n",
        "\n",
        "# 左の2次元散布図\n",
        "ax0 = fig.add_subplot(121)\n",
        "ax0.set_xlim(0, 10)\n",
        "ax0.set_ylim(0, 10)\n",
        "ax0.set_aspect('equal')\n",
        "ax0.scatter(X[Yt == 0, 1], X[Yt == 0, 2]) # blue\n",
        "ax0.scatter(X[Yt == 1, 1], X[Yt == 1, 2]) # orange\n",
        "ax0.set_xlabel('$x_1$')\n",
        "ax0.set_ylabel('$x_2$')\n",
        "\n",
        "# 右の3次元散布図\n",
        "elevation = 20\n",
        "azimuth = -70\n",
        "ax1 = fig.add_subplot(122, projection='3d')\n",
        "ax1.scatter(X[Yt==0, 1], X[Yt==0, 2], 1) # blue\n",
        "ax1.scatter(X[Yt==1, 1], X[Yt==1, 2], 2) # orange\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.set_ylim(0, 10)\n",
        "ax1.view_init(elevation, azimuth)\n",
        "ax1.set_xlabel('$x_1$')\n",
        "ax1.set_ylabel('$x_2$')\n",
        "ax1.set_zlabel('$y$')\n",
        "\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "0epKdv_Iopv4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 学習\n"
      ],
      "metadata": {
        "id": "2n9yYFPOojW1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルは，学習のための関数の定義です．"
      ],
      "metadata": {
        "id": "_VswovwcvEh9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# モデル出力の計算\n",
        "def model(X, w):\n",
        "    return 1.0 / (1.0 + np.exp(-(X @ w)))\n",
        "\n",
        "# 交差エントロピーと正解数\n",
        "def score(Y, Yt):\n",
        "    ce = -np.sum(Yt*np.log(Y)+(1.0-Yt)*np.log(1.0-Y)) # 交差エントロピー\n",
        "    count = np.sum((Y >= 0.5)*Yt) + np.sum((Y < 0.5)*(1 - Yt)) # 正解数\n",
        "    return ce, count\n",
        "\n",
        "# 勾配の計算\n",
        "def grad(X, Y, Yt):\n",
        "    return (Y - Yt) @ X"
      ],
      "metadata": {
        "id": "XM0mlpoYqMko"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "次のセルを実行すると，上記の2次元2クラスデータのロジスティック回帰モデルの学習を行います．"
      ],
      "metadata": {
        "id": "3iaPoPFFvSJr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータの初期化\n",
        "w = (np.random.random(D+1) - 0.5) * 0.2 # [-0.1, 0.1) の一様乱数\n",
        "\n",
        "# 学習係数と学習繰り返し回数\n",
        "eta = 0.2/N\n",
        "nitr = 1000\n",
        "\n",
        "# 学習\n",
        "for i in range(nitr+1):\n",
        "    Y = model(X, w)     # モデル出力の計算\n",
        "    ce, count = score(Y, Yt) # 交差エントロピーと正解数の計算\n",
        "    dw = grad(X, Y, Yt) # 勾配の計算\n",
        "    w -= eta * dw       # パラメータの更新\n",
        "    if (i < 100 and i % 10 == 0) or (i % 100 == 0):\n",
        "        print(f'{i}  {ce:.4f}  {float(count)/N}')"
      ],
      "metadata": {
        "id": "FlIx4N09qjES"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力される数値は，左から順に「学習繰り返し回数」，「（学習データに対する）交差エントロピー」，「（同）識別率」です．最急降下法のステップを繰り返すごとに，交差エントロピーの値が減少していること，それに連れて識別率は上昇していることが分かります．"
      ],
      "metadata": {
        "id": "CGib2dqCvaWN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "学習（アニメーション版）"
      ],
      "metadata": {
        "id": "1pP_FjFqvyYA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のセルを実行すると，上記と同様の学習の過程をアニメーションに描きます．"
      ],
      "metadata": {
        "id": "uWqnfR-oy2o9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータの初期化\n",
        "w = (np.random.random(D+1) - 0.5) * 0.2 # [-0.1, 0.1) の一様乱数\n",
        "\n",
        "# 学習係数と学習繰り返し回数\n",
        "eta = 0.2/N\n",
        "nitr = 1000\n",
        "\n",
        "fig = plt.figure(facecolor='white', figsize=(12, 6))\n",
        "ax1 = fig.add_subplot(121, projection='3d')\n",
        "ax2 = fig.add_subplot(122)\n",
        "elevation = 20\n",
        "azimuth = -70\n",
        "ax1.view_init(elevation, azimuth)\n",
        "ax1.set_xlim(0, 10)\n",
        "ax1.set_ylim(0, 10)\n",
        "ax1.set_zlim(0, 1)\n",
        "ax1.scatter(X[Yt==0, 1], X[Yt==0, 2], 0)\n",
        "ax1.scatter(X[Yt==1, 1], X[Yt==1, 2], 1)\n",
        "#fig.show()\n",
        "\n",
        "ax2.set_xlim(0, nitr)\n",
        "ax2.set_ylim(0, 300)\n",
        "\n",
        "aList = []\n",
        "xx, yy = np.meshgrid(np.linspace(0, 10, num=16), np.linspace(0, 10, num=16))\n",
        "xxr, yyr = xx.ravel(), yy.ravel()\n",
        "XX = np.vstack((np.ones(xxr.shape[0]), xxr, yyr)).T\n",
        "\n",
        "iList = []\n",
        "ceList = []\n",
        "\n",
        "for i in range(nitr+1):\n",
        "\n",
        "    Y = model(X, w)     # モデル出力の計算\n",
        "    ce, count = score(Y, Yt) # 交差エントロピーと正解数の計算\n",
        "    dw = grad(X, Y, Yt) # 勾配の計算\n",
        "    w -= eta * dw       # パラメータの更新\n",
        "\n",
        "    if (i < 100 and i % 10 == 0) or i % 100 == 0:\n",
        "        iList.append(i)\n",
        "        ceList.append(ce)\n",
        "        ZZ = model(XX, w)\n",
        "        zz = ZZ.reshape(xx.shape)\n",
        "        a1 = ax1.plot_wireframe(xx, yy, zz, color='green')\n",
        "        a2 = ax2.plot(iList, ceList, color='blue', marker='.')\n",
        "        rr = count/N*100\n",
        "        s = f'H = {ce:.3f}\\nrate = {rr:.1f}%'\n",
        "        a3 = ax2.text(500, 240, s, size=20)\n",
        "        aList.append([a1]+a2 + [a3])\n",
        "\n",
        "anim = animation.ArtistAnimation(fig, aList, interval=300)\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "anim\n"
      ],
      "metadata": {
        "id": "mV8N4fqIwCiV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記のセルは実行するたびにパラメータが異なる乱数で初期化されます．何度かセルを実行して，どんな違いが出るか確認してみましょう．"
      ],
      "metadata": {
        "id": "2-prHLgJPCGz"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "81ldLfpCk9DN"
      },
      "source": [
        "----\n",
        "### 一般のロジスティック回帰モデルとその学習\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ここまではクラス数が2の場合限定でロジスティック回帰の話をしてきました．ここからは，クラス数が3以上の問題でも使える一般のロジスティック回帰モデルとその学習について説明します．\n",
        "以下，クラス数を文字 $K$ で表します．"
      ],
      "metadata": {
        "id": "Ireyp8sG_wOh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 一般のロジスティック回帰モデル"
      ],
      "metadata": {
        "id": "-24if-4xJzQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**［ロジスティック回帰の問題設定（$K$クラスの場合）］**\n",
        "\n",
        "$D$次元のデータを$K$個のクラスに識別するモデルを学習させる．学習データは $N$ 個あり，次のように与えられる．\n",
        "\n",
        "$$\n",
        "(\\mathbf{x}_1, \\mathbf{y}_1), (\\mathbf{x}_2, \\mathbf{y}_2),\\ldots , (\\mathbf{x}_N, \\mathbf{y}_N)\n",
        "$$\n",
        "\n",
        "ただし，$\\mathbf{x}_n \\in {\\cal R}^{D}$ はモデルへの入力である．また，$\\mathbf{y}_n \\in \\{0, 1\\}^{K}$ （$0$か$1$のみを要素にもつ$K$次元ベクトル）はこのデータの所属クラスの正解を表す値である（$n=1,2,\\ldots,N$）．\n",
        "\n",
        "$\\mathbf{y}_n = (y_{n,1}, y_{n,2}, \\ldots, y_{n,K})$ の要素 $y_{n,k}$ は，$n$番目の学習データが $k$ 番目のクラスに所属するならば $1$，さもなくば $1$ をとる．したがって，$\\mathbf{y}_n$ の要素はどれか一つだけが $1$ で他は全て $0$ である．\n",
        "\n",
        "学習モデルは次式で定める．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\hat{y}_k &= \\frac{\\exp s_k}{\\displaystyle\\sum_{k=1}^{K}\\exp{s_k}} \\qquad (k = 1, 2, \\ldots, K) \\qquad (10)\\\\\n",
        "s_k &= w_{k,0} + \\sum_{d=1}^{D}w_{k,d}x_d \n",
        "\\end{aligned}\n",
        "$$\n",
        "このモデルのパラメータは $w_{k,d}$ ($k = 1, 2, \\ldots, K, d = 0, 1, \\ldots, D$) の $K\\times (D+1)$ 個ある．\n",
        "\n",
        "このとき，モデルの出力と正解の値との間の「遠さ」を，次式の交差エントロピーで定義する．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "H &= -\\sum_{n=1}^{N} \\sum_{k=1}^{K} y_{n,k}\\log{\\hat{y}_{n,k}}\n",
        "\\end{aligned}\n",
        "$$\n",
        "この $H$ の値がなるべく小さくなるようにパラメータ $\\{ w_{k,d} \\}$ を求めたい．"
      ],
      "metadata": {
        "id": "U3WrawaFAS6t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2クラス識別のロジスティック回帰モデルの出力は1つでしたが，一般化したロジスティック回帰モデルでは，$\\hat{y}_1,\\hat{y}_2,\\ldots, \\hat{y}_K$ と $K (=\\mbox{クラス数})$ 個あります．大雑把にいうと，2クラス識別のロジスティック回帰モデルを $K$ 個あわせたものになっています．\n",
        "\n",
        "ただし，2クラス識別では出力の値をシグモイド関数によって計算していたところが，こちらでは式(10)のようになっています．\n",
        "この式(10)は，**ソフトマックス関数**（**softmax**関数）と呼ばれるものです．\n",
        "式の形から明らかですが，$0 < \\hat{y}_k < 1$ および $\\sum_{k=1}^{K}\\hat{y}_k = 1$ となる性質があります．\n",
        "\n",
        "このことから，ある入力データに対するモデル出力 $\\hat{y}_1,\\hat{y}_2,\\ldots, \\hat{y}_K$ の値は，そのデータがそれぞれのクラスに所属する確信度合い（確率もどき）を表すと考えることができます．\n",
        "上記の交差エントロピーを最小化することで，正解クラスに対応する出力が $1$ に近づき，それ以外のクラスに対応する出力が $0$ に近づくようになります．"
      ],
      "metadata": {
        "id": "Zc5eUkHvFHWj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 一般のロジスティック回帰モデルの学習"
      ],
      "metadata": {
        "id": "NolKoqHjJ57x"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "導出過程は省略しますが，上記の交差エントロピのパラメータに関する勾配は次のようになります．\n",
        "\n",
        "$$\n",
        "\\frac{\\partial H}{\\partial w_{k,d}} =  -\\sum_{n}^{N}(y_{n,k} - \\hat{y}_{n,k}) x_{n,d} \\qquad(k = 1, 2, \\ldots, K, d = 0, 1, \\ldots, D) \n",
        "$$\n",
        "\n",
        "2クラスの場合と同じ構造をしていますね．\n",
        "\n",
        "以下，最急降下法の手順は2クラスの場合とほとんど同じですので，説明を省略します．"
      ],
      "metadata": {
        "id": "HmbNSAuSKgNX"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "----\n",
        "### 例: ロジスティック回帰による手書き数字の識別\n",
        "\n",
        "最短距離法や最近傍法の実験に使ったのと同じ手書き数字のデータを使ってロジスティック回帰モデルの学習の実験をやってみましょう．\n",
        "このデータがどんなものかという話は，最短距離法や最近傍法について説明した notebook の方に書いています．"
      ],
      "metadata": {
        "id": "c3c5pVlIPpZa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "まずはデータの準備．"
      ],
      "metadata": {
        "id": "b8nt7IyLpwmS"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hyAGNwhe5S0"
      },
      "outputs": [],
      "source": [
        "# 手書き数字データの入手\n",
        "! wget -nc https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/minimnist.npz\n",
        "rv = np.load('minimnist.npz')\n",
        "datL = rv['datL'].astype(float)\n",
        "labL = rv['labL']\n",
        "datT = rv['datT'].astype(float)\n",
        "labT = rv['labT']\n",
        "print(datL.shape, labL.shape, datT.shape, labT.shape)\n",
        "\n",
        "K = 10 # クラス数\n",
        "\n",
        "# 学習データの用意\n",
        "NL, D = datL.shape # 学習データの数と次元数\n",
        "XL = np.empty((NL, D+1))\n",
        "XL[:, 0] = 1.0\n",
        "XL[:, 1:] = datL/255\n",
        "YtL = np.zeros((NL, K))\n",
        "for ik in range(K):\n",
        "    YtL[labL == ik, ik] = 1.0\n",
        "\n",
        "# テストデータの用意\n",
        "NT, _ = datT.shape # テストデータの数\n",
        "XT = np.empty((NT, D+1))\n",
        "XT[:, 0] = 1.0\n",
        "XT[:, 1:] = datT/255\n",
        "YtT = np.zeros((NT, K))\n",
        "for ik in range(K):\n",
        "    YtT[labT == ik, ik] = 1.0"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "以下のセルが，上記のデータを 10 クラスに分類するロジスティック回帰の学習のコードです．"
      ],
      "metadata": {
        "id": "-WhbR7W1p0AR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# パラメータの初期化\n",
        "W = (np.random.random((K, D+1)) - 0.5) * 0.2 # [-0.1, 0.1) の一様乱数\n",
        "\n",
        "# 学習係数と学習繰り返し回数\n",
        "eta = 0.01\n",
        "nitr = 20000\n",
        "\n",
        "# 学習\n",
        "for i in range(nitr+1):\n",
        "\n",
        "    # 学習データの一つをランダムに選択\n",
        "    n = np.random.randint(NL)\n",
        "    x, yt = XL[n, :], YtL[n]\n",
        "    # モデル出力の計算\n",
        "    exps = np.exp(W @ x)\n",
        "    y = exps / np.sum(exps)\n",
        "    # 最急降下法\n",
        "    dW = (y - yt)[:, np.newaxis] @ x[np.newaxis, :]\n",
        "    W -= eta * dW\n",
        "\n",
        "    if (i < 1000 and i % 100 == 0) or (i % 1000 == 0):\n",
        "        # モデル出力の計算\n",
        "        exps = np.exp(XL @ W.T)\n",
        "        Y = exps / np.sum(exps, axis=1)[:, np.newaxis]\n",
        "        # 交差エントロピー\n",
        "        ce = -np.sum(YtL * np.log(Y))\n",
        "        # 正解数\n",
        "        count = np.sum(labL == np.argmax(Y, axis=1))\n",
        "        print(f'{i}  {ce:.2f}  {count/NL:.3f}')\n",
        "\n",
        "print()\n",
        "\n",
        "# テスト\n",
        "exps = np.exp(XT @ W.T)\n",
        "Y = exps / np.sum(exps, axis=1)[:, np.newaxis]\n",
        "ce = -np.sum(YtT * np.log(Y))\n",
        "count = np.sum(labT == np.argmax(Y, axis=1))\n",
        "print(f'テスト: {ce:.2f}  {count/NT:.3f}')"
      ],
      "metadata": {
        "id": "AuxGKbnojXBj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "出力される数値は，左から順に「学習繰り返し回数」，「（学習データに対する）交差エントロピー」，「（同）識別率」です．学習を繰り返すごとに交差エントロピーの値が減少していること，それに連れて識別率が上昇していることが分かります．\n",
        "\n",
        "このコードでは，実行するたびにパラメータの初期値が変化します．何度も実行して結果の違いを観察するとよいでしょう．\n",
        "\n"
      ],
      "metadata": {
        "id": "5zMZgmLMqLNC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### ［発展］確率的勾配降下法"
      ],
      "metadata": {
        "id": "NAGnw3jEqpzz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "実は，上記の手書き数字の学習のコードでは，最急降下法とは少し異なるパラメータ最適化手法を用いています．ここで用いているのは，**確率的勾配降下法**(stochastic gradient descent, SGD)と呼ばれる手法です．\n",
        "\n",
        "上記の学習のような問題では，パラメータ最適化のための目的関数は通常，個々の学習データに対して定義された誤差などの和の形で表されます．目的関数を $E(\\mathbf{w})$ とおくと，\n",
        "$$\n",
        "E(\\mathbf{w}) = \\sum_{n=1}^{N}e_n(\\mathbf{w})\n",
        "$$\n",
        "みたいな形をしている，ということです．\n",
        "\n",
        "このとき，最急降下法のパラメータ更新式は次の通りです．\n",
        "$$\n",
        "\\mathbf{w}^{\\rm new} = \\mathbf{w} - \\eta \\nabla E(\\mathbf{w}) \n",
        "$$\n",
        "これに対して，確率的勾配降下法（より正確には，確率的勾配法を用いる最急降下法）では，学習データの中から一つをランダムに選んでは\n",
        "$$\n",
        "\\mathbf{w}^{\\rm new} = \\mathbf{w} - \\eta \\nabla e_n(\\mathbf{w}) \n",
        "$$\n",
        "とパラメータ更新することを何回も繰り返します．\n",
        "\n",
        "最急降下法で用いる勾配は，全ての学習データに関する情報を含んでいます．したがって，学習データたちの目的関数の値を平均的に小さくする方向へパラメータを修正します．\n",
        "一方，確率的勾配降下法の場合は，一度のパラメータ更新では一つのデータに関する目的関数の値しか小さくしません．他の学習データについては逆に目的関数の値が大きくなってしまうこともありえます．しかし，この操作を何度も繰り返すことで，「確率的に」全体の目的関数の値を最小化することをねらっています．\n",
        "\n",
        "詳しいことは省きますが，確率的勾配降下法の方が計算時間を短くできたり，使用するメモリ量を節約できたり，より少ない計算量で解に収束することがあるため，実用的にはほとんどこちらが用いられています（注）．\n",
        "\n",
        "※注: より正確には，一つの学習データだけではなく少数のデータをまとめて勾配を計算する ''Batch SGD'' が用いられます．その方がCPUやGPUが並列計算できてより効率的なので，"
      ],
      "metadata": {
        "id": "EQZuuKQIrBRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "3er-CKximIlW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "ex05noteB.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
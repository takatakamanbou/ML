{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/takatakamanbou/ML/blob/2024/ML2024_ex06notebookA.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2UzDNLmVS3po"
      },
      "source": [
        "# ML ex06notebookA\n",
        "\n",
        "<img width=72 src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/ML-logo.png\"> [この授業のウェブページ](https://www-tlab.math.ryukoku.ac.jp/wiki/?ML/2024)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_WXvQDHGjLD8"
      },
      "source": [
        "----\n",
        "## ニューラルネットワークと深層学習 (2)\n",
        "----\n",
        "\n",
        "ひとつ前の notebook では，ニューラルネットワークとはどういうものか，ということを説明しました．この notebook では，階層型ニューラルネットワークの学習の方法について説明します．また，簡単な回帰の問題への適用例を示します．\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7glgWbdlh34P"
      },
      "source": [
        "----\n",
        "### 準備\n",
        "\n",
        "以下，コードセルを上から順に実行してながら読んでいってね．"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JBTXo5eHh34P"
      },
      "outputs": [],
      "source": [
        "# 準備あれこれ\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import animation, rc  # アニメーションのため\n",
        "import seaborn\n",
        "seaborn.set()\n",
        "\n",
        "# 深層学習フレームワーク PyTorch のため\n",
        "import torch\n",
        "import torch.nn as nn"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 階層型ニューラルネットワークの学習"
      ],
      "metadata": {
        "id": "eJsCdU1jqXi6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "階層型ニューラルネットワークのパラメータは，通常，教師あり学習によって決定されます．教師あり学習ですので，「ネットワークの出力とその正解の値との間のずれ」の大きさを表す量を定式化し，それが小さくなるようにパラメータを調節します．この「ずれ」を表す関数を，**損失関数**(loss function)と呼びます．\n",
        "\n",
        "平面あてはめの最小二乗法では，「モデル出力とその正解の値との間の二乗誤差」を最小化するという，似たような話がありました．また，ロジスティック回帰では，「モデル出力と正解との間の交差エントロピー」を最小化していました．\n",
        "ニューラルネットワークの場合，損失関数としては，二乗誤差・交差エントロピーのどちらも使用することができます．ニューラルネットワークを回帰問題に適用する場合には二乗誤差が，識別問題に適用する場合には交差エントロピーがよく用いられます（注）．\n",
        "\n",
        "損失関数の最適化（最小化）は，ロジスティック回帰と同様，**最急降下法**などの**勾配法**が用いられます．\n",
        "ニューラルネットワークのモデルは複雑な式をしていますが，損失関数のパラメータに関する勾配を求めることが可能です．というか，ニューラルネットワークモデルは，勾配を計算可能なように微分可能な要素の組み合わせで作られています．\n",
        "\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 問題によって，これ以外にも様々な損失関数が用いられます．\n",
        "</span>\n"
      ],
      "metadata": {
        "id": "Q1cjIr5vq3FZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 例: 中間層が一つで損失関数が二乗誤差の場合\n",
        "\n",
        "<img width=\"40%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/neuralnet2.png\" align=\"right\">\n",
        "\n",
        "ニューラルネットワークモデルの構造と損失関数を定めれば，勾配法でモデルのパラメータを学習させることができます．\n",
        "ここでは，図のように，中間層を一つもち，入力が$D$次元，中間層および出力層のニューロン数がそれぞれ $H$，$M$ のニューラルネットワークを考えます．\n",
        "このネットワークの出力 $z_m$ は次式で求まります．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "y_h &= \\sigma\\left( v_{h,0} + \\sum_{d=1}^{D}v_{h,d}x_d\\right) & (h = 1, 2, \\ldots, H) \\qquad (1)\\\\\n",
        "z_m &= \\sigma\\left(w_{m,0} + \\sum_{h=1}^{H}w_{m,h}y_h \\right) & (m=1,2,\\ldots,M) \\qquad (2)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "損失関数に二乗誤差を用いるものとして，その勾配を式で表してみます．\n",
        "\n",
        "学習データが $N$ 個与えられるとして，出力の正解を $\\widetilde{z}_{n,m}$ ($n=1,2,\\ldots,N$) とおくと，二乗誤差の損失関数 $L$ は\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{2}\\sum_{n=1}^{N}\\sum_{m=1}^{M}(\\widetilde{z}_{n,m} - z_{n,m})^2\n",
        "$$\n",
        "\n",
        "となります．ここで，$\\ell_{n} = \\frac{1}{2}\\sum_{m=1}^{M}(\\widetilde{z}_{n,m} - z_{n,m})^2$ とおけば， $L = \\sum_{n=1}^{N}\\ell_n$ より，$L$ の勾配は $\\ell_n$ の勾配の和となります．\n",
        "簡単のため添字 $n$ を省略して $\\ell$ の勾配を計算すると，次のようになります．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial \\ell}{\\partial w_{m,h}} &= -(\\widetilde{z}_m - z_m)\\sigma'(z_m)y_h & (3)\\\\\n",
        "\\frac{\\partial \\ell}{\\partial v_{h,d}}\n",
        "&= -\\left(\\sum_{m=1}^{M}(\\widetilde{z}_m - z_m) \\sigma'(z_m)w_{m,h}\\right)\\sigma'(y_h)x_d & (4)\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "これらの式の導出過程や記号$\\sigma'$の意味は「［発展］損失関数の勾配の導出」節を参照してください．勾配の導出は，ロジスティック回帰のときと同様に，高校数学（合成関数の微分）＆大学初年次の数学（偏微分）の知識があればできます．"
      ],
      "metadata": {
        "id": "kg3gAk73PC1t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ［発展］損失関数の勾配の導出"
      ],
      "metadata": {
        "id": "dZL_dJFh5k9d"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\ell_n = \\frac{1}{2}\\sum_{m=1}^{M}(\\widetilde{z}_{n,m} - z_{n,m})^2$ に対する勾配 $\\frac{\\partial \\ell_n}{\\partial v_{h,d}}$ と $\\frac{\\partial \\ell_n}{\\partial w_{h}}$ を求めます．\n",
        "これらが求まれば，$L=\\sum_{n=1}^{N}\\ell_n$ より，$\\frac{\\partial L}{\\partial v_{h,d}} = \\sum_{n=1}^{N}\\frac{\\partial \\ell_n}{\\partial v_{h,d}}$  および $\\frac{\\partial L}{\\partial w_{h}} = \\sum_{n=1}^{N}\\frac{\\partial \\ell_n}{\\partial w_{h}}$ となります．\n",
        "\n",
        "以下では，式を見やすくするため，添字 $n$ も適宜省略しています．"
      ],
      "metadata": {
        "id": "WSI35Zy6Whqd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "まず，準備として $\\frac{\\partial z_m}{\\partial w_{m,h}}$ および\n",
        "$\\frac{\\partial z_m}{\\partial v_{h,d}}$ を求めます．\n",
        "式(1),(2)より，次のように求まります．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial z_m}{\\partial w_h} &= \\sigma'(z_m)y_h \\qquad\n",
        "\\frac{\\partial y_h}{\\partial v_{h,d}} = \\sigma'(y_h)x_d\\\\\n",
        "\\frac{\\partial z_m}{\\partial v_{h,d}} &= \\sigma'(z_m)\\frac{\\partial }{\\partial v_{h,d}}\\left( w_{m,0} + \\sum_{h=1}^{H}w_{m,h}y_h \\right)\\\\\n",
        "&= \\sigma'(z_m) w_{m,h} \\frac{\\partial y_h}{\\partial v_{h,d}} = \\sigma(z_m)w_{m,h}\\sigma'(y_h)x_d\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "ここで，$\\sigma'$ というのは，$\\frac{d\\sigma(s)}{ds}$ を $\\sigma(s)$ 自身で表したものです．例えばシグモイド関数の場合は，$\\sigma' = \\sigma(1-\\sigma)$ より，$\\sigma'(z_m) = z_m(1-z_m)$ などとなります（2クラス識別のロジスティック回帰の学習アルゴリズム導出過程参照）．\n",
        "\n"
      ],
      "metadata": {
        "id": "9amUhbzy_JmF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "これらの式から，勾配 $\\frac{\\partial \\ell}{\\partial w_{m,h}}$ と $\\frac{\\partial \\ell}{\\partial v_{h,d}}$ は次式のように求まります．\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial \\ell}{\\partial w_{m,h}} &= (\\widetilde{z}_m - z_m)\\left( - \\frac{\\partial z_m}{\\partial w_{m,h}} \\right) = -(\\widetilde{z}_m - z_m)\\sigma'(z_m)y_h\\\\\n",
        "\\frac{\\partial \\ell}{\\partial v_{h,d}} &= \\sum_{m=1}^{M}(\\widetilde{z}_m - z_m)\\left( - \\frac{\\partial z_m}{\\partial v_{h,d}} \\right) \\\\\n",
        "&= -\\left(\\sum_{m=1}^{M}(\\widetilde{z}_m - z_m) \\sigma'(z_m)w_{m,h}\\right)\\sigma'(y_h)x_d\n",
        "\\end{aligned}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "GWJtp-IFAFul"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 誤差逆伝播\n",
        "\n",
        "階層型ニューラルネットワークのパラメータを勾配法で学習させる方法のことを，**誤差逆伝播法**(error backpropagation)といいます．\n",
        "\n",
        "ニューラルネットワークの出力の計算の際には， ネットワークへ入力された値が，最初の中間層 → … → 最後の中間層 → 出力層と順に伝わっていきます．\n",
        "これに対して，勾配の計算では，出力層で求めた誤差の値が，出力層 → 最後の中間層 → … 最初の中間層と，ネットワークを逆向きに伝わっていきます．\n",
        "そのため，「誤差が逆向きに伝播する」ということで，誤差逆伝播と呼ばれています．"
      ],
      "metadata": {
        "id": "Hwd8KGlJWwBE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "参考までに，式(3),(4)を変形して，「誤差が逆向きに伝播する」ことを確認してみます．\n",
        "\n",
        "式(3)において，$\\varepsilon_{m} = (\\widetilde{z}_m - z_m)\\sigma'(z_m)$ とおくと，\n",
        "\n",
        "$$\n",
        "\\frac{\\partial \\ell}{\\partial w_{m,h}} = -(\\widetilde{z}_m - z_m)\\sigma'(z_m)y_h = -\\varepsilon_{m}y_h\\\\\n",
        "$$\n",
        "\n",
        "と表わせます．このとき，式(4)の方は\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "\\frac{\\partial \\ell}{\\partial v_{h,d}}\n",
        "&= -\\left(\\sum_{m=1}^{M}(\\widetilde{z}_m - z_m) \\sigma'(z_m)w_{m,h}\\right)\\sigma'(y_h)x_d \\\\\n",
        "&= -\\left(\\sum_{m=1}^{M} w_{m,h} \\varepsilon_{m} \\right)\\sigma'(y_h)x_d\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "となります．$\\varepsilon_{m}$を「$m$番目の出力層ニューロンの誤差」とみなすと，中間層ニューロンの勾配は，それらを重み $w_{m,h}$ を介して逆向きに伝えたものから計算できることが分かります．\n",
        "\n",
        "中間層が複数ある場合は，このような過程を出力層側から入力側へと一層ずつ繰り返すことになります．"
      ],
      "metadata": {
        "id": "drhPhm8vZTGW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "### 具体例: 非線形回帰\n",
        "\n"
      ],
      "metadata": {
        "id": "QwiydHgvxvXW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 問題設定\n",
        "「汎化と過適合」の回の notebook で多項式あてはめしてみていた例をニューラルネットワークにやらせてみましょう．\n",
        "以下のグラフに描かれた8つの点のX座標を入力，Y座標を出力の正解とする回帰問題です．"
      ],
      "metadata": {
        "id": "1HbH-THJBTIO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# データを用意\n",
        "X = np.linspace(-3, 4, num=8)\n",
        "Y = np.sin(X)  # 真の関係は y = sin(x)\n",
        "xmin, xmax = -4.5, 5.5\n",
        "Xr =  np.linspace(xmin, xmax, num=100)\n",
        "Yr = np.sin(Xr)\n",
        "# グラフを描く\n",
        "fig, ax = plt.subplots(1, facecolor='white', figsize=(6, 4))\n",
        "ax.scatter(X, Y)\n",
        "ax.plot(Xr, Yr, color='gray')\n",
        "ax.set_xlim(xmin, xmax)\n",
        "ax.set_ylim(-1.5, 1.5)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "9-dKPzZ9ysW7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img width=\"40%\" src=\"https://www-tlab.math.ryukoku.ac.jp/~takataka/course/ML/neuralnet4.png\" align=\"right\">\n",
        "\n",
        "このような問題のためのニューラルネットワークとして最も単純なものは，図のように，中間層が一層で出力層にニューロンをひとつだけ含む形のものです．このニューラルネットの入出力は次式で表されます．\n",
        "\n",
        "$$\n",
        "\\begin{aligned}\n",
        "y_h &= \\sigma\\left( v_{h,0} + v_{h,1}x\\right) \\qquad (h = 1, 2, \\ldots, H) \\\\\n",
        "z &= w_{0} + \\sum_{h=1}^{H}w_{h}y_h\n",
        "\\end{aligned}\n",
        "$$\n",
        "\n",
        "一般に回帰の問題では，出力の値の範囲を限定しない方が適切であるため，出力層ニューロンには活性化関数を用いません（注）．\n",
        "また，中間層ニューロンの活性化関数にはシグモイド関数を用いることにします．\n",
        "\n",
        "<span style=\"font-size: 75%\">\n",
        "※注: 「活性化関数を恒等関数 $\\sigma(s) = s$ としている」という方がより適切．\n",
        "</span>"
      ],
      "metadata": {
        "id": "WjovrXKSzcfL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "損失関数には二乗誤差を用いることにします．$N$ 個の学習データのうち $n$ 番目のものを入力したときのネットワークの出力を $z_n$，その正解（上記のY座標の値）を $\\widetilde{z}_n$ とおいて，損失関数を $L$ と表すことにすると，\n",
        "\n",
        "$$\n",
        "L = \\frac{1}{2}\\sum_{n=1}^{N}(\\widetilde{z}_n - z_n)^2\n",
        "$$\n",
        "\n",
        "です．\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hkQftltK1qV3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 実験\n"
      ],
      "metadata": {
        "id": "lifEgFfDBdNL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "上記の問題について実際にコードを実行して，ニューラルネットに学習させてみましょう．\n",
        "\n",
        "ここでは，[PyTorch](https://pytorch.org/) という深層学習フレームワーク（注）を用いてコードを書いています．\n",
        "深層学習フレームワークというのは，ニューラルネットワークの学習などの処理を簡単なコードで実装できるようにしたソフトウェアの集まり（ライブラリ等とも呼ばれる）のことです．PyTorch の他にも， Google の [TensorFlow](https://www.tensorflow.org/) などいくつか有名なものがあります．複雑なニューラルネットワークを簡単に実装できます．\n",
        "例えば，以下のコードでは，ニューラルネットワークの構造を定義して，利用する損失関数を指定するだけで，勾配の計算は PyTorch が自動的にやってくれています．\n"
      ],
      "metadata": {
        "id": "wO8XJ6_8bgdZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# PyTorch を用いたニューラルネットのクラスの定義\n",
        "#\n",
        "class NeuralNetR(nn.Module):\n",
        "\n",
        "    # コンストラクタ．\n",
        "    #   D: 入力次元数，H: 中間層ニューロン数，D: 出力層ニューロン数\n",
        "    def __init__(self, D, H, M):\n",
        "        super(NeuralNetR, self).__init__()\n",
        "        self.layer1 = nn.Linear(D, H) # 入力 → 中間層\n",
        "        self.sigmoid = nn.Sigmoid()   # 中間層の活性化関数\n",
        "        self.layer2 = nn.Linear(H, M) # 中間層 → 出力層\n",
        "        # 以下ではデモのためあえて過適合を起こしやすい初期値にしている\n",
        "        nn.init.uniform_(self.layer2.weight, -10, 10)\n",
        "\n",
        "    # 入力 X に対するネットワークの出力を計算\n",
        "    def forward(self, X):\n",
        "        Y = self.sigmoid(self.layer1(X))\n",
        "        Z = self.layer2(Y)\n",
        "        return Z\n",
        "\n",
        "\n",
        "# データの準備\n",
        "X = np.linspace(-3, 4, num=8, dtype=np.float32) # 学習データ（入力）\n",
        "Zt = np.sin(X)                                  # 学習データ（出力の正解）\n",
        "X_tensor = torch.from_numpy(X[:, np.newaxis])\n",
        "Zt_tensor = torch.from_numpy(Zt[:, np.newaxis])\n",
        "xmin, xmax = -4.5, 5.5\n",
        "Xr = np.linspace(xmin, xmax, num=100, dtype=np.float32)\n",
        "Xr_tensor = torch.from_numpy(Xr[:, np.newaxis])"
      ],
      "metadata": {
        "id": "00L_7mueBuC4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#@title #### ニューラルネットワークによる非線形回帰\n",
        "#@markdown このセルを実行すると，再生ボタンなどが付いたグラフが現れます（**少し時間かかるかも**）．以下のメニューから中間層ニューロン数$H$を変えられるよ．\n",
        "H = 10 #@param [10, 100] {type: 'raw'}\n",
        "\n",
        "# 学習の準備\n",
        "model = NeuralNetR(1, H, 1) # モデルのインスタンスを生成\n",
        "loss_func = nn.MSELoss()    # 平均二乗誤差を損失関数とする\n",
        "optimizer = torch.optim.SGD(model.parameters(), lr=0.01) # 最適化法を選ぶ\n",
        "nitr = 2000\n",
        "\n",
        "# グラフの準備\n",
        "fig = plt.figure(facecolor='white', figsize=(12, 6))\n",
        "ax1 = fig.add_subplot(121)\n",
        "ax1.set_xlim(xmin, xmax)\n",
        "ax1.set_ylim(-3, 3)\n",
        "ax1.scatter(X, Zt)\n",
        "ax1.plot(Xr, np.sin(Xr), color='gray')\n",
        "ax2 = fig.add_subplot(122)\n",
        "ax2.set_xlim(0, nitr)\n",
        "ax2.set_ylim(0, 1)\n",
        "iList = []\n",
        "mseList = []\n",
        "aList = []\n",
        "\n",
        "for i in range(nitr+1):\n",
        "\n",
        "    Z = model(X_tensor) # 出力を計算\n",
        "    loss = loss_func(Z, Zt_tensor) # 損失関数の値を計算\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward() # 勾配を計算\n",
        "    optimizer.step() # パラメータを更新\n",
        "\n",
        "    if (i < 100 and i % 10 == 0) or i % 100 == 0:\n",
        "        #print(i, loss.item())\n",
        "        Zr = model(Xr_tensor).detach().numpy()\n",
        "        a1 = ax1.plot(Xr, Zr, color='red')\n",
        "        iList.append(i)\n",
        "        mseList.append(loss.item())\n",
        "        a2 = ax2.plot(iList, mseList, color='blue', marker='.')\n",
        "        aList.append(a1+a2)\n",
        "\n",
        "anim = animation.ArtistAnimation(fig, aList, interval=300)\n",
        "rc('animation', html='jshtml')\n",
        "plt.close()\n",
        "anim"
      ],
      "metadata": {
        "id": "WTdydVHOIarY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##### ★★ やってみよう ★★\n",
        "\n",
        "中間層のニューロン数 `H=10` と `H=100` それぞれの条件で，上記のセルを複数回実行して，学習の様子を観察しましょう．\n",
        "中間層ニューロン数の違いでどのような違いが生じているか考えて，紙媒体にメモしておきましょう．"
      ],
      "metadata": {
        "id": "xSun21zEfS-k"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}